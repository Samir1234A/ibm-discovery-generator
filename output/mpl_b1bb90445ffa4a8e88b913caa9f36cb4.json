{
  "sdl_source_type": "MPL",
  "productName": "StairMaster's cosmology's",
  "uploadedate": "2016-06-11T00:00:00",
  "productUrl": "http://embezzlement.com",
  "creatorNames": "Fedra m Boulaioune;Meifang j Oaks;Fulvio m De Lis;Donnetta o Hananiah",
  "uploaded": "2015-07-17T00:00:00",
  "sdl_extracted_summary": "a description of the effects of those risk factors. Such an assessment is \\ncrucial for risk management; by enabling risk awareness, risk mitigation, and risk-based decision \\nmaking. \\nA variety of risk assessment methods exist, across many industries [1], [2]. These methods all \\nhave the same basic steps in common: Determine what can go wrong1.\\nDetermine how often this happens2.\\nDetermine the extent of the consequences3.\\nDetermine if the assessed risk is acceptable4. Methods to perform the risk assessment vary widely across industry domains. Furthermore, even \\nwithin the same domain, the risk assessment can take different forms which may differ in \\ndimension. A primary driver of model design is the desired output: Will risk be expressed as a \\nnumerical value as a function of probability? Or will the risk be expressed as a qualitative term \\nexpressed linguistically or as a relative index on a normalized rating scale? This first design \\ndecision drives the selection of model expression (quantitative versus rule-based), and model \\ninputs (quantitative versus qualitative data).\\nRegardless of methodology, risk is the probability of an undesired outcome with the severity of \\nthat outcome. Risk (R) is the Probability (P) of Severity (S) Risk Assessment Model Dimensions / Evaluation Criteria2.1\\nIn evaluating risk modeling for airworthiness assessments, there are several dimensions along \\nwhich comparisons may be made, as indicated in the following list. There is no order or \\nweighting of these criteria. Nor are these criteria the only basis for the final recommendation. How is the risk model expressed? \\nWhat kind of input data feeds the approach?\\nWhat kind of output is produced? Additional metrics for model implementation evaluation include: Complexity of model\\nIntended use (as designed)\\nDecision support automation\\nRisk areas covered Risk to pedestrians on the ground-\\nRisk to other aircraft- ii\\n Risk of penetration of sensitive/restricted areas-\\nRisk due to human error (whether in planning, controlling, responding to -\\nunforeseen/adverse conditions) Model Expression2.1.1\\nModel expression refers to the formalism used to encode the risk computation. This may take the \\nform of fault and event trees [3], [4], mathematical formulations [5], risk matrices [6], rule-based \\napproach [7], or some combination of these and others [8]. Choice of representation is dependent \\non intended use and available data. \\nQuantitative models enable rapid exploration of input parameters. Trials (what-if tests) are easy \\nand inexpensive to run, replicate, and quantify. Probabilistic distributions of input parameters can \\nbe randomly sampled to explore the interactions among inputs, and results (e.g., most likely \\noutput).\\nQualitative models emphasize representation of a maximal set of risk factors. Presentation of \\nnumerical input and output of a model may take various formats such as formulas, look-up tables, \\nflow charts, and risk matrices. The rules which combine inputs are typically knowledge-driven, \\nmeaning that they are derived from subject matter expert knowledge elicitation instead of \\nformulas. Input Data2.1.2\\nRisk models vary in their data requirements. Input data can range from precise quantification of \\nrisk factor probabilities and probabilistic distributions to subjective assessments, expressed on a \\nnumerical scale, or a descriptive term (such as low and high). For example, the probability of \\na component failure may be expressed quantitatively as a point probability (e.g., 110-5) or as a \\nprobabilistic function (e.g., a normal distribution around a mean value). A qualitative reliability \\ninput may be expressed as very reliable.\\nQuantitative models will depend on quantitative data to compute a quantitative output, while \\nqualitative or non-numerical data can be used only by qualitative methods (e.g., rule-based \\nmodels). Outputs2.1.3\\nRisk model output is motivated by demands of the assessment problem and constrained by the \\nmodeling methodology and input data. Methods which produce a quantitative expression of the \\nseverity of a hazard, in conjunction with its probability of occurrence, are referred to as \\nquantitative methods.\\nInterpretation of a quantitative risk model output, probability of severity, is straightforward. For \\nexample, probability of an adverse effect during a particular time period, such as probability of \\nsUAS failures in one flight hour. Severity in this example relates sUAS failures to fatalities. \\nQuantitative models can be validated over time based on actual event statistics, however, this is \\nproblematic for rare events. As a substitute, the input probability of relatively more common \\nadverse events can be refined (e.g., motor failure). The model algorithm, how output is calculated \\nbased on input, can also be challenged and reevaluated by various researchers until a safe \\nconsensus is achieved. \\nQuantitative modeling can use both statistical and deterministic parameters to represent real iii\\n world situations. These values can be presented in various formats such as graphs, equations, \\ndiagrams, scatterplots, and tree diagrams. The model provides an abstraction that reduces a \\nproblem to its essential characteristics. The model describes, in a quantifiable way, the impact of \\nthe inputs on the output. Inputs represent parameters which can be determined, measured, or \\ndescribed probabilistically and the output is similarly numerical or probabilistic. \\nQuantitative risk assessment is sometimes also known as probabilistic risk assessment. The goal \\nis to use quantitative data (measurable, comparable) to support an evaluation whose result is a \\ncalculated expression of the severity of a hazard in conjunction with its probability of occurrence. \\nThere are various understandings and definitions of risk. [2], [9]. \\nProbabilistic risk assessment uses data to develop a probability distribution of risk. This enables \\nthe evaluation of input data variability and uncertainty and its effect on the risk assessment. Strengths and Weakness of Quantitative Modeling2.1.4\\nThe openly shared algorithm is the main strength of a quantitative approach to risk assessment. Increased model clarity: inputs and outputs are explicit\\nComparability\\nRepeatability\\nAutomation, modeling and simulation A risk model which shares its algorithm description is clear and comprehensible in the sense that \\nit specifies exactly how the inputs of the model are determined or measured. The formal \\nexpression relating the input parameters to the output can be verified by other researchers. \\nFor the same inputs, the model will always generate the same output. Because of this \\nconsistency, it is easy to compare model outputs under different conditions. Changes in output \\nare due only to changes in measurable input variables. Finally, a quantitative model with a clearly \\ndefined range of determined or measured inputs is programmable and can be run automatically to \\nsupport various modeling and simulation strategies, such as Monte Carlo methods.\\nIn contrast, the weaknesses of quantitative modeling are: The quality of the output is only as good as the quality of the input\\nNarrower scope of available features, since data is required to support probabilities\\nIt may ignore some important qualitative (non-quantifiable) differences for simplicity\\nIt may blur a desired distinction between high probability, low severity events, and low \\nprobability, high severity events (which may have the same risk score) A qualitative feature must be given a scale of numeric values (e.g., 1= Strongly Agree, 5= \\nStrongly Disagree). Not all qualitative features are amenable to this mapping, which can restrict \\nthe inputs. As such, in situations where the process to be modeled is incompletely understood or \\nspecified, model expressivity is lost and the model will be incomplete. Assumptions may be \\nneeded to fill in the unquantifiable aspects. Reducing a complex process into a simple equation \\nmay risk losing important distinctions. For example, events A and B may appear to have equal \\nrisk, when the product of the high likelihood and low severity of event A may be equal to the low \\nseverity and high likelihood of event B. iv\\n Knowledge-Driven Risk Assessment2.2\\nIn a knowledge-driven system, risk inference methods are based on variables, rules, and \\nrelationships derived from subject matter experts. Although a qualitative process may be \\nformalized and rigorous, it does not necessarily require a precise algorithm. The dominant feature \\nof such models is that input data may be qualitative, reflecting inputs which are not measurable \\nquantities, and output is qualitative. Output may be a risk classification, or relative risk index on \\nsome scale (such as 1 to 10). Qualitative inference methods may include matrices relating \\nlikelihood categories and severity categories to risk, such as the FAAs safety system \\nmanagement (SMS) risk matrix [6]. Other methods include event and fault trees, rule-based \\nsystems, and various combinations of such methods. Strengths and Weaknesses of Knowledge-Driven Assessment2.2.1\\nThe risk value inference process is a logical and repeatable process, which can consider a very \\nbroad range of risk and mitigation factors. For instance, in the SORA process, the guidance \\nindicates that more than 100 inputs are required for the risk assessment. At least 40 of the \\nrequisite inputs are quantifiable, measurable parameters, such as aircraft dimensions and mass. \\nOther inputs are qualitative (e.g., high/medium/low) and free text (e.g., name of aircraft design \\norganization). \\nThe breadth of inputs required for SORA indicate the span of the risk assessment. If a system is \\nmanufactured by a well-established company and undergoes regular maintenance and \\nconfiguration management, the risk may be assessed to be less than for a new applicant.\\nA knowledge-driven risk assessment is generally more labor-intensive and complex. There are a \\nseries of required steps, most of which are manually derived. Although the model is a logical and \\nrepeatable process, the complexity of input parameters entails a significant amount of subjectivity \\nby the assessor. i\\n Risk Modeling for Small UAS Airworthiness Assessment3\\nModeling sUAS mission risk, in terms of",
  "sdl_date": "2020-03-11T00:00:00",
  "countryPublished": "Israel",
  "conference": "wilderness's refuse sy Rajiv",
  "originalAuthorName": "Pat k Manias",
  "title": "taster leapt",
  "declaredTags": "testing|healthcare data|hyperspectral imaging|intellectual property|next generation of innovators",
  "releaseReason": "redbreast's/cockroach's",
  "docName": "JP_61_9901",
  "fundingCenter": 97,
  "resourceURL": "https://noblewoman.com",
  "fundingDepartment": "gy40",
  "caseNumber": "74-8178",
  "publicationDate": "6/20/2017 12:00:00 AM",
  "releaseYear": 2008,
  "releaseStatement": "Public Collaboration/Benchmarking/Standards Body",
  "approver": "$Anje $De Cozar",
  "handCarry": 1,
  "authorDivision": "jx50",
  "copyrightOwner": "Lyubomyr Escale",
  "lastModifiedDate": "10/28/2008 12:00:00 AM",
  "releaseDate": "9/24/2012 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "8366CPWD75",
  "materialType": "Book",
  "publicationType": "Book",
  "authorCenter": 48,
  "originalAuthorID": "Mouhcin",
  "mitrePublicServer": 0,
  "subjectTerminology": "Management (General)",
  "dateEntered": "5/6/2004 12:00:00 AM",
  "documentInfoURL": "https://octogenarian's deficiency's outperforming Themistocles's deliberation.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE4  17-3133|CASE1: 18-0296|CASE1: 18-0562",
  "organization": "bp18",
  "authorDepartment": "bm94",
  "publicationYear": 2011,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "CAASD Non-Direct Project",
  "level1": "Corporate Ops & Transformation",
  "fundingDivision": "Corrine fray simultaneous torched pawpaw's",
  "publishedOutsideUSA": 0,
  "level3": "wg41",
  "level2": "tx73",
  "sdl_id": "b1bb90445ffa4a8e88b913caa9f36cb4",
  "text": "made if the approach is continued or expanded. This report and the prototypes that were created position a team of software developers to build this type of functionality in the current web server environment, in a relatively fast manner (estimated as just a few months). If successful, the pilot program could be expanded on a service-wide or even DoD wide basis. Once the data base is established, data based records for new programs would replace current methods of routing documents for approval. It would also be possible to request programs to convert their current documents to the new system and enter the relevant data, which could be done in a phased manner over 12 to 18 months. This would allow earlier use of data analytics and correlation based statistical analysis. In summary, an electronic data-based version of current defense procurement planning documents should not be just a direct translation of the current reports, since that misses an opportunity to collect certain new data that is critical to creating more valuable management tools. The transition also provides the option or ability to streamline the current documentation, eliminating less useful components like organization charts or graphical schedules that were hard to read, and collects data sets that will allow better planning and comparative oversight using data analytics. Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 References/Bibliography \\n[1] Defense Acquisition University Milestone Document Tool, https://www.dau.mil/mdid/Pages/Default.aspx [2] Department of Defense Instruction 5000.02, Defense Acquisition https://www.dau.mil/mdid/Pages/Default.aspx Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Appendix A Summary, Rapid Technology Insertion Process \\nThe Navy continues to require methods to rapidly adopt and procure mature technology in order to counter evolving and complex threats and implement its adopt/buy/develop strategy to provide new functionality to naval ship combat systems. An Acquisition Plan (AP) has been developed that adopts and improves the Rapid Technology Insertion (RTI) method developed by PEO LCS for use by the Naval Warfare Center enterprise. The RTI process provides for the acquisition of product adaptation and project transition engineering in a broad range of topical areas related to the development of new or upgraded naval warfare capabilities. The RTI method involves the steps as shown in Figure 2-1, which includes the following activities. a) Identification of a problem or need, with a validated technical definition and context. b) Completion of a short RTI Appendix to authorize the procurement action. c) Completion of technical documentation or problem statements and associated GFI. d) A draft RFP is posted, with a request for White Paper responses. e) White papers are evaluated and feedback is provided. f) The Final RFP is posted, and proposals are received and evaluated. g) One or more contracts are awarded, with a firm fixed-price Transition Study as the basic \\naward. h) Integration and ship design agent expertise is leveraged to evaluate the project and \\nestimate associated resources to complete an integrated solution. i) If multiple awards were made, a down-select to one contract is made, if desired. j) Associated integrator or shipyard agent tasks are created and funded. k) A contract option for the engineering and assessment phase is awarded, and the \\ncontractor works with the integrator to develop the technology solution and deliver a test article. l) Government testing is conducted to formally assess the technology or solution. m) If the testing was successful, additional (up to 3 years of) contract options are exercised \\nto produce and deliver units. n) Field support is enabled, and preparations are made for follow-on production contracts if \\nnecessary. These actions were designed to account for several typical problems and allow most programs to rapidly leverage them and achieve a standard program structure. The process also allows program teams the flexibility to customize the RFP as much as it needs to, although the amount of time needed to review, edit, approve, and process documents will also increase as the level of customization increases. Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Figure 12-1. RTI Process Overview, its standard structure and its phased structure. \\nUsing the pre-approved standard document approach, the RTI appendix to authorize a project was reduce to just a 3 page appendix, which was routed and approved in just 2 weeks. A sample schedule comparison is shown in Figure 3-1, where the timeline from a trade study, writing an AP and an RFP, and contract award typically take about 3 years. The RTI approach, which folds the trade study into the procurement process and uses standard documents, should take less than 15 months. Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Figure 12-2, Schedule Comparison, Legacy Program Timeline and RTI Appendix B RTI Project Appendix Format Distribution Unlimited. Public Release Case Number 18-2212 APPENDIX RTI-tbd RTI Project: Appendix Template FOR OFFICIAL USE ONLY ACQUISITION PLAN NUMBER: NSWC/NUWC 18-01 REV: 0 RTI PROJECT TITLE: ACAT ACQUISITION PROGRAM MANAGER: CODE CAPABILITIES/REQUIREMENTS DOCUMENT: See Section 6 of this appendix. . ACQUISITION STRATEGY APPROVAL: See Section 6 of this appendix. DESCRIPTION OF PROGRAM: See Section 7 of this appendix. APPROVED BY: HCA, PEO, or DRPM (include title) Date Chief of Contracting Office Date Contracting Officer Date Program Manager Date Questions concerning this AP should be referred to Dr. Megan Fillinich, NSWC Chief Technology Officer, at (202) 781-3937, Megan.Fillinich@navy.mil. The cutoff date for information contained in this Acquisition Plan is xx March, 2018. Questions about this specific RTI project should be referred to , at (202) 781-xxxx, @navy.mil. The cutoff date for information contained in this appendix to the RTI Acquisition Plan is 201x. mailto: @navy.mil Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 CHANGE / PROJECT AUTHORIZATION, ACQUISITION PLAN NO. NSWC/NUWC 18-01. Appendix A: Project Specific Appendix for (Insert RTI Topic Name Here ) 1. Topic Name: Provide the RTI topic or project name 1a. Prototype: _Yes or No. 1b. ACAT: _1 to 4_ 1c. MDAP: Yes or No. 2. Technology Focus Area: Which focus area is this RTI project associated with? 3. DoD Product or Services Description Code: 4. Lead Program Office: Identify the lead PMO. 5. Lead Contracting Office: Specific Warfare Center or Naval Activity. 6.1 Statement of need. Introduce the plan by a brief statement of need (referencing generic RTI Section 1 authority is sufficient). Include status of any applicable Acquisition Strategy, Acquisition Decision Memorandum, Defense Acquisition Board, and/or any other internal service reviews. 6.2 Historical Summary: Provide one or several paragraphs describing the background for this RTI project. Identify the capabilities (e.g., Capability Development Document (CDD) or requirements (e.g., Operational Requirements Document (ORD)) or other document that authorizes program initiation, include approval date, and revalidation date, if applicable (referencing generic RTI Section 1 authority is sufficient). 6.3 Previous Contract History: Identify any previous related contracts over the past 5 years (contract number, contractor, contract type, supply/service description (title only), quantities, period of performance, historical or estimated contract value and whether a sole source or competitive contract award). 6.4 RTI Rationale: Provide a rationale as to why the RTI approach is appropriate and a discussion on what Government resources are available to award and manage the procurement and why they are sufficient and adequate (e.g. the approach was used on similar projects). Identify the likely or possible system or platform integrators, if needed, for the project, the annual resources they will need each year, and the contact at the relevant program office that has agreed to coordinate and support the project. 7. Project Objective: Describe the project in brief, non-technical language; e.g., a brief description similar to that forwarded in the Congressional data sheets with the annual budget. Characterize that the project is just starting RTI process. Include a project description, likely quantity to be procured in the RTI contract (including the fielding/production phase). Provide the primary and secondary engineering measurement parameters being used for this RTI procurement. Describe the tradeoffs or inter-relationships between the primary and secondary parameters. 8. Significant Patents or Copyrights: Describe any significant items with intellectual property issues or limitations and how they affect the procurement. Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 9. Estimated Budget & Costs: The funds in this chart represent the funding considered adequate to execute the proposed RTI project. Table A.1 shows the funding available by fiscal year. Table A.2 shows the RTI funding by project phase and planned procurement quantities. FY1x FY1x FY1x FY1x FY1x Total RDT&E,N Funding $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX OP,N $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX O&M,N $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX SC,N $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX Total Funding $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX $X,XXX Table A-1: RTI Topic Budget Data ($K) FY FY1x FY1x FY1x FY1x FY1x Total Phase 1, Transition Study & Eng. Svcs, Est.Cost $X,XXX $X,XXX Phase II, Assessment & NRE, Est. Cost $X,XXX",
  "updated_at": "12/4/1997 12:00:00 AM",
  "created_at": "5/14/2012 12:00:00 AM"
}