{
  "sdl_source_type": "MPL",
  "productName": "tardy go",
  "uploadedate": "2018-11-04T00:00:00",
  "productUrl": "http://Guinevere.com",
  "creatorNames": "Vlad p Blondeau;Dorotea g Rohrmann",
  "uploaded": "2014-04-10T00:00:00",
  "sdl_extracted_summary": "action. However, once the Exceedance Warning was \\ndisplayed for the Lead, some controllers in this scenario may have then also decided to \\nthen intervene with the slowly-deviating Trail instead of waiting for it to cross the Lateral \\nBoundary. It is also notable that these cases occurred more often in the Separate Monitor \\nconfiguration (12/14) versus Combined (2/14). This is likely due to the difference in the \\ntiming of the communications between the configurations, especially with the scenarios \\ninvolving traffic file O, which comprised ten of these 14 cases. The timing of these O \\nscenarios was such that that the shallow-deviating Trail crossed its Lateral Boundary and \\ntriggered its Exceedance Warning shortly after the Lead Aircraft (of a different pair) \\ntriggered its Exceedance Warning. In the Combined Monitor configuration, the controller \\ntypically contacted the sharply-deviating Lead Aircraft first to turn it back, then \\ncontacted its Trail Aircraft to instruct its breakout. This was given higher priority than the \\nshallow-deviating Trail, and so it then crossed its Lateral Boundary and triggered the \\nExceedance Alert before the controller could contact it. In the Separate Monitor cv\\n configuration, however, the 28R controller was not required to contact the deviating \\nLead Aircraft. Therefore, there was more time for the 28R controller to break out the \\nTrail from the deviating lead and then break out the shallow-deviating Trail before it \\nactually crossed its Lateral Boundary. As noted in Section 4.5.5.3, the O scenario was deliberately designed as an extreme case. In \\nreal world operations, a near-simultaneous lateral deviation of multiple aircraft on CSPR \\nis expected to occur rarely, if at all. An investigation of over 1.8 million approach paths \\ndid not detect any (Eckstein, Massimini, McNeill, & Niles, 2012), nor was there a record \\nof any in an examination of 7790 go-arounds that were logged over multiple years by \\nNCT (Stassen, Domino, Hefley, & Weitz, 2019). Still, this was included in the simulation to \\nstress the display features and probe for a potential failure point. The findings here, \\nthough not conclusive, may suggest controller behavioral differences with respect to \\nintervening with aircraft deviating laterally may only be apparent in the most extreme \\nsituations. When this occurs, however, the findings are consistent with a general \\nexpectation that controllers may be able to take action more quickly when only having \\nresponsibility for one arrival. Simulation Evaluation6.7\\nAt the end of Day 3 (QT8) controllers were asked about their overall experience in the \\nsimulation and its effectiveness in evaluating IM PA. This section summarizes their responses. Controllers were first asked if the training I received was adequate for the IM Clearance \\nCompletion and IM PA monitoring tasks. Given their specific experience in the areas being \\nsimulated, NCT responses are shown separately. Non-NCT and Tower responses are thus also \\nshown as separate categories. Response Means and Standard Deviations are summarized in \\nTable 5-70. Scale responses are shown in Figure 5-79. Table 5-70. Controller Responses to Adequacy of Simulation Training by Task Participant Experience Non-NCT NCT Tower IM Clearance Completion\\nSample Size (n) 6 4 2 Mean (M) 96.0 88.0 96.5 Standard Deviation (SD) 7.5 5.4  IM PA Monitoring Sample Size (n) 6 4 2 Mean (M) 97.2 90.5 96.0 Standard Deviation (SD) 4.0 2.5  cvi\\n Figure 5-79. The training I received was adequate. All controllers responded that the training they received was adequate for both tasks. --------------------------------- Controller participants were also asked whether the overall activity was effective as a context \\nfor evaluating the IM Clearance Completion task and IM PA monitoring task. Given their \\nspecific experience in the areas being simulated, NCT responses are shown separately. Non-NCT \\nand Tower responses are thus also shown as separate categories. Response Means and \\nStandard Deviations are summarized in Table 5-71. Scale responses are shown in Figure 5-80. Table 5-71. Controller Responses to Simulation Effectiveness by Task Participant Experience Non-NCT NCT Tower IM Clearance Completion\\nSample Size (n) 6 4 2 Mean (M) 95.3 61.5 96.5 Standard Deviation (SD) 7.2 29.8  IM PA Monitoring Sample Size (n) 6 4 2 Mean (M) 96.2 90.5 96.5 Standard Deviation (SD) 5.3 7.8  cvii\\n Figure 5-80. The overall activity was effective as a context for evaluating the IM Clearance completion \\ntask and the IM PA monitoring task. All controllers agreed the overall activity was effective as a context for evaluating the IM PA \\nmonitoring task. The majority (83%) agreed the simulation was effective for the IM Clearance \\ncompletion task. One NCT controller neither agreed nor disagreed and commented that Issuing \\nthe clearance did not seem realistic. Another NCT controller disagreed and commented: IM \\nPA initiation did not take into account how much workload the final controller is doing prior to \\nissuing the instructions. --------------------------------- Finally, at the end of Day 3, controllers were asked if there was anything about the simulation \\nthat artificially affected using it as a context for evaluating IM PA operations. Four (4) \\ncontrollers answered Yes, seven (7) answered No, and one (1) did not provide a response. \\nSelected comments included: One Yes comment: (NCT) Lack of practical scenarios in which vectoring and other \\nevents are taking place.\\nOne Yes comment: (TRACON) At first, too much speed control required to maintain \\nseparation between pairs that took focus off IM PA functionality. It did bring to light how \\neasy/noticeable the alerts were while focused on other tasks. Note: this was a Week 1 controller response. Scenarios were modified between o\\nWeek 1 and Weeks 2-3 to reduce the need for this speed control. One Yes comment: (TRACON) No instant override makes some blunders and overtakes \\nlook even worse because of the delay in implementing controllers instructions.\\nOne Yes comment: (TRACON) I didn't feel like the aircraft speeds throughout the final \\napproach were necessarily realistic. Hypothesis Evaluations6.8\\nAs discussed in Section 3.3, six research questions and seven hypotheses were defined prior to cviii\\n HITL data collection. This section consolidates and summarizes the findings relevant for each \\nhypothesis. As needed, statistical tests were performed on the subjective questionnaire data to \\ninform the hypothesis evaluation. H1(RQ1): Given an appropriate tool set, controllers will find it acceptable and 6.8.1\\nfeasible to monitor IM PA operations with respect to both minimum and \\nmaximum separation limits. This hypothesis was measured via responses to three questions related to monitoring comfort, \\nseparation assessment, and task acceptability. The first part of the hypothesis, acceptability, \\nwas measured by controller responses to the following Day 1-2 post-run (QT1) question: Given \\nthe IM PA-related tools provided in this scenario, I was comfortable monitoring IM PA \\noperations when both a CSL and WSL were active at the same time. As described in Section \\n5.2.2, nearly all Week 2-3 TRACON controllers agreed they were comfortable monitoring IM PA \\noperations with both limits at the same time for both the monitor configurations and IM PA \\nTools configurations. Only one controller neither agreed nor disagreed, and this was for the \\nSeparate 28R Monitor with IM PA Tools Off configuration. To determine if this resulted in a statistically significant difference between controller responses \\nfor the On versus Off IM PA Tool configurations for the Separate 28R Monitor controller, a \\npaired-sample T-Test analysis was run on the Week 2-3 TRACON controller agreement ratings \\nfor this question. The descriptive statistics (n, M, and SD) are shown in Table 5-4. The probability \\nof finding an effect when there was none, alpha (), was set to 0.05. The results of a two-tailed \\nT-Test analysis failed to find statistical significance (p = 0.22). This suggests no significant \\ndifference in strength of controller agreement between the two IM PA Tool configurations with \\nrespect to monitoring IM PA operations when both a CSL and WSL were active at the same time. Acceptability was also measured by controller responses to the following Day 2 end (QT3) \\nquestion: The tasks required of each simulation position were acceptable. As described in \\nSection 5.2.5.2, nearly all TRACON controllers agreed for the positions that required IM Trail \\nAircraft Monitoring (Combined and Separate 28R). One controller neither agreed nor disagreed \\n(for the Combined Monitor configuration). --------------------------------- The second part of the hypothesis, feasibility, was measured by controller responses to the \\nfollowing Day 2 end (QT3) question: Overall, I was confident that I could assess whether the \\nseparation between the IM Trail Aircraft and their Lead Aircraft would be maintained. As \\ndescribed in Section 5.2.5, the majority (80%) of TRACON controllers agreed. This self-\\nassessment was supported by the objective separation analysis in Section 5.2.5.2, which found \\nonly a single instance of a within-pair IM PA separation violation. Conclusion: This hypothesis is supported by the simulation results. Overall, controllers found it \\nacceptable and feasible to monitor IM PA operations with respect to both minimum and \\nmaximum separation limits. This did not appear to be affected by monitor configuration or IM \\nPA Tools configuration. cix\\n H2(RQ2): Given an appropriate tool set, controllers will find it acceptable and 6.8.2\\nfeasible to provide PA separation with respect to separation values that change \\nover the course of an approach. Though not asked this question directly, controllers in every scenario experienced a CSL that \\nchanged in",
  "sdl_date": "2020-06-15T00:00:00",
  "countryPublished": "United Arab Emirates",
  "conference": "airship's compatibility op Indra",
  "originalAuthorName": "Eloysa l Vodden",
  "title": "legality",
  "declaredTags": "veterans at risk|LinkBioMan|Drone Assisted Radar Target|spectral clustering|physical relationships",
  "releaseReason": "Honolulu/gingham's",
  "docName": "VC_78_8574",
  "fundingCenter": 44,
  "resourceURL": "https://unsolicited.com",
  "fundingDepartment": "po87",
  "caseNumber": "51-2721",
  "publicationDate": "6/4/2018 12:00:00 AM",
  "releaseYear": 2005,
  "releaseStatement": "Advertising/Recruiting",
  "approver": "$Leonides $Lian",
  "handCarry": 5,
  "authorDivision": "pa31",
  "copyrightOwner": "Yusraa Gerhard",
  "lastModifiedDate": "9/18/2020 12:00:00 AM",
  "releaseDate": "9/29/2018 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "2590CBAF36",
  "materialType": "Book",
  "publicationType": "Article",
  "authorCenter": 67,
  "originalAuthorID": "Sharie",
  "mitrePublicServer": 0,
  "subjectTerminology": "Information Systems (General)",
  "dateEntered": "11/22/2006 12:00:00 AM",
  "documentInfoURL": "https://carafes shuffleboard's crayons legends sagest.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 18-0935|CASE1: 18-0469|CASE1: 17-3768|CASE1: 18-0442",
  "organization": "by51",
  "authorDepartment": "qd71",
  "publicationYear": 1996,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "CAMH FFRDC Contracts",
  "level1": "Programs & Technology",
  "fundingDivision": "invincibility's backache bop's sentimental loadstone",
  "publishedOutsideUSA": 0,
  "level3": "xo34",
  "level2": "hj89",
  "sdl_id": "d906c569529c47d69a8d6a6028e9b50f",
  "text": "T6-R1 Over the course of the three-day data collection days, the controllers experienced traffic file 4\\nwith en route initiation, while pilots were trained. For the remainder of the day, controllers\\nexperienced the other six traffic files with the flight deck participants. The block order of the\\ntraffic files was counter-balanced across participant groups. Off-nominal events were introduced for the controllers through pseudo-pilot action. Each day\\neach controller experienced an event where the termination or suspension of IM was required.\\nThe pseudo-pilot was told to acknowledge the IM clearance from the feeder controller but to\\nfly a constant speed without engaging IM. The trail aircraft held its speed and started\\nencroaching upon the lead aircraft (which eventually led to a separation issue) until the\\ncontroller intervened. The issue started to evolve in the feeder controllers airspace, but the\\nspacing issue may not have been fully realized until the final controllers airspace based on the\\nslow progression of the overtake. Table 3-6 shows the traffic file run order and traffic overtake\\nevents for the three groups of controllers. 3-51 Table 3-6. Controller Independent Variable Exposure by Day, Traffic File, and Run Note: Traffic overtake conditions are highlighted in orange. Group Operation IM Tools A Controller Role /B Controller Role\\nBaseline NA Feeder/Final D1-T4-R1\\nBaseline NA Final/Feeder D1-T4-R2\\nIM En Route Initiation TSAS tools Feeder/Feeder D4-T4-RC1\\nIM En Route Initiation TSAS tools and slot marker color change Feeder/Feeder D3-T4-RC1\\nIM En Route Initiation TSAS tools, slot marker color change, and ETA differential Feeder/Feeder D5-T4-RC1\\nIM TSAS tools Feeder/Final D3-T4-R1 D4-T5-R1 D5-T6-R1\\nIM TSAS tools and slot marker color change Feeder/Final D3-T1-R2 D4-T6-R2 D5-T4-R2\\nIM TSAS tools, slot marker color change, and ETA differential Feeder/Final D3-T2-R3 D4-T4-R3 D5-T1-R3\\nIM TSAS tools Final/Feeder D3-T3-R4 D4-T1-R4 D5-T2-R4\\nIM TSAS tools and slot marker color change Final/Feeder D3-T5-R5 D4-T2-R5 D5-T3-R5\\nIM TSAS tools, slot marker color change, and ETA differential Final/Feeder D3-T6-R6 D4-T3-R6 D5-T5-R6\\nBaseline NA Feeder/Final D1-T4-R1\\nBaseline NA Final/Feeder D1-T4-R2\\nIM En Route Initiation TSAS tools Feeder/Feeder D3-T4-RC1\\nIM En Route Initiation TSAS tools and slot marker color change Feeder/Feeder D5-T4-RC1\\nIM En Route Initiation TSAS tools, slot marker color change, and ETA differential Feeder/Feeder D4-T4-RC1\\nIM TSAS tools Feeder/Final D3-T4-R2 D4-T1-R2 D5-T6-R2\\nIM TSAS tools and slot marker color change Feeder/Final D3-T1-R3 D4-T2-R3 D5-T4-R3\\nIM TSAS tools, slot marker color change, and ETA differential Feeder/Final D3-T6-R1 D4-T4-R1 D5-T5-R1\\nIM TSAS tools Final/Feeder D3-T3-R5 D4-T5-R5 D5-T2-R5\\nIM TSAS tools and slot marker color change Final/Feeder D3-T5-R6 D4-T6-R6 D5-T3-R6\\nIM TSAS tools, slot marker color change, and ETA differential Final/Feeder D3-T2-R4 D4-T3-R4 D5-T1-R4\\nBaseline NA Feeder/Final D1-T4-R1\\nBaseline NA Final/Feeder D1-T4-R2\\nIM En Route Initiation TSAS tools Feeder/Feeder D5-T4-RC1\\nIM En Route Initiation TSAS tools and slot marker color change Feeder/Feeder D4-T4-RC1\\nIM En Route Initiation TSAS tools, slot marker color change, and ETA differential Feeder/Feeder D3-T4-RC1\\nIM TSAS tools Feeder/Final D3-T4-R3 D4-T1-R3 D5-T2-R3\\nIM TSAS tools and slot marker color change Feeder/Final D3-T5-R1 D4-T6-R1 D5-T4-R1\\nIM TSAS tools, slot marker color change, and ETA differential Feeder/Final D3-T6-R2 D4-T4-R2 D5-T1-R2\\nIM TSAS tools Final/Feeder D3-T3-R6 D4-T5-R6 D5-T6-R6\\nIM TSAS tools and slot marker color change Final/Feeder D3-T1-R4 D4-T2-R4 D5-T3-R4\\nIM TSAS tools, slot marker color change, and ETA differential Final/Feeder D3-T2-R5 D4-T3-R5 D5-T5-R5 (D)ay-(T)raffic File-(R)un Order 1 2 3 3-52 A summary of each data collection day (1 day for pilots; 3 days for controllers) is provided in\\nTable 3-7. Table 3-7. Data Collection Day Details Traffic\\nFile(s) Flight Crew (2 per day) Controllers (3 per day) 4\\n(with en route\\ninitiation) Introductory Briefing and Background\\nQuestionnaire. Training in lab Controller role:\\nA Feeder; B- Feeder\\nScenario:\\nTSAS and IM en route initiation\\nIndependent variable (order varied between days):\\n(1) Basic,\\n(2) Basic+ cue, or\\n(3) Basic+ cue and prediction 1, 2, 3, 4,\\n5, and 6 Pilot role (remained fixed):\\nPF; PM\\nOperation (order varied within day):\\n(1) Baseline No IM (x2 runs)\\n(2) IM (x10 runs)\\nTools (order varied within day):\\nMin CDTI then Min CDTI+ or\\nMin CDTI+ then Min CDTI Controller role (swapped after 3rd scenario):\\nFeeder; Final\\nOperation (traffic file order varied within day):\\nTSAS and IM (x6)\\nTools (order varied within day):\\n(1) Basic,\\n(2) Basic+ cue, or\\n(3) Basic+ cue and prediction 3.6 Data Collection\\nFour methods of data collection were used for this simulation: paper questionnaires, system\\nrecorded data, observations, and final debriefs (when chosen by the participants). Three types\\nof questionnaires were used, including: 1. Demographics: Upon arrival, participants were asked to fill out a demographics\\nquestionnaire which captured participants experience. Controllers and pilots had\\nseparate questionnaires. 2. Post- scenario: After each run / scenario, participants were asked to fill out a\\nquestionnaire based on the run / scenario just experienced. All post-scenario\\nquestionnaires included the Bedford Workload Rating Scale (Roscoe, 1984) along with\\nadditional rating scale and yes / no questions with a comment field for each. The\\ncontroller questionnaires included the Controller Acceptance Rating Scale (Lee, Kerns,\\nBone, and Nickelson, 2001). Pilot participants completed a post-scenario questionnaire\\nafter each run during a scenario (two runs per scenario) and controllers completed this\\nquestionnaire after each scenario. Separate post-scenario questionnaires were used for\\nthe baseline and IM scenarios. Controllers and pilots also had separate questionnaires\\n(Appendix B). 3. Post-simulation: After the final scenario, participants were asked to complete the longer,\\nfinal questionnaire covering all the scenarios experienced. The questionnaire included a\\nseries of rating-scale and yes / no questions with a comment field for each. Controllers\\nand pilots had separate questionnaires (Appendix C). 3-53 In these questionnaires, participants were asked to provide subjective feedback on areas such\\nas the overall IM concept, workload, situation awareness, head down / scan time, displays,\\ncommunications, and simulation realism. Objective metric data was automatically recorded by the simulation platform or by the\\nobservers and included: ATC o Interactions with displays Inputs for IM initiation, rejection, suspension, resumption, and termination o IM initiation delay o Location of IM initiations, rejections, suspensions, resumptions, and terminations All aircraft o Schedule conformance o Slot marker deviation o Events below the applicable separation standard o Frequency of infeasible / no speed events o Time on the RNAV procedure o Spacing error at key locations o How well the ASG was maintained o Arrival rates / throughput Participant aircraft o IM speeds o MCP selected speed o Distance to ABP o Frequency of IM terminations o Interactions with displays (e.g., TTF selection and data entry) 4-1 4 Results\\nThis section begins with a description of the analysis method for both subjective and objective\\ndata. It then describes baseline scenarios and the operations (terminal metering and RNP RF\\nturns) that form the operational foundation for IM. It then covers the objective data, including:\\nthe conduct of IM operations (mainly controller actions related to IM), IM speeds (for\\nparticipant pilot aircraft) and flight crew actions related to those speeds, and aircraft spacing\\nand separation results (for pseudo-pilot and participant pilot aircraft). Subjective data for both\\npilots and controllers is covered next (e.g., acceptability of IM, displays, responsibilities). Time\\non RNAV arrivals and communication results are then provided, followed by en route IM\\ninitiation results, as related to controllers. Finally, the section ends with results for the\\nparticipants assessments of the simulation. 4.1 Analysis Method 4.1.1 Subjective Data\\nThe subjective results are based on responses to the statements from both the post-scenario\\nand post-simulation questionnaires. The post-simulation questionnaires comprise most of the\\ndata so in these cases, the source will not be noted unless it helps for clarity. Any data from the\\npost-scenario questionnaires will be noted. Controller and pilot comments were included if they\\nwere enlightening or if a sufficient number of participants made similar comments. Controller results are based on nine participants while pilot results are based on 18\\nparticipants. Controller responses are divided by the independent variable of controller role.\\nPilot responses are usually combined (as role was not a planned independent variable), unless\\nthere was a clear reason to report the roles separately. Some questions in the questionnaires were yes / no with an opportunity for open-ended\\ncomments. Most response-scale items were statements with 100 hash marks (without numeric\\nlabels) and an opportunity to provide open-ended comments. The scale was anchored on the\\nleft with the label Strongly Disagree and on the right with the label Strongly Agree (Figure\\n4-1). Figure 4-1. 100-Point Agreement Scale Most items were presented as a statement, and participants were asked to rate their level of\\nagreement. Participants were told to draw a straight line anywhere on the scale, including\\nbetween the lines and right on the end points. During data reduction, responses were rounded\\nto the nearest single digit between 0 and 100. In the presentation of the results, any responses\\nbelow the midpoint (i.e., lower than 50) on the scale were considered to be on the disagree\\nside while any responses above the midpoint (i.e., higher than 50) on the scale were considered 4-2 to be on the agree side. Any responses at the midpoint (i.e., equal to 50) were considered to\\nbe neutral (Figure 4-2). Figure 4-2. 100-Point Agreement Scale Agreement Rating Breakdown When presenting results on the 100-point agreement scale in the post-simulation\\nquestionnaires, the following terminology / methodology is used to describe the levels of\\nagreement. All [controllers / pilots] [agreed / disagreed] o All of the participants are on the agree or disagree side of the scale The majority (n; %) of [controllers / pilots] [agreed / disagreed] o Low variability, e.g., SD of less than approximately 25 (unless one value is driving a\\nSD slightly higher) [Controller / Pilot] responses were variable but the majority (n; %) [agreed / disagreed] o Responses have a SD of greater than approximately 25 and distribution is relatively\\nbiased in",
  "updated_at": "6/5/2013 12:00:00 AM",
  "created_at": "8/7/2001 12:00:00 AM"
}