{
  "sdl_source_type": "tcas",
  "field_tca_short_name": "forgery's cautiousness proved",
  "changed": "2015-05-03T00:00:00",
  "field_tca_organizationleadername": "Gissela H Ventrel",
  "capability_rul": "snuggled.com",
  "sdl_date": "2020-02-14T00:00:00",
  "countryPublished": "Nepal",
  "conference": "Inez roost's ve Lorenz",
  "originalAuthorName": "Nada c Sausmikat",
  "title": "optimizes laminating monomania Gatorade's",
  "declaredTags": "best practices|posture-monitoring system|Air Traffic Management",
  "releaseReason": "enameling/rared",
  "docName": "NL_31_8489",
  "fundingCenter": 53,
  "resourceURL": "https://Methodist's.com",
  "fundingDepartment": "si49",
  "caseNumber": "16-8868",
  "publicationDate": "8/14/2017 12:00:00 AM",
  "releaseYear": 2018,
  "releaseStatement": "MITRE External Publication",
  "approver": "$Sander $Haeniche",
  "handCarry": 8,
  "authorDivision": "cu16",
  "copyrightOwner": "Florisa Draxl",
  "lastModifiedDate": "11/20/2020 12:00:00 AM",
  "releaseDate": "3/27/2011 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "3537WWNL80",
  "materialType": "Paper",
  "publicationType": "Paper",
  "authorCenter": 75,
  "originalAuthorID": "Samya",
  "mitrePublicServer": 0,
  "subjectTerminology": "Mathematics (General)",
  "dateEntered": "5/19/2013 12:00:00 AM",
  "documentInfoURL": "https://cynic biased Essen adopt Salado.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 16-2039",
  "organization": "xc63",
  "authorDepartment": "xn71",
  "publicationYear": 2017,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "Non-Sponsored",
  "level1": "MITRE Public Sector",
  "fundingDivision": "Harbin's Surat Babel abrading erupt",
  "publishedOutsideUSA": 0,
  "level3": "xx61",
  "level2": "ns10",
  "sdl_id": "904be8b41f6148d0a38edb6ca8e67152",
  "text": "& add LinkBioMan (500k) 0.799 0.634 0.532 0.433 0.318 0.706 embed & add LinkBioMan (1M) 0.795 0.627 0.525 0.425 0.315 0.698 Show and Tell (500K) [5] on clips with audio 0.780 0.601 0.499 0.397 0.298 0.588 concat LinkBioMan on clips with audio (500k) 0.768 0.584 0.479 0.382 0.295 0.598 The last three rows (green) pertain to LinkBioMan Captioner models (concat, embed & add) on the \\nYouTube2Text clips with audio and the six rows above (orange) on the official YouTube2Text Dataset, \\nwith the best score underlined. All other rows display results of video captioner models using the official \\nYouTube2Text Dataset, except Attention Fusion (AV). The LinkBioMan Captioner results are better \\nwithout audio because the audio dubbing contained in some clips is unrelated to the content, as \\ninvestigated by [3]. xxx embed & add LinkBioMan on clips with audio (500k) 0.792 0.622 0.521 0.421 0.309 0.638 Conclusion6\\nLinkBioMan is a multimodal near-real-time decision framework capable of producing \\nactionable context, a preliminary interpretation of events and automated alerting. With the \\nYouTube2Text Dataset, the LinkBioMan Captioner was able to achieve a maximum METEOR \\nscore of 0.318 and a maximum CIDEr score of 0.706, which is in line with the performance of \\nother video captioners and our highest score exceeding two of the other video captioners \\nconsidered. Additionally, we have demonstrated the effectiveness of the LinkBioMan's context \\nmodule, ContextNet, which achieved a mAP score of 0.325 (and 0.379 with TRECVid clips with \\naudio), in relevant categories, outperforming the TRECVid 2015 teams' maximum mAP score of \\n0.273 in the same categories. Ablative tests on ContextNet demonstrated the critical contribution \\nof raw visual features but motion and audio features have also been shown to boost certain \\ncategories of semantic concepts depending on the nature of the context. These test results are \\npromising but more tests could be done, such as, measuring the speed/accuracy trade-off of the \\nvarious module inputs for ContextNet and the user feedback loop to be considered for the \\nproduction environment. We hope that LinkBioMan can be relevant to use cases from the \\nDepartment of Homeland Security, the Department of Defense, law enforcement agencies and the \\nIntel Community. For future study, we would like to include additional modalities (e.g., text, \\ngeotags) and incorporate unsupervised learning to overcome limitations in training data. ReferencesAppendix A\\n[1] K. Xu et al., Show, attend and tell: Neural image caption generation with visual attention, in International conference on machine learning, 2015, pp. 20482057.\\n[2] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, Image captioning with semantic attention, in Proceedings of the IEEE conference on computer vision and pattern \\nrecognition, 2016, pp. 46514659. [3] C. Hori et al., Attention-based multimodal fusion for video description, in Computer \\nVision (ICCV), 2017 IEEE International Conference on, 2017, pp. 42034212. [4] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, Show and tell: A neural image \\ncaption generator, in Proceedings of the IEEE conference on computer vision and \\npattern recognition, 2015, pp. 31563164. [5] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, Show and tell: Lessons learned from \\nthe 2015 MSCOCO image captioning challenge, IEEE Trans. Pattern Anal. Mach. \\nIntell., vol. 39, no. 4, pp. 652663, 2017. [6] A. Lavie and M. J. Denkowski, The Meteor metric for automatic evaluation of \\nmachine translation, Mach. Transl., vol. 23, no. 23, pp. 105115, Sep. 2009. [7] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, Cider: Consensus-based image \\ndescription evaluation, in Proceedings of the IEEE conference on computer vision \\nand pattern recognition, 2015, pp. 45664575. [8] P. Overretired et al., TRECVID 2015-An overview of the goals, tasks, data, \\nevaluation mechanisms, and metrics, 2016. [9] S. K. Divvala, D. Hoiem, J. H. Hays, A. A. Efros, and M. Hebert, An empirical study \\nof context in object detection, in Computer Vision and Pattern Recognition, 2009. \\nCVPR 2009. IEEE Conference on, 2009, pp. 12711278. [10] J. Wang, X. Zhu, S. Gong, and W. Li, Attribute recognition by joint recurrent \\nlearning of context and correlation, 2017. [11] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, Semantic \\nunderstanding of scenes through the ADE20K dataset, arXiv Prepr. \\narXiv1608.05442, 2016. [12] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, Deeplab: \\nSemantic image segmentation with deep convolutional nets, atrous convolution, and \\nfully connected crfs, IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 4, pp. \\n834848, 2018. [13] L. Yang, K. D. Tang, J. Yang, and L.-J. Li, Dense saptioning with joint inference and \\nvisual context, in CVPR, 2017, pp. 19781987. [14] R. Mottaghi et al., The role of context for object detection and semantic segmentation \\nin the wild, in Proceedings of the IEEE Conference on Computer Vision and Pattern \\nRecognition, 2014, pp. 891898. [15] M. R. Naphade and T. S. Huang, Detecting semantic concepts using context and \\naudiovisual features, in event, 2001, p. 92. [16] G. T. Papadopoulos, V. Mezaris, I. Kompatsiaris, and M. G. Strintzis, Combining \\nmultimodal and temporal contextual information for semantic video analysis, in \\nImage Processing (ICIP), 2009 16th IEEE International Conference on, 2009, pp. \\n43254328. [17] M. Bertini, A. Del Bimbo, and G. Serra, Learning ontology rules for semantic video \\nannotation, in Proceedings of the 2nd ACM workshop on Multimedia semantics, \\n2008, pp. 18. [18] L. Ballan, M. Bertini, A. Del Bimbo, L. Seidenari, and G. Serra, Event detection and \\nrecognition for semantic annotation of video, Multimed. Tools Appl., vol. 51, no. 1, \\npp. 279302, 2011. [19] Z. Gan et al., Semantic compositional networks for visual captioning, in \\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, \\n2017, vol. 2. [20] Y. Pan, T. Yao, H. Li, and T. Mei, Video captioning with transferred semantic \\nattributes, in CVPR, 2017, vol. 2, p. 3. [21] R. Socher, A. Karpathy, Q. V Le, C. D. Manning, and A. Y. Ng, Grounded \\ncompositional semantics for finding and describing images with sentences, Trans. \\nAssoc. Comput. Linguist., vol. 2, no. 1, pp. 207218, 2014. [22] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille, Deep captioning with \\nmultimodal recurrent neural networks (m-rnn), arXiv Prepr. arXiv1412.6632, 2014. [23] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. J. Mooney, T. Darrell, and K. \\nSaenko, Captioning images with diverse objects, in CVPR, 2017, vol. 3, p. 8. [24] J. W. Fisher III, T. Darrell, W. T. Freeman, and P. A. Viola, Learning joint statistical \\nmodels for audio-visual fusion and segregation, in Advances in neural information \\nprocessing systems, 2001, pp. 772778. [25] J. Wagner, F. Lingenfelser, T. Baur, I. Damian, F. Kistler, and E. Andr, The social \\nsignal interpretation (SSI) framework: multimodal signal processing and recognition in \\nreal-time, in Proceedings of the 21st ACM international conference on Multimedia, \\n2013, pp. 831834. [26] T. Perperis et al., Multimodal and ontology-based fusion approaches of audio and \\nvisual processing for violence detection in movies, Expert Syst. Appl., vol. 38, no. 11, \\npp. 1410214116, 2011. xxxiii [27] P. K. Atrey, M. A. Hossain, A. El Saddik, and M. S. Kankanhalli, Multimodal fusion \\nfor multimedia analysis: a survey, Multimed. Syst., vol. 16, no. 6, pp. 345379, 2010. [28] L. Pang and C.-W. Ngo, Mutlimodal learning with deep boltzmann machine for \\nemotion prediction in user generated videos, in Proceedings of the 5th ACM on \\nInternational Conference on Multimedia Retrieval, 2015, pp. 619622. [29] Y. Gwon, W. Campbell, K. Brady, D. Sturim, M. Cha, and H. T. Kung, Multimodal \\nsparse coding for event detection, arXiv Prepr. arXiv1605.05212, 2016. [30] Q. Jin, J. Liang, and X. Lin, Generating Natural Video Descriptions via Multimodal \\nProcessing., in Interspeech, 2016, pp. 570574. [31] J. Shao, C. C. Loy, K. Kang, and X. Wang, Crowded scene understanding by deeply \\nlearned volumetric slices, in IEEE Transactions on circuits and systems for video \\ntechnologiy, VOL. 27, NO. 3, 2017. [32] B. Thomee et al., YFCC100M: The new data in multimedia research, \\narXiv:1503.01817. [33] C.-H. Demarty, C. Penet, M. Soleymani, and G. Gravier, VSD, a public dataset for \\nthe detection of violent scenes in movies: design, annotation, analysis and evaluation, \\nMultimed. Tools Appl., vol. 74, no. 17, pp. 73797404, Sep. 2015. [34] T. Hassner, Y. Itcher, and O. Kliper-Gross, Violent Flows: Real-Time Detection of \\nViolent Crowd Behavior. [35] S. Ali and M. Shah, A Lagrangian Particle Dynamics Approach for Crowd Flow \\nSimulation and Stability Analysis, Proc. IEEE Comput. Soc. Conf. Comput. Vis. \\nPattern Recognit., 2007. [36] B. Castellano, PySceneDetect v0.3.4. 2017.\\n[37] D. L. Chen and W. B. Dolan, Collecting highly parallel data for paraphrase evaluation, in Proceedings of the 49th Annual Meeting of the Association for \\nComputational Linguistics: Human Language Technologies-Volume 1, 2011, pp. \\n190200. [38] A. F. Smeaton, P. Over, and W. Kraaij, Evaluation campaigns and TRECVid, in \\nProceedings of the 8th ACM international workshop on Multimedia information \\nretrieval, 2006, pp. 321330. [39] NIST, TRECVID data availability by year and task. [Online]. Available: \\nhttps://www-nlpir.nist.gov/projects/trecvid/past.data.table.html. [40] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the \\ninception architecture for computer vision, in Proceedings of the IEEE conference on \\ncomputer vision and pattern recognition, 2016, pp. 28182826. xxxiv [41] O. Russakovsky et al., Imagenet large scale visual",
  "updated_at": "8/11/1992 12:00:00 AM",
  "created_at": "9/6/2017 12:00:00 AM"
}