{
  "sdl_source_type": "platform",
  "field_launch_date": "2023-04-24T00:00:00",
  "changed": "2020-08-03T00:00:00",
  "field_platform_contacts": "Romaissa C Mihalchenko| Ilyes G Ellman| Kasie N Lupia√±ez| Noemy B Ainsa| Bree U Benker| Petronela V Lutjen| Cristina G Lastras",
  "field_communities_of_practice": "amicability's| Elisabeth| decidedly| calculi| spindlier| ideology| Orwellian's| mongering",
  "platform_leader_name": "Estafania L Berenjeno",
  "field_banner_subhead": "aerate Monmouth's",
  "platform_url": "https://sunbeam.com",
  "sdl_date": "2020-01-26T00:00:00",
  "countryPublished": "United Kingdom",
  "conference": "reversal offensively hc Wahba",
  "originalAuthorName": "Maheen w Kirchschlager",
  "title": "Merriam's nihilism's McMillan Gould",
  "declaredTags": "communication|location|mobile devices|TRS|Project Story",
  "releaseReason": "defenseless/pshaws",
  "docName": "GP_45_2603",
  "fundingCenter": 26,
  "resourceURL": "https://pickups.com",
  "fundingDepartment": "cl24",
  "caseNumber": "32-3424",
  "publicationDate": "8/27/2018 12:00:00 AM",
  "releaseYear": 2019,
  "releaseStatement": "MITRE External Publication",
  "approver": "$Ping $Jogin",
  "handCarry": 7,
  "authorDivision": "lq94",
  "copyrightOwner": "Juana Morote",
  "lastModifiedDate": "8/20/2008 12:00:00 AM",
  "releaseDate": "4/9/2004 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "9497ANAL38",
  "materialType": "Article",
  "publicationType": "Paper",
  "authorCenter": 52,
  "originalAuthorID": "Walid",
  "mitrePublicServer": 0,
  "subjectTerminology": "Software Engineering (General)",
  "dateEntered": "1/10/2006 12:00:00 AM",
  "documentInfoURL": "https://madder vibrancy end underneath's slipshod.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 18-0469",
  "organization": "ps19",
  "authorDepartment": "xb38",
  "publicationYear": 1991,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "IRS and VA FFRDC Contracts",
  "level1": "MITRE Public Sector",
  "fundingDivision": "Sabine wrench devilled unfrequented torque",
  "publishedOutsideUSA": 0,
  "level3": "yk35",
  "level2": "dl69",
  "sdl_id": "2ed66d8dea244b9f9165c05d2738619e",
  "text": "Week 2 Timing Order \\n(sec) Comb \\nMonitor Week 3 Timing Order (sec)\\nPredictive Caution Predictive Caution ATC 5 45 24 ATC 9 25 15\\nATC 6 45 24 ATC 10 25 15\\nATC 7 45 24 ATC 11 25 15\\nATC 9 45 24 ATC 12 25 15\\nATC 5 35 20 ATC 9 35 20\\nATC 6 35 20 ATC 10 35 20\\nATC 7 35 20 ATC 11 35 20\\nATC 9 35 20 ATC 12 35 20\\nATC 5 25 15 ATC 9 45 24\\nATC 6 25 15 ATC 10 45 24\\nATC 7 25 15 ATC 11 45 24\\nATC 9 25 15 ATC 12 45 24 Day 3 Scenario Matrix: Lateral Deviations (Participants 1-12)5.5.6.6\\nAs described in Section 4.5.5.3, the Lateral Deviations portion of the study focused on \\ndifferences to the Monitor controller with respect to IM Lead Aircraft and IM Trail Aircraft \\ndeviations against the Lateral Boundaries. Two variables were manipulated: Monitor \\nconfiguration (Combined versus Separate) and Display Type (STARS versus FMA 4:1). Four \\nseparate traffic files (termed L, M, N, and O) were developed for the Lateral Deviation scenarios. \\nThey included variations on deviation location (earlier versus later), deviation order (whether \\nType 1 occurred before Type 2, or whether Type 2 occurred before Type 1), and which of the \\nfour IM PA pairings experienced them. The Combined Monitor configuration scenarios were run in the same manner as the Alert \\nTiming scenarios: two participants side-by-side, independent and in parallel (indicated by the \\nshading in the following tables). The Separate Monitor configuration scenarios were run in the \\nsame manner as the Day 1-2 Nominal scenarios. Each scenario lasted approximately 10 minutes. Table 4-13 shows the scenario run order and Independent Variable manipulations for Week 1. \\nThe traffic file used in each scenario is reflected in its name code. Due to time constraints, \\nscenarios 1N, 4O, 1O, and 4N were not run for ATC 1-4 (indicated by the strikethroughs in Table \\n4-13). Table 4-13. Week 1 Lateral Deviation Run Order and Variable Manipulations Scenario \\nName Traffic File Monitor \\nConfig Display Type Monitor Position\\n28L (M) 28R (M) 1L L Combined STARS ATC 1\\n2L L Combined STARS ATC 2 xlii\\n 3L L Combined STARS ATC 3\\n4L L Combined STARS ATC 4\\n1M M Combined FMA 4:1 ATC 1\\n2M M Combined FMA 4:1 ATC 2\\n3M M Combined FMA 4:1 ATC 3\\n4M M Combined FMA 4:1 ATC 4\\n1N N Separate STARS ATC 1 ATC 2\\n2O O Separate STARS ATC 2 ATC 1\\n3N N Separate STARS ATC 3 ATC 4\\n4O O Separate STARS ATC 4 ATC 3\\n1O O Separate FMA 4:1 ATC 1 ATC 2\\n2N N Separate FMA 4:1 ATC 2 ATC 1\\n3O O Separate FMA 4:1 ATC 3 ATC 4\\n4N N Separate FMA 4:1 ATC 4 ATC 3 Table 4-14 and Table 4-15 show the scenario run order and Independent Variable manipulations \\nfor the Week 2 and Week 3 participants, respectively. The traffic file used in each scenario is \\nreflected in its name code. Table 4-14. Week 2 Lateral Deviation Run Order and Variable Manipulations Scenario \\nName Monitor \\nConfig Display Type Monitor Position\\n28L (M) 28R (M) 1N Separate STARS ATC 5 ATC 6\\n3N Separate STARS ATC 7 ATC 8\\n2O Separate STARS ATC 6 ATC 5\\n4O Separate STARS ATC 8 ATC 7\\n1L Combined STARS ATC 5\\n2L Combined STARS ATC 6\\n3L Combined STARS ATC 7\\n4L Combined STARS ATC 8\\n1M Combined FMA 4:1 ATC 5\\n2M Combined FMA 4:1 ATC 6\\n3M Combined FMA 4:1 ATC 7\\n4M Combined FMA 4:1 ATC 8\\n1O Separate FMA 4:1 ATC 5 ATC 6\\n3O Separate FMA 4:1 ATC 7 ATC 8\\n2N Separate FMA 4:1 ATC 6 ATC 5\\n4N Separate FMA 4:1 ATC 8 ATC 7 Table 4-15. Week 3 Lateral Deviation Run Order and Variable Manipulations Scenario \\nName Monitor \\nConfig Display Type Monitor Position\\n28L (M) 28R (M) 1M Combined FMA 4:1 ATC 9 \\n2M Combined FMA 4:1 ATC 10\\n3M Combined FMA 4:1 ATC 11\\n4M Combined FMA 4:1 ATC 12\\n1N Separate FMA 4:1 ATC 9 ATC 10 xliii\\n 3N Separate FMA 4:1 ATC 11 ATC 12\\n2O Separate FMA 4:1 ATC 10 ATC 9 \\n4O Separate FMA 4:1 ATC 12 ATC 11\\n1L Combined STARS ATC 9 \\n2L Combined STARS ATC 10\\n3L Combined STARS ATC 11\\n4L Combined STARS ATC 12\\n1O Separate STARS ATC 9 ATC 10\\n3O Separate STARS ATC 11 ATC 12\\n2N Separate STARS ATC 10 ATC 9 \\n4N Separate STARS ATC 12 ATC 11 Data Collection 5.5.7\\nTwo main methods of data collection were used for this simulation. These were subjective data \\n(i.e., participant questionnaires) and objective data (i.e., system recorded data). In addition, \\nsimulation observers made notes throughout the sessions and a final discussion/debrief was \\nheld at the end of each week. Subjective Data Collection5.5.7.1\\nAs described in Section 4.5.3, the subjective data included questionnaires after each run, each \\nday, and at the end of the simulation. The topics included: workload, acceptability of displays / \\nindividual IM PA Tools, communications and concepts, monitoring configuration, and ideas for \\nimprovements. The individual questionnaires are included in the appendices and are as follows: Day 1-2 Questionnaires\\nDemographics (Appendix B)\\nCombined Monitor Post-Run (Appendix C)\\n28R Final Approach Post-Run (Appendix C)\\n28R Monitor Post-Run (Appendix C)\\n28L Monitor Post-Run (Appendix C)\\nDay 1 End (Appendix D)\\nDay 2 End (Appendix D) Day 3 Questionnaires\\nAlert Timing Post-Run (Appendix E)\\nAlert Timing Final (Appendix E)\\nLateral Deviation Post-Run (Appendix F)\\nLateral Deviation Final (Appendix F)\\nSimulation Final (Day 3 End) (Appendix G)\\nDebrief Questions (Appendix G) Objective Data Collection5.5.7.2\\nObjective data was automatically collected and recorded by the simulation environment after \\neach run. This data included: xliv\\n Aircraft state including position, altitude, heading, speed, etc.\\nIM Clearances provided.\\nIM Trail Aircraft tolerance within Assigned Spacing Goal and location relative to safety \\nlimits.\\nOccurrences of Predictive and Caution Alerts.\\nIM Speed changes, reversals, and increases.\\nTime between IM Speed changes and distance to go.\\nAircraft broken out per longitudinal alert.\\nAircraft broken out relative to Lateral Bound proximity.\\nTimes and occurrences of Push-To-Talk (PTT) clicks.\\nController screen video recordings. This data was filtered and reduced to provide the summary data for analyzing the effect of the \\nprocedures and tools on controller response time to developing separation issues and any \\nactual separation violations. i\\n Results6\\nThis section summarizes the results from the questionnaire subjective data and objective data \\nanalyses. Section 5.1 first discusses the methods for data reduction, analysis and presentation \\nused to convey the results of the subjective and objective data. Results are organized and \\npresented by topic in Sections 5.2 through 5.7. Section 5.8 evaluates the hypotheses described \\nin Section 3.3 in consideration of results across various related metrics. All of the major results \\nare then listed in Section 5.9. Data Analysis Methodology6.1\\nSubjective Data6.1.1 The subjective data analysis methodology and presentation of the subjective (questionnaire) \\ndata are summarized in this section. To reduce the potential for family-wise error (i.e., \\nerroneously finding a significant result due to excessive unplanned comparisons), statistical \\ntests were only performed on subjective results that were specifically used to examine a \\nhypothesis. Therefore, the subjective results reported in Sections 5.2 through 5.7 only include \\ndescriptive statistics and any trends that are inferred are based on the methodology described \\nin Section 5.1.1.3. The statistical analysis results for subjective data are reported separately in \\nthe hypothesis evaluations in Section 5.8, though results are referenced in the individual \\nquestions in the prior sections. Sample Sizes6.1.1.1\\nDespite having a total of 12 controllers participate in the experiment, not all 12 questionnaire \\nresponses were included in every question analysis. This results in different sample size (n) \\nvalues across the various questions. One of the main reasons was that the Nominal and Alert \\nTiming scenarios were modified after Week 1. The Week 1 post-run results could therefore not \\nbe appropriately combined with Week 2-3 results. However, the experimenters felt that the \\nWeek 1 controller participants still received sufficient experience with the IM PA Tools and thus \\ntheir responses were still typically included in the Day End and Final Questionnaires. No changes \\nwere made to the Lateral Deviation scenarios after Week 1; therefore, all controller responses \\nwere included in the analyses for these cases. In addition, as noted in Section 4.5.2, two participants only had Tower experience; on \\nparticipant in Week 1 and one participant in Week 3. The results from these participants were \\nrarely pooled with the TRACON controller responses, unless the experimenters felt it was \\nappropriate to do so. The Tower controller responses are thus usually reported separately. ii\\n Unless otherwise noted, the following rationales were used for the typically occurring n values \\nused for each of the reported statistical calculations. If an individual result does not include a \\nspecific explanation for the n, it falls into one of the below cases. n = 12: All 12 participant controllers responses were included in reported findings.\\nn = 10: Includes responses for participant controllers with TRACON experience across all \\nthree run weeks. It does not include Tower-only responses.\\nn = 7: Due to scenario changes made between Week 1 and Weeks 2-3, it was not always \\nappropriate to include Week 1 TRACON controller responses in the data pool for the \\npost-run questionnaires. Therefore, this case represents only the Week 2-3 TRACON \\ncontroller responses.\\nn = 6, n = 4: For questions where NCT responses may be of particular interest, the four \\nNCT",
  "updated_at": "3/3/1995 12:00:00 AM",
  "created_at": "8/28/1997 12:00:00 AM"
}