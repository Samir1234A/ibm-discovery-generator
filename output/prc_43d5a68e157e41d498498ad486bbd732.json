{
  "sdl_source_type": "PRC",
  "sdl_date": "2020-08-22T00:00:00",
  "countryPublished": "Philippines",
  "conference": "extempore dissolutely hs Abdelhalim",
  "originalAuthorName": "Krassimire n Arets",
  "title": "Tasmanian brownest",
  "declaredTags": "monitoring|HSSEDI|ACE Direct",
  "releaseReason": "winding/heehaw's",
  "docName": "CN_28_7170",
  "fundingCenter": 32,
  "resourceURL": "https://helpmate's.com",
  "fundingDepartment": "ty78",
  "caseNumber": "94-7742",
  "publicationDate": "2/12/2017 12:00:00 AM",
  "releaseYear": 2006,
  "releaseStatement": "Other",
  "approver": "$AÃ±es $Vits",
  "handCarry": 1,
  "authorDivision": "xe78",
  "copyrightOwner": "Sevinch Neureither",
  "lastModifiedDate": "11/18/2010 12:00:00 AM",
  "releaseDate": "4/18/2006 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "3199YLPX30",
  "materialType": "Paper",
  "publicationType": "Article",
  "authorCenter": 42,
  "originalAuthorID": "Elisio",
  "mitrePublicServer": 0,
  "subjectTerminology": "Management (General)",
  "dateEntered": "8/1/2010 12:00:00 AM",
  "documentInfoURL": "https://son's gag lechery's forfeiting displaced.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE3  17-2945|CASE1: 18-0935|CASE1: 16-4753",
  "organization": "jp32",
  "authorDepartment": "ut13",
  "publicationYear": 1998,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "CAASD Non-Direct Project",
  "level1": "Corporate Ops & Transformation",
  "fundingDivision": "unattended equipage's Will publisher's modify",
  "publishedOutsideUSA": 0,
  "level3": "sg15",
  "level2": "bo49",
  "sdl_id": "43d5a68e157e41d498498ad486bbd732",
  "text": "LSTM, which can lead to overfitting. Additionally, \\nembedding the LinkBioMan features on top of the Show and Tell Captioners CNN feature \\nembedding space learned on the MSCOCO dataset may allow the model to better learn \\nrelationships between them.\\nIt should be noted that the scores achieved in Table 4 are lower than the values typically seen on \\nother datasets such as the MSCOCO dataset. This is likely because of the small size of the LBM \\nDataset, which has only 4404 video clips and 1 reference caption per clip. Because most metrics \\nare designed to compare a candidate caption against a group of references, they will tend to give \\nlower scores when only one ground truth reference is provided. We also observed that all \\ncaptioners that were fine-tuned on the LBM Dataset had a strong tendency to produce captions \\nidentical to ones in the training set. Depending on the model and the number of fine-tuning steps, \\n92-99% of captions generated came from the training dataset. This phenomenon of captioning by \\nexemplar was also observed in the original Show and Tell paper [5] where roughly 80% of their \\ntop captions generated came directly from the training dataset. The authors of that paper attributed \\nit to the relatively small size of their training datasets. Given that the LBM Dataset is even smaller, xxv this increased tendency to use training captions as exemplars is to be expected. Captioner: On the YouTube2Text Dataset5.6\\nWe trained and tested the Show and Tell Captioner, concat LinkBioMan and embed & add \\nLinkBioMan on the two configurations of YouTube2Text Dataset: the official \\nYouTube2Text Dataset and YouTube2Text clips with audio. The official YouTube2Text \\nDataset contains 1970 video clips without audio. We split the dataset into 1300 training clips, with \\nthe remaining 670 clips used for testing. Due to the relatively small size of the official \\nYouTube2Text Dataset, we again pre-trained our models on the MSCOCO dataset and then fine-\\ntuned the model on the YouTube2Text Dataset. Similar to the captioner training on the LBM \\nDataset, we maintained the same decaying learning rate between the pre-training and fine-tuning \\nsessions. Additionally, we experimented with 500k and then with 1 million fine-tuning training \\niterations on the YouTube2Text Dataset. We uniformly sampled clips at a rate of 1 frame per 2 \\nseconds for both training and testing. As each clip in the YouTube2Text Dataset contains multiple \\nreference captions, we chose to assign all captions to all clips for training. In total, our training \\ndataset consisted of 343,980 frame-caption pairs. Although the official YouTube2Text Dataset does not contain audio, it is distributed with \\nURLs to the original video clips that contain audio. Therefore, we followed [3] in \\ndownloading the YouTube2Text clips with audio to perform additional experiments. Because \\nsome clips were no longer available, we were able to download 1,620 of the 1,970, which \\namounts to approximately 82% of the official YouTube2Text Dataset size. Despite the \\nreduced dataset size, the proportion of the remaining subsets of the training and testing data \\nroughly matched the proportion of training and testing data in the official YouTube2Text \\nDataset, and thus, there was no need for redistribution between our training and testing \\ndatasets. As noted below, our models trained on the official YouTube2Text Dataset for 500k \\niterations performed better on almost all metrics than those trained on 1 million iterations. \\nTherefore, we chose to train captioner models on the YouTube2Text clips with audio for 500k \\niterations only, following the same training and evaluation procedure as done on the official \\nYouTube2Text Dataset. We also trained the Show and Tell Captioner model on the \\nYouTube2Text clips with audio; however, note that the Show and Tell Captioner does not \\nhandle audio features. For experiments, we normalized the captions by converting all generated words to lower-case \\nand removing any quotation marks from the produced captions. As both concat LinkBioMan \\nand embed & add LinkBioMan are based upon the Show and Tell Captioner [5], each model \\ngenerates one caption per sample, resulting in multiple captions per clip. To determine the \\nfinal caption to be used, we summed up the predicted confidence produced by the model for \\neach unique caption and chose the caption with the largest accumulated confidence. If \\nmultiple captions had the same accumulated confidence, then we would arbitrarily choose \\none caption at random (although this never happened in practice during our experiment). This \\ncaption voting scheme provides a balance in weight between the caption frequency and the xxvi confidence per caption. Because the model tends to copy reference captions from training, \\nmany clips had multiple verbatim duplicate captions. This behavior is not unexpected as \\npreviously explained in Section 5.5. To compare the LinkBioMan Captioner performance with other video captioners \\nperformance on the YouTube2Text Dataset, we used the BLEU [64], METEOR [6] and \\nCIDEr metrics [7]. CIDEr is a text evaluation metric that compares generated captions to an \\nestimated consensus, constructed through weighted n-grams from the ground truth captions. \\nIt replaces ROGUE due to a more thoroughly defined ground truth consensus (multiple \\nground truth captions per video) compared to the LBM Dataset (limited consensus due to 1 \\nground truth caption per video). [3] uses an encoder-decoder architecture fusing auditory, \\nvisual and motion information via an attention model that inputs feedback from the decoder \\nnetwork. [68] uses a recurrent model with multiple encoding rates and learns temporal \\nfeatures using a loss that incorporates the reconstruction of sequences both forward and \\nbackwards in time. [69] encodes local temporal video structure using a 3D-CNN as encoder \\nand uses an attention mechanism to select a subset of frames in order to exploit the global \\nstructure. [19] uses a Semantic Compositional Network, which weighs the decoder with tag-\\ndependent weight matrices and thus encodes semantic concepts. [70] uses two LSTMS, one \\nstacked onto the other, to map a sequence of video frames to a sequence of words that form \\npredicted caption. Table 5 displays our performance on the Youtube2Text Dataset with other video captioners \\nperformance. The last nine rows (in orange and green) contain results from our experiments. \\nIn the Model column, the numbers in parentheses correspond to the number of training \\nsteps fine-tuned on the official YouTube2Text Dataset. We found that both concat \\nLinkBioMan and embed & add LinkBioMan performed worse on the YouTube2Text clips \\nwith audio than on the official YouTube2Text Dataset. This is likely due to audio dubbing \\ncontained in some videos that are unrelated to the content, as investigated by [3]. Note that \\nthe Show and Tell Captioner did not use the audio features when trained on the \\nYouTube2Text clips with audio, and thus did not suffer or rather saw a slight increase in its \\nperformance because the YouTube2Text clips with audio is approximately 82% of the \\nofficial YouTube2Text Dataset. We note that embed & add LinkBioMan performs better than \\nthe concat LinkBioMan for all metrics used in our evaluation. This is similar to our results on \\nthe LBM Dataset, perhaps for the same reasons discussed in Section 5.5. Additionally, both \\nconcat LinkBioMan and embed & add LinkBioMan performed better than the Show and Tell \\nCaptioner for all metrics on the official YouTube2Text Dataset. Furthermore, both the concat \\nLinkBioMan and embed & add LinkBioMan performed better for almost all metrics when \\ntrained on 500k iterations compared to 1 million iterations, possibly because the networks \\nwere beginning to overfit to the training dataset when trained with a larger number of training \\niterations. It is interesting to note that embed & add LinkBioMan (500k) achieved higher \\nBLEU-4, METEOR, and CIDEr scores than [69] and a higher CIDEr score than [3] without \\nthe use of an explicit attention mechanism. Embed & add LinkBioMan (500k) also achieved xxvii a higher METEOR score than [70], suggesting that the embed & add layer might be more \\neffective than the additional LSTM layer. xxviii Table 5: Captioner Performance on the YouTube2Text Dataset and YouTube2Text Clips with Audio The last three rows (green) pertain to LinkBioMan Captioner models (concat, embed & add) on the YouTube2Text clips with audio \\nand the six rows above (orange) on the official YouTube2Text Dataset, with the best score underlined. All other rows display results of \\nvideo captioner models using the official YouTube2Text Dataset, except Attention Fusion (AV). The LinkBioMan Captioner results are \\nbetter without audio because the audio dubbing contained in some clips is unrelated to the content, as investigated by [3]. Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEO\\nR CIDEr Attention Fusion (V) [3] - 0.524 0.320 0.404 Attention Fusion (AV) [3] - 0.539 0.322 0.400 mGRU + pre-train GoogLeNet Features [68] 0.808 0.695 0.600 0.495 0.334 0.755 mGRU + pre-train ResNet-200 Features [68] 0.825 0.722 0.633 0.538 0.345 0.812 Enc-Dec + Local + Global [69] - 0.419 0.296 0.517 SCN-LSTM Ensemble of 5 [19] - 0.511 0.335 0.777 S2VT + RGB (VGG) + Flow (AlexNet) [70] - - 0.298  Show and Tell (500k) [5] 0.775 0.594 0.490 0.388 0.300 0.585 Show and Tell (1M) [5] 0.773 0.589 0.486 0.384 0.297 0.585 concat LinkBioMan (500k) 0.784 0.610 0.504 0.400 0.305 0.638 concat LinkBioMan (1M) 0.780 0.606 0.500 0.397 0.304 0.647 embed",
  "updated_at": "7/6/2010 12:00:00 AM",
  "created_at": "6/10/2011 12:00:00 AM"
}