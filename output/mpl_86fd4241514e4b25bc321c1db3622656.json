{
  "sdl_source_type": "MPL",
  "productName": "leaden standard",
  "uploadedate": "2016-10-24T00:00:00",
  "productUrl": "http://Philip.com",
  "creatorNames": "Jozefa f Csauth;Oscar w Chepik",
  "uploaded": "2014-10-09T00:00:00",
  "sdl_extracted_summary": "Week 2 Timing Order \\n(sec) Comb \\nMonitor Week 3 Timing Order (sec)\\nPredictive Caution Predictive Caution ATC 5 45 24 ATC 9 25 15\\nATC 6 45 24 ATC 10 25 15\\nATC 7 45 24 ATC 11 25 15\\nATC 9 45 24 ATC 12 25 15\\nATC 5 35 20 ATC 9 35 20\\nATC 6 35 20 ATC 10 35 20\\nATC 7 35 20 ATC 11 35 20\\nATC 9 35 20 ATC 12 35 20\\nATC 5 25 15 ATC 9 45 24\\nATC 6 25 15 ATC 10 45 24\\nATC 7 25 15 ATC 11 45 24\\nATC 9 25 15 ATC 12 45 24 Day 3 Scenario Matrix: Lateral Deviations (Participants 1-12)5.5.6.6\\nAs described in Section 4.5.5.3, the Lateral Deviations portion of the study focused on \\ndifferences to the Monitor controller with respect to IM Lead Aircraft and IM Trail Aircraft \\ndeviations against the Lateral Boundaries. Two variables were manipulated: Monitor \\nconfiguration (Combined versus Separate) and Display Type (STARS versus FMA 4:1). Four \\nseparate traffic files (termed L, M, N, and O) were developed for the Lateral Deviation scenarios. \\nThey included variations on deviation location (earlier versus later), deviation order (whether \\nType 1 occurred before Type 2, or whether Type 2 occurred before Type 1), and which of the \\nfour IM PA pairings experienced them. The Combined Monitor configuration scenarios were run in the same manner as the Alert \\nTiming scenarios: two participants side-by-side, independent and in parallel (indicated by the \\nshading in the following tables). The Separate Monitor configuration scenarios were run in the \\nsame manner as the Day 1-2 Nominal scenarios. Each scenario lasted approximately 10 minutes. Table 4-13 shows the scenario run order and Independent Variable manipulations for Week 1. \\nThe traffic file used in each scenario is reflected in its name code. Due to time constraints, \\nscenarios 1N, 4O, 1O, and 4N were not run for ATC 1-4 (indicated by the strikethroughs in Table \\n4-13). Table 4-13. Week 1 Lateral Deviation Run Order and Variable Manipulations Scenario \\nName Traffic File Monitor \\nConfig Display Type Monitor Position\\n28L (M) 28R (M) 1L L Combined STARS ATC 1\\n2L L Combined STARS ATC 2 xlii\\n 3L L Combined STARS ATC 3\\n4L L Combined STARS ATC 4\\n1M M Combined FMA 4:1 ATC 1\\n2M M Combined FMA 4:1 ATC 2\\n3M M Combined FMA 4:1 ATC 3\\n4M M Combined FMA 4:1 ATC 4\\n1N N Separate STARS ATC 1 ATC 2\\n2O O Separate STARS ATC 2 ATC 1\\n3N N Separate STARS ATC 3 ATC 4\\n4O O Separate STARS ATC 4 ATC 3\\n1O O Separate FMA 4:1 ATC 1 ATC 2\\n2N N Separate FMA 4:1 ATC 2 ATC 1\\n3O O Separate FMA 4:1 ATC 3 ATC 4\\n4N N Separate FMA 4:1 ATC 4 ATC 3 Table 4-14 and Table 4-15 show the scenario run order and Independent Variable manipulations \\nfor the Week 2 and Week 3 participants, respectively. The traffic file used in each scenario is \\nreflected in its name code. Table 4-14. Week 2 Lateral Deviation Run Order and Variable Manipulations Scenario \\nName Monitor \\nConfig Display Type Monitor Position\\n28L (M) 28R (M) 1N Separate STARS ATC 5 ATC 6\\n3N Separate STARS ATC 7 ATC 8\\n2O Separate STARS ATC 6 ATC 5\\n4O Separate STARS ATC 8 ATC 7\\n1L Combined STARS ATC 5\\n2L Combined STARS ATC 6\\n3L Combined STARS ATC 7\\n4L Combined STARS ATC 8\\n1M Combined FMA 4:1 ATC 5\\n2M Combined FMA 4:1 ATC 6\\n3M Combined FMA 4:1 ATC 7\\n4M Combined FMA 4:1 ATC 8\\n1O Separate FMA 4:1 ATC 5 ATC 6\\n3O Separate FMA 4:1 ATC 7 ATC 8\\n2N Separate FMA 4:1 ATC 6 ATC 5\\n4N Separate FMA 4:1 ATC 8 ATC 7 Table 4-15. Week 3 Lateral Deviation Run Order and Variable Manipulations Scenario \\nName Monitor \\nConfig Display Type Monitor Position\\n28L (M) 28R (M) 1M Combined FMA 4:1 ATC 9 \\n2M Combined FMA 4:1 ATC 10\\n3M Combined FMA 4:1 ATC 11\\n4M Combined FMA 4:1 ATC 12\\n1N Separate FMA 4:1 ATC 9 ATC 10 xliii\\n 3N Separate FMA 4:1 ATC 11 ATC 12\\n2O Separate FMA 4:1 ATC 10 ATC 9 \\n4O Separate FMA 4:1 ATC 12 ATC 11\\n1L Combined STARS ATC 9 \\n2L Combined STARS ATC 10\\n3L Combined STARS ATC 11\\n4L Combined STARS ATC 12\\n1O Separate STARS ATC 9 ATC 10\\n3O Separate STARS ATC 11 ATC 12\\n2N Separate STARS ATC 10 ATC 9 \\n4N Separate STARS ATC 12 ATC 11 Data Collection 5.5.7\\nTwo main methods of data collection were used for this simulation. These were subjective data \\n(i.e., participant questionnaires) and objective data (i.e., system recorded data). In addition, \\nsimulation observers made notes throughout the sessions and a final discussion/debrief was \\nheld at the end of each week. Subjective Data Collection5.5.7.1\\nAs described in Section 4.5.3, the subjective data included questionnaires after each run, each \\nday, and at the end of the simulation. The topics included: workload, acceptability of displays / \\nindividual IM PA Tools, communications and concepts, monitoring configuration, and ideas for \\nimprovements. The individual questionnaires are included in the appendices and are as follows: Day 1-2 Questionnaires\\nDemographics (Appendix B)\\nCombined Monitor Post-Run (Appendix C)\\n28R Final Approach Post-Run (Appendix C)\\n28R Monitor Post-Run (Appendix C)\\n28L Monitor Post-Run (Appendix C)\\nDay 1 End (Appendix D)\\nDay 2 End (Appendix D) Day 3 Questionnaires\\nAlert Timing Post-Run (Appendix E)\\nAlert Timing Final (Appendix E)\\nLateral Deviation Post-Run (Appendix F)\\nLateral Deviation Final (Appendix F)\\nSimulation Final (Day 3 End) (Appendix G)\\nDebrief Questions (Appendix G) Objective Data Collection5.5.7.2\\nObjective data was automatically collected and recorded by the simulation environment after \\neach run. This data included: xliv\\n Aircraft state including position, altitude, heading, speed, etc.\\nIM Clearances provided.\\nIM Trail Aircraft tolerance within Assigned Spacing Goal and location relative to safety \\nlimits.\\nOccurrences of Predictive and Caution Alerts.\\nIM Speed changes, reversals, and increases.\\nTime between IM Speed changes and distance to go.\\nAircraft broken out per longitudinal alert.\\nAircraft broken out relative to Lateral Bound proximity.\\nTimes and occurrences of Push-To-Talk (PTT) clicks.\\nController screen video recordings. This data was filtered and reduced to provide the summary data for analyzing the effect of the \\nprocedures and tools on controller response time to developing separation issues and any \\nactual separation violations. i\\n Results6\\nThis section summarizes the results from the questionnaire subjective data and objective data \\nanalyses. Section 5.1 first discusses the methods for data reduction, analysis and presentation \\nused to convey the results of the subjective and objective data. Results are organized and \\npresented by topic in Sections 5.2 through 5.7. Section 5.8 evaluates the hypotheses described \\nin Section 3.3 in consideration of results across various related metrics. All of the major results \\nare then listed in Section 5.9. Data Analysis Methodology6.1\\nSubjective Data6.1.1 The subjective data analysis methodology and presentation of the subjective (questionnaire) \\ndata are summarized in this section. To reduce the potential for family-wise error (i.e., \\nerroneously finding a significant result due to excessive unplanned comparisons), statistical \\ntests were only performed on subjective results that were specifically used to examine a \\nhypothesis. Therefore, the subjective results reported in Sections 5.2 through 5.7 only include \\ndescriptive statistics and any trends that are inferred are based on the methodology described \\nin Section 5.1.1.3. The statistical analysis results for subjective data are reported separately in \\nthe hypothesis evaluations in Section 5.8, though results are referenced in the individual \\nquestions in the prior sections. Sample Sizes6.1.1.1\\nDespite having a total of 12 controllers participate in the experiment, not all 12 questionnaire \\nresponses were included in every question analysis. This results in different sample size (n) \\nvalues across the various questions. One of the main reasons was that the Nominal and Alert \\nTiming scenarios were modified after Week 1. The Week 1 post-run results could therefore not \\nbe appropriately combined with Week 2-3 results. However, the experimenters felt that the \\nWeek 1 controller participants still received sufficient experience with the IM PA Tools and thus \\ntheir responses were still typically included in the Day End and Final Questionnaires. No changes \\nwere made to the Lateral Deviation scenarios after Week 1; therefore, all controller responses \\nwere included in the analyses for these cases. In addition, as noted in Section 4.5.2, two participants only had Tower experience; on \\nparticipant in Week 1 and one participant in Week 3. The results from these participants were \\nrarely pooled with the TRACON controller responses, unless the experimenters felt it was \\nappropriate to do so. The Tower controller responses are thus usually reported separately. ii\\n Unless otherwise noted, the following rationales were used for the typically occurring n values \\nused for each of the reported statistical calculations. If an individual result does not include a \\nspecific explanation for the n, it falls into one of the below cases. n = 12: All 12 participant controllers responses were included in reported findings.\\nn = 10: Includes responses for participant controllers with TRACON experience across all \\nthree run weeks. It does not include Tower-only responses.\\nn = 7: Due to scenario changes made between Week 1 and Weeks 2-3, it was not always \\nappropriate to include Week 1 TRACON controller responses in the data pool for the \\npost-run questionnaires. Therefore, this case represents only the Week 2-3 TRACON \\ncontroller responses.\\nn = 6, n = 4: For questions where NCT responses may be of particular interest, the four \\nNCT",
  "sdl_date": "2020-01-13T00:00:00",
  "countryPublished": "Lesotho",
  "conference": "B's lackey ub Iyad",
  "originalAuthorName": "Addaia n Vorjohann",
  "title": "daring's",
  "declaredTags": "analytics|technical and professional activities|wideband phased array antennas|modernized workspaces|self-development",
  "releaseReason": "Desdemona/triumvirate",
  "docName": "VU_58_2905",
  "fundingCenter": 45,
  "resourceURL": "https://snippier.com",
  "fundingDepartment": "qf93",
  "caseNumber": "50-6491",
  "publicationDate": "7/30/2020 12:00:00 AM",
  "releaseYear": 2005,
  "releaseStatement": "Public Collaboration/Benchmarking/Standards Body",
  "approver": "$Garnet $Prat",
  "handCarry": 7,
  "authorDivision": "vo67",
  "copyrightOwner": "Adahy Etayo",
  "lastModifiedDate": "6/13/2009 12:00:00 AM",
  "releaseDate": "2/28/2003 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "6393QSDW33",
  "materialType": "Paper",
  "publicationType": "Article",
  "authorCenter": 60,
  "originalAuthorID": "Cinderella",
  "mitrePublicServer": 0,
  "subjectTerminology": "Systems Engineering (General)",
  "dateEntered": "3/22/2013 12:00:00 AM",
  "documentInfoURL": "https://fissure's signed Romans impenetrably carjacker.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 17-4547|CASE2: 17-2450|CASE1: 14-3139",
  "organization": "mk46",
  "authorDepartment": "nx29",
  "publicationYear": 2019,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "CAMH FFRDC Contracts",
  "level1": "MITRE Legacy",
  "fundingDivision": "Paterson's motorcycled constituent Marc Moorish",
  "publishedOutsideUSA": 0,
  "level3": "mx67",
  "level2": "ut91",
  "sdl_id": "86fd4241514e4b25bc321c1db3622656",
  "text": "the IoT-inclusive SES model. By using such a \\nmodel, one can place a technical consequence in the context needed to understand what, if any, operational \\nconsequence could result from that technical consequence. The concepts provided in previous sections can be combined into a theoretical risk evaluation framework. \\nThe steps in this framework are as follows and shown in Figure 9. 1. IoT Theoretical Model A set of\\nscenarios describing nominal use of\\nan IoT system in the SES hypergraph\\nframework. 2. Model Pruning The SES producing\\nPES and/or CES hypergraphs. 3. Risk Framework The devices\\nand/or functional units in the PES or\\nCES are annotated with their at-\\ntendant set of technical capabilities\\nand nominal walks. The list of unde-\\nsired behaviors are applied to the ca-\\npabilities in these generated walks,\\nleading to a set of potential technical\\nconsequences. 4. Impacts By examining the gener-\\nated technical consequences within\\nthe context of the walks in which they\\nappear, one can derive a set of opera-\\ntional consequences associated with\\nthese walks to be used directly as part\\nof a risk assessment. Figure 9. Combined Risk Evaluation Framework Mittal, Cane, Schmidt, Tufarolo and Harris 8 CONCLUSIONS AND FUTURE WORK IoT modeling is a complex endeavor in itself as the model requires description across multiple perspectives. \\nThis paper presented an IoT theoretical model in the form of a SES hypergraph model. This model includes \\nboth IoT elements as well as the environment surrounding these elements. It presented a device-centric \\nworld-view necessary to understand the expanded impact a device has on the larger technology-oriented \\necosystem in different operational contexts. Having a hypergraph-based methodology is a suitable means \\nto understand the dependencies and interdependencies between the model elements. Hypergraphs, in their \\noriginal flavor, cannot be used for automated analysis due to the inherent computational and mathematical \\ncomplexity. They have to be supported by various axioms that allow analysis on hypergraphs in a tractable \\nmanner. The SES ontological framework provides the needed structured framework to construct \\nhypergraphs in an iterative manner both visually and structurally. The pruning process facilitates the \\nspecialization of a generalized hypergraph to a component-based architecture within the SES framework. \\nWe discussed the IT and the OT aspects of IoT through the fourteen aspects of the conceptual IoT-Inclusive \\nsystem. The proposed model provides a start towards a common set of terms describing the essential \\nelements of the IoT and its environment, creating a better understanding of the overall picture. A significant \\namount of work needs to be done before the theoretical framework described in this paper could be \\npractically employed as part of an actual risk assessment. Future research could expand upon this work to develop a detailed risk calculation methodology, automated \\npruning algorithms, and eventually automated risk assessment and mitigation strategies adaptable to \\nvariations in IoT implementation from both static (design) and dynamic (implementation) perspectives. It \\nshould be cautioned, however, that the path from the general model to automated risk assessment and \\nmitigation strategies is not short. Benefits of this future research include a way to enable improved stability \\nand resilience for IoT enterprises, and an understanding of potential paths to improved situational awareness \\nthat can leverage industrial and commercial applications. ACKNOWLEDGEMENT The Homeland Security Act of 2002 (Section 305 of PL 107-296, as codified in 6 U.S.C. 185), herein \\nreferred to as the Act, authorizes the Secretary of the Department of Homeland Security (DHS), acting \\nthrough the Under Secretary for Science and Technology, to establish one or more federally funded \\nresearch and development centers (FFRDCs) to provide independent analysis of homeland security \\nissues. MITRE Corp. operates the Homeland Security Systems Engineering and Development Institute \\n(HSSEDI) as an FFRDC for DHS under contract HSHQDC-14-D-00006. The HSSEDI FFRDC provides the government with the necessary systems engineering and development \\nexpertise to conduct complex acquisition planning and development; concept exploration, \\nexperimentation and evaluation; information technology, communications and cyber security processes, \\nstandards, methodologies and protocols; systems architecture and integration; quality and performance \\nreview, best practices and performance measures and metrics; and, independent test and evaluation \\nactivities. The HSSEDI FFRDC also works with and supports other federal, state, local, tribal, public and \\nprivate sector organizations that make up the homeland security enterprise. The HSSEDI FFRDCs \\nresearch is undertaken by mutual consent with DHS and is organized as a set of discrete tasks. This \\nreport presents the results of research and analysis conducted under: \\nTask Order Number: 43161204, \\nTask Title: HSHQDC-16-J-00526:Core Research Program, Internet of Things (IoT) Modeling \\nTask Order Sponsor: Department of Homeland Security, National Protection and Programs Directorate \\nPurpose statement: The purpose of this research was to develop an IoT theoretical model, and describe how such a \\nmodel could be used to perform risk assessments in IoT-centric systems. The results presented in this report do not necessarily reflect official DHS opinion or policy. Approved for Public Release; Distribution Unlimited. Case Number 18-1212 / DHS reference number 17-J-00100-01 Mittal, Cane, Schmidt, Tufarolo and Harris REFERENCES Berge, C. 1973. Hypergraphs. North-Holland, Amsterdam: American Elsevier Pub. Co. \\nBretto, A. 2013. Hypergraph Theory. Switzerland: Mathematical Engineering DOI:10.1007/978-3-319- 00080-0_2 Springer. \\nCASAGRAS. n.d. \\\"RFID and the Inclusive Model for the Internet of Things.\\\" www.rfidglobal.eu. Accessed December 28, 2016. http://www.rfidglobal.eu/userfiles/documents/FinalReport.pdf. \\nCERP-IoT. n.d. \\\"Internet of Things: Strategic Research Roadmap.\\\" www.grifs-project.eu. Accessed December 28, 2016. http://www.grifs-project.eu/data/File/CERP-IoT%20SRA_IoT_v11.pdf. \\nCIO, DoD. 2013. The DoDAF ARchitecture Framework Version 2.02. DoD CIO. \\nCisco. n.d. \\\"The Internet of Things How the Next Evolution of the Internet is Changing Everything.\\\" CISCO IBSG. Accessed December 28, 2016. \\nhttp://www.cisco.com/web/about/ac79/docs/innov/IoT_IBSG_0411FINAL.pdf. Coley, S. 2014. Common Weakness Scoring System (CWSS). MITRE. \\nhttps://cwe.mitre.org/cwss/cwss_v1.0.1.html. Domingue, J., D. Fensel, and P. Traverso. 2008. Future Internet FIS 2008: First Future Internet \\nSymposium. Vienna, Austria: Springer. First.org. n.d. Common Vulnerability Scoring System v3.0: Specification Document. First.org. \\nhttps://www.first.org/cvss/specification-document. Gallo, G., G. Longo, and S. Pallottino. 1993. \\\"Directed hypergraphs and applications.\\\" Discrete Applied \\nMathematics (North-Holland, Elsevier Science Publishers) 42: 177-201. Group, The Open Architecture. n.d. TOGAF 9.1 Enterprise Edition. https://www.opengroup.org/togaf/. \\nGroup, UK Future Internet Strategy. 2011. \\\"Future Internet Report.\\\" https://connect.innovateuk.org. May. Accessed December 28, 2016. \\nhttps://connect.innovateuk.org/documents/ 595/Future+Internet+report.pdf. Haller, Stephen. n.d. \\\"Internet of Things: An Integral Part of the Future Internet.\\\" services.future-\\ninternet.eu. Accessed December 28, 2016. http://services.future-\\ninternet.eu/images/1/16/A4_Things_Haller.pdf. Huang Y. and G. Li. 2010. \\\"Descriptive Model for Internet of Things,\\\" in International Conference on \\nIntelligent Control and Infomation Processing, Dalian, China, Lee, G.M., J. Park, N. Kong, and N. Crespi. 2011. IETF-The Internet of Things Concepts and Problem \\nStatement. Internet Draft, IETF. Mckinsey. n.d. \\\"The Internet of Things.\\\" www.mckinseyquarterly.com. Accessed December 28, 2016. \\nhttps://www.mckinseyquarterly.com/High_Tech/Hardware/The_Internet_of_Things_2538. Mittal, S., and J.L.R Martin. 2013. Netcentric System of Systems Engineering with DEVS Unified Process. \\nBoca Raton, FL: CRC Press. Molnar, B. 2014. \\\"Applications of hypergraphs in informatics: a survey and opportunities for research.\\\" \\nAnn. Univ. Sci. Budapest. Sect. Comput. 42: 261-282. Newman, M.E.J. 2010. Networks, An Introduction. Oxford: Oxford University Press. \\nNIC, National Intelligence Council. n.d. \\\"Disruptive Technologies Global Trends 2025.\\\" www.fas.org. Accessed December 28, 2016. http://www.fas.org/irp/nic/disruptive.pdf. \\nPhilippe Jgou a, Samba Ndojh Ndiaye. 2009. \\\"On the notion of cycles in hypergraphs.\\\" Discrete Mathematics 309 (23-24): 65356543. Accessed Jan 17, 2017. \\nhttp://www.sciencedirect.com/science/article/pii/S 3446?np=y. SIG, IoT. 2013. Internet of Things (IoT) and Machine to Machine Communications (M2M)Challenges and \\nopportunities: Final paper May 2013. Technology Strategy Board IoT Special Interest Group. Sundmaeker, H., P., Friess, P. Guillemin, and S. Woelffle. 2010. Vision and Challenges for Realizing the \\nInternet of Things. European Research Project, Brussels: CERP-IoT. Voas, J. 2016. Network of 'Things'. NIST Special Publication 800-183. Approved for Public Release; Distribution Unlimited. Case Number 18-1212 / DHS reference number 17-J-00100-01 Mittal, Cane, Schmidt, Tufarolo and Harris Approved for Public Release; Distribution Unlimited. Case Number 18-1212 / DHS reference number 17-J-00100-01 Zachman, J. n.d. A Concise Definition of Zachman Framework. Zachman International. \\nhttps://www.zachman.com/about-the-zachman-framework. Zeigler, B.P. 1984. Multifaceted modeling and discrete event simulation. London, UK: Academic Press. \\nZeigler, B.P., and G. Zhang. 1989. \\\"The system entity structure: knowledge representation for simulation modeling and design.\\\" In Artificial Intelligence, Simulation and Modeling, by L. Widman and N. \\nNielseen K Loparo, 47-73. Hoboken, NJ: John Wiley. Zeigler, B.P., and P.E. Hammonds. 2007. Modeling and Simulation-based Data Engineering: Introducing \\nPragmatics into Ontologies for Net-centric Information Exchange. Academic Press. Zeigler, B.P., H. Praehofer, and T.G. Kim. 2000. Theory of Modeling and Simulation: Integrating discrete \\nevent and continuous complex dynamical systems. Academic Press. Zhou, D., J. Huang, and B. Schoelkopf. 2006. \\\"Learning with hypergraphs: Clustering,.\\\" Advances in \\nNeural Information Processing Systems. Zimmerman, Rae, and Carlos E. Restrepo. 2009. \\\"Analyzing Cascading Effects within Infrastructure \\nSectors for Consequence Reduction.\\\" IEEE International Conference on Technologies for Homeland \\nSecurity. Waltham. \\nhttp://research.create.usc.edu/cgi/viewcontent.cgi?article=1146&context=nonpublished_reports. AUTHOR BIOGRAPHIES SAURABH MITTAL is a lead systems engineer/scientist at Modeling, Simulation, Experimenation and \\nAnalytics Tech Center, MITRE. He has over 15 years experience in modeling and simulation of \\ncomplex systems and has co-authored 3 books and over 80 publications. He has M.S and Ph.D. in \\nElectrical and Computer Engineering from the University of Arizona, Tucson. His email is \\nsmittal@mitre.org. SHEILA A. CANE is Strategic Technical Advisor at MITRE. She has over 30 years' experience in system \\nengineering, architecture analysis, and applications of queueing and graph theory. She holds a B.S. in \\nApplied Mathematics from Buffalo State College, a M.S. in Industrial Engineering from SUNY \\nBuffalo, and a D.B.A. in Information Systems",
  "updated_at": "11/23/2005 12:00:00 AM",
  "created_at": "4/29/2003 12:00:00 AM"
}