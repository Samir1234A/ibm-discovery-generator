{
  "sdl_source_type": "MPL",
  "productName": "retirement's plaintive",
  "uploadedate": "2019-04-09T00:00:00",
  "productUrl": "http://sorority.com",
  "creatorNames": "Werner j Pain;Jinchai d Vyjletsov",
  "uploaded": "2018-02-06T00:00:00",
  "sdl_extracted_summary": "responses. The majority (80%) of TRACON controllers agreed (M=76.5; SD=19.8) that they were \\ncomfortable allowing an IM Trail Aircraft to manage its own speed to achieve the desired \\nspacing goal at the FAF. Two controllers neither agreed nor disagreed. From the open-ended comments, at least three of the lower ratings appeared to be a result of \\ncontrollers not being allowed to manually keep the IM Trail Aircraft between the limits when it \\nbecame apparent that a Caution Alert was imminent. xii\\n Traffic Awareness6.2.4\\nAt the end of Day 1 and Day 2, controllers were asked about the acceptability of their overall \\nlevel of traffic awareness with respect to the IM Lead Aircraft, IM Trail Aircraft, and Other \\nAircraft. Their responses were examined with respect to monitor configuration. Response \\nMeans and Standard Deviations are summarized in Table 5-6. Scale responses are shown in \\nFigure 5-7. Only Week 2-3 TRACON controller responses were included in the analysis. Table 5-6. Controller Responses to Traffic Awareness Acceptability: Aircraft Type Monitor Configuration Separate Combined\\nIM Lead Aircraft Sample Size (n) 7 7 Mean (M) 95.4 90.9 Standard Deviation (SD) 4.7 8.2 IM Trail Aircraft\\nSample Size (n) 7 7 Mean (M) 83.7 87.3 Standard Deviation (SD) 18.7 15.3 Other Aircraft\\nSample Size (n) 7 7 Mean (M) 82.9 88.0 Standard Deviation (SD) 23.5 8.4 Figure 5-7. My overall level of traffic awareness today was acceptable with respect to [IM Lead Aircraft \\n IM Trail Aircraft / Other Aircraft]. xiii\\n Note: o indicates Week 1 TRACON controller responses for reference. These are not included in the M and SD computations. All Week 2-3 TRACON controllers agreed their overall level of traffic awareness was acceptable \\nwith respect to all aircraft types for both monitor configurations, except for Separate Monitors / \\nOther Aircraft. In this case, the one participant that disagreed noted in the open-ended \\ncomments: Difficult to determine distance between successive lead aircraft. This participant \\nhas TRACON experience, though is currently assigned to a Tower position. This may help explain \\nthe lower rating. The overall results do not suggest any apparent differences in acceptability by monitor \\nconfiguration among aircraft types. However, Lead Aircraft response variability for the IM Lead \\nAircraft appeared to be lower for the Separate Monitor configuration than for all the other \\ncases. Other open-ended comments for this question included: (Non-NCT TRACON / Separate): Trails do not need to be monitored as closely because \\nthere is automation to ensure separation. Focus can be spent elsewhere.\\n(Non-NCT TRACON / Combined): Definitely was more aware of the trail aircraft because \\nof the new procedures. But spacing required more attention to the lead aircraft \\nfollowing other pairs.\\n(Non-NCT TRACON / Combined): As the day went on, I focused more on distance \\nbetween pairs as opposed to the aircraft in the PA. This was easy because of the alerts \\nand warnings. Separation6.2.5\\nThis section summarizes the subjective questionnaire data with respect to controller confidence \\nof ensuring separation within the IM PA pairs. It also includes the objective data analysis with \\nrespect to observed separation violations within and between the IM PA pairs on the arrivals. Subjective Data Assessment6.2.5.1\\nAt the end of Day 2 (QT3), after experiencing the nominal scenarios across all IM PA Tool and \\nmonitor configurations, controllers were asked: overall, I was confident that I could assess \\nwhether the separation between the IM Trail Aircraft and their Lead Aircraft would be \\nmaintained. Responses are shown for all TRACON (NCT and non-NCT combined) and Tower \\nOnly. Response Means and Standard Deviations are summarized in Table 5-7. Scale responses \\nare shown in Figure 5-8. Table 5-7. Monitor Controller Responses to Confidence in Assessing Separation between IM Trail and \\ntheir Lead Aircraft Participant Experience TRACON Tower\\nSample Size (n) 10 2 xiv\\n Mean (M) 77.9 75.5 Standard Deviation (SD) 21.1  Figure 5-8. Overall, I was confident that I could assess whether the separation between the IM Trail \\nAircraft and their Lead Aircraft would be maintained. Note: o indicates NCT Controller responses. When asked if they could assess whether the separation between the IM Trail Aircraft and their \\nLead Aircraft would be maintained, the majority (80%) of TRACON controllers agreed (M=77.9; \\nSD=21.1). One TRACON controller disagreed and commented: Need CSL & WSL sooner. It is \\nunclear why CSL was noted since it was present at the start of the IM PA operation. Though not \\nshown, further analysis suggests the subjective separation assessment did not appear to be \\naffected by varying the alert timing. Another open-ended comment from a TRACON controller noted: Relying on the automation. \\nWithout it, I could not at this proximity. Objective Data Assessment6.2.5.2\\nAs noted in Section 4.5.7.2, the simulation recorded aircraft state data and PTT click data for \\neach scenario. This data was then post-processed to determine if any losses of separation \\noccurred and if so, whether monitor configuration or IM PA Tool configuration had any effect. \\nThis data only includes the Nominal and Alert Timing scenarios for Weeks 2-3 due to scenario \\nchanges after Week 1. The Day 3 Lateral Deviation scenarios were not included in the post-\\nprocessing analysis due to their complexity. There was one controller in Week 3 who had \\nconsiderable experience at a major tower, but no TRACON experience. This participants \\nseparation data is not included and 21 runs (across seven controller participants) were \\nevaluated for the Day 1-2 Nominal scenarios. The same number of runs, 21, were also evaluated \\nfor the Day 3 Alert Timing scenarios. Separation was considered within and between IM PA pairs. Within IM PA Pairs. Here, separation was considered lost if a trail aircraft crossed a CSL \\nor WSL boundary before a controller contacted it to provide breakout instructions. \\nContact was determined to have occurred if there was a PTT click between the time of a \\nCaution Alert and the aircraft crossing either safety limit. xv\\n Between Pairs. Here, separation was defined per the minima as described in Section \\n4.3.3. If the spacing between any non-IM PA combination of aircraft went below than \\nthe Wake Turbulence or MRS minima, it was considered a separation violation. The post-processing involved an automated script that examined the traffic data and flagged \\ninstances where the above separation definitions were violated. Each resulting case was then \\nverified manually against the controller display video capture videos and/or other MITRE \\nvisualization tools. MITRE controller subject matter expert (SME) input was used to make a final \\ndetermination of the instances that should count as violations. Across all the scenarios, the \\nnumber of separation violations that were observed is summarized in Table 5-8: Table 5-8. Number of Observed Separation Violations (Weeks 2-3) Separation Violations Within IM PA \\nPairs Between \\nPairs Day 1-2 Nominal 0 10 Day 3 Alert Timing 1 2 Within IM PA Pair Separation Violations For the seven TRACON Week 2-3 participants, there were zero IM PA separation violations \\nobserved for the 21 evaluated Day 1-2 Nominal scenarios. For the 21 evaluated Week 2-3, Day 3 Alert Timing scenarios, 5 cases were observed in which an \\nIM Trail Aircraft crossed a safety limit line while it was still being displayed. These may have \\nbeen cases, however, where the controller prioritized communicating a break out instruction to \\nthe aircraft before terminating IM PA in the automation. A controller PTT click was observed \\nbetween the time of the Caution Alert and the time of the crossing. As described in Section \\n5.1.2, due to the lack of override capability, it was also possible that the controller was blocked \\nand therefore the break out instruction could not be provided to the IM Trail Aircraft before IM \\nPA was terminated and the safety limit line was crossed. These cases were therefore not \\ncounted as violations as the controller may have in effect terminated the operation before the \\nline was crossed. Further analysis was performed to examine how soon the PTT click happened \\nin these cases before the crossing occurred. The results are shown in Table 5-9. Table 5-9. Day 3 Alert Timing Safety Limit Exceedances with PTT Response (Weeks 2-3) Case Alert Timing Safety Limit Crossed Time Between \\nPreceding PTT click and Crossing\\n1 25/15 WSL 8 sec\\n2 25/15 CSL 11 sec\\n3 25/15 CSL 9 sec\\n4 25/15 CSL 17 sec xvi\\n 5 25/15 WSL 9 sec M 10.8 sec\\nSD 3.6 sec Only a single separation violation case was observed for the 21 evaluated Week 2-3, Day 3 Alert \\nTiming scenarios, and it involved the 25/15 sec alert timing. In this scenario, an IM Trail Aircraft, \\nAAL2435, was deliberately designed to maintain a faster-than-normal speed on the approach so \\nthat the IM PA alerts would be triggered relative to its CSL. Shortly after the Predictive Alert was \\ndisplayed, the controller issued a 30 kt speed reduction to this aircraft. Per the instructions to \\nthe participants, the controller should then have terminated IM PA and broken the aircraft out. \\nHowever, the controller elected to keep the IM PA active and watch the situation. The aircraft \\nbegan to implement the 30 kt speed reduction but due to a simulation artifact, did not reduce \\nfurther to its final approach speed after the FAF. A",
  "sdl_date": "2020-08-10T00:00:00",
  "countryPublished": "Algeria",
  "conference": "Nicola putrefaction's wc Vaska",
  "originalAuthorName": "Bozena x Uryus",
  "title": "reconnaissance's road's derive",
  "declaredTags": "Ethereum|Russian cyber planners|warfighters|wideband phased array antennas",
  "releaseReason": "Taiwanese/diphthongs",
  "docName": "KV_74_4383",
  "fundingCenter": 57,
  "resourceURL": "https://Geo.com",
  "fundingDepartment": "wc30",
  "caseNumber": "45-4330",
  "publicationDate": "4/22/2019 12:00:00 AM",
  "releaseYear": 2000,
  "releaseStatement": "Academic Program Submission",
  "approver": "$Kia $Steward",
  "handCarry": 3,
  "authorDivision": "al53",
  "copyrightOwner": "Nu Ziliani",
  "lastModifiedDate": "1/30/2014 12:00:00 AM",
  "releaseDate": "6/3/2014 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "9655SVUC96",
  "materialType": "Paper",
  "publicationType": "Paper",
  "authorCenter": 84,
  "originalAuthorID": "Juvenal",
  "mitrePublicServer": 0,
  "subjectTerminology": "Software Engineering (General)",
  "dateEntered": "3/26/2004 12:00:00 AM",
  "documentInfoURL": "https://straitened brood selections pterodactyl birdcage.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 13-0202|CASE1: 10-4072",
  "organization": "vp21",
  "authorDepartment": "qd18",
  "publicationYear": 2002,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "FAA FFRDC Contracts",
  "level1": "MITRE National Security Sector",
  "fundingDivision": "downturns elusiveness's Taoism toucan's reaps",
  "publishedOutsideUSA": 0,
  "level3": "hb78",
  "level2": "av24",
  "sdl_id": "6f618fe28e774a7486ce62af4d75c721",
  "text": "T6-R1 Over the course of the three-day data collection days, the controllers experienced traffic file 4\\nwith en route initiation, while pilots were trained. For the remainder of the day, controllers\\nexperienced the other six traffic files with the flight deck participants. The block order of the\\ntraffic files was counter-balanced across participant groups. Off-nominal events were introduced for the controllers through pseudo-pilot action. Each day\\neach controller experienced an event where the termination or suspension of IM was required.\\nThe pseudo-pilot was told to acknowledge the IM clearance from the feeder controller but to\\nfly a constant speed without engaging IM. The trail aircraft held its speed and started\\nencroaching upon the lead aircraft (which eventually led to a separation issue) until the\\ncontroller intervened. The issue started to evolve in the feeder controllers airspace, but the\\nspacing issue may not have been fully realized until the final controllers airspace based on the\\nslow progression of the overtake. Table 3-6 shows the traffic file run order and traffic overtake\\nevents for the three groups of controllers. 3-51 Table 3-6. Controller Independent Variable Exposure by Day, Traffic File, and Run Note: Traffic overtake conditions are highlighted in orange. Group Operation IM Tools A Controller Role /B Controller Role\\nBaseline NA Feeder/Final D1-T4-R1\\nBaseline NA Final/Feeder D1-T4-R2\\nIM En Route Initiation TSAS tools Feeder/Feeder D4-T4-RC1\\nIM En Route Initiation TSAS tools and slot marker color change Feeder/Feeder D3-T4-RC1\\nIM En Route Initiation TSAS tools, slot marker color change, and ETA differential Feeder/Feeder D5-T4-RC1\\nIM TSAS tools Feeder/Final D3-T4-R1 D4-T5-R1 D5-T6-R1\\nIM TSAS tools and slot marker color change Feeder/Final D3-T1-R2 D4-T6-R2 D5-T4-R2\\nIM TSAS tools, slot marker color change, and ETA differential Feeder/Final D3-T2-R3 D4-T4-R3 D5-T1-R3\\nIM TSAS tools Final/Feeder D3-T3-R4 D4-T1-R4 D5-T2-R4\\nIM TSAS tools and slot marker color change Final/Feeder D3-T5-R5 D4-T2-R5 D5-T3-R5\\nIM TSAS tools, slot marker color change, and ETA differential Final/Feeder D3-T6-R6 D4-T3-R6 D5-T5-R6\\nBaseline NA Feeder/Final D1-T4-R1\\nBaseline NA Final/Feeder D1-T4-R2\\nIM En Route Initiation TSAS tools Feeder/Feeder D3-T4-RC1\\nIM En Route Initiation TSAS tools and slot marker color change Feeder/Feeder D5-T4-RC1\\nIM En Route Initiation TSAS tools, slot marker color change, and ETA differential Feeder/Feeder D4-T4-RC1\\nIM TSAS tools Feeder/Final D3-T4-R2 D4-T1-R2 D5-T6-R2\\nIM TSAS tools and slot marker color change Feeder/Final D3-T1-R3 D4-T2-R3 D5-T4-R3\\nIM TSAS tools, slot marker color change, and ETA differential Feeder/Final D3-T6-R1 D4-T4-R1 D5-T5-R1\\nIM TSAS tools Final/Feeder D3-T3-R5 D4-T5-R5 D5-T2-R5\\nIM TSAS tools and slot marker color change Final/Feeder D3-T5-R6 D4-T6-R6 D5-T3-R6\\nIM TSAS tools, slot marker color change, and ETA differential Final/Feeder D3-T2-R4 D4-T3-R4 D5-T1-R4\\nBaseline NA Feeder/Final D1-T4-R1\\nBaseline NA Final/Feeder D1-T4-R2\\nIM En Route Initiation TSAS tools Feeder/Feeder D5-T4-RC1\\nIM En Route Initiation TSAS tools and slot marker color change Feeder/Feeder D4-T4-RC1\\nIM En Route Initiation TSAS tools, slot marker color change, and ETA differential Feeder/Feeder D3-T4-RC1\\nIM TSAS tools Feeder/Final D3-T4-R3 D4-T1-R3 D5-T2-R3\\nIM TSAS tools and slot marker color change Feeder/Final D3-T5-R1 D4-T6-R1 D5-T4-R1\\nIM TSAS tools, slot marker color change, and ETA differential Feeder/Final D3-T6-R2 D4-T4-R2 D5-T1-R2\\nIM TSAS tools Final/Feeder D3-T3-R6 D4-T5-R6 D5-T6-R6\\nIM TSAS tools and slot marker color change Final/Feeder D3-T1-R4 D4-T2-R4 D5-T3-R4\\nIM TSAS tools, slot marker color change, and ETA differential Final/Feeder D3-T2-R5 D4-T3-R5 D5-T5-R5 (D)ay-(T)raffic File-(R)un Order 1 2 3 3-52 A summary of each data collection day (1 day for pilots; 3 days for controllers) is provided in\\nTable 3-7. Table 3-7. Data Collection Day Details Traffic\\nFile(s) Flight Crew (2 per day) Controllers (3 per day) 4\\n(with en route\\ninitiation) Introductory Briefing and Background\\nQuestionnaire. Training in lab Controller role:\\nA Feeder; B- Feeder\\nScenario:\\nTSAS and IM en route initiation\\nIndependent variable (order varied between days):\\n(1) Basic,\\n(2) Basic+ cue, or\\n(3) Basic+ cue and prediction 1, 2, 3, 4,\\n5, and 6 Pilot role (remained fixed):\\nPF; PM\\nOperation (order varied within day):\\n(1) Baseline No IM (x2 runs)\\n(2) IM (x10 runs)\\nTools (order varied within day):\\nMin CDTI then Min CDTI+ or\\nMin CDTI+ then Min CDTI Controller role (swapped after 3rd scenario):\\nFeeder; Final\\nOperation (traffic file order varied within day):\\nTSAS and IM (x6)\\nTools (order varied within day):\\n(1) Basic,\\n(2) Basic+ cue, or\\n(3) Basic+ cue and prediction 3.6 Data Collection\\nFour methods of data collection were used for this simulation: paper questionnaires, system\\nrecorded data, observations, and final debriefs (when chosen by the participants). Three types\\nof questionnaires were used, including: 1. Demographics: Upon arrival, participants were asked to fill out a demographics\\nquestionnaire which captured participants experience. Controllers and pilots had\\nseparate questionnaires. 2. Post- scenario: After each run / scenario, participants were asked to fill out a\\nquestionnaire based on the run / scenario just experienced. All post-scenario\\nquestionnaires included the Bedford Workload Rating Scale (Roscoe, 1984) along with\\nadditional rating scale and yes / no questions with a comment field for each. The\\ncontroller questionnaires included the Controller Acceptance Rating Scale (Lee, Kerns,\\nBone, and Nickelson, 2001). Pilot participants completed a post-scenario questionnaire\\nafter each run during a scenario (two runs per scenario) and controllers completed this\\nquestionnaire after each scenario. Separate post-scenario questionnaires were used for\\nthe baseline and IM scenarios. Controllers and pilots also had separate questionnaires\\n(Appendix B). 3. Post-simulation: After the final scenario, participants were asked to complete the longer,\\nfinal questionnaire covering all the scenarios experienced. The questionnaire included a\\nseries of rating-scale and yes / no questions with a comment field for each. Controllers\\nand pilots had separate questionnaires (Appendix C). 3-53 In these questionnaires, participants were asked to provide subjective feedback on areas such\\nas the overall IM concept, workload, situation awareness, head down / scan time, displays,\\ncommunications, and simulation realism. Objective metric data was automatically recorded by the simulation platform or by the\\nobservers and included: ATC o Interactions with displays Inputs for IM initiation, rejection, suspension, resumption, and termination o IM initiation delay o Location of IM initiations, rejections, suspensions, resumptions, and terminations All aircraft o Schedule conformance o Slot marker deviation o Events below the applicable separation standard o Frequency of infeasible / no speed events o Time on the RNAV procedure o Spacing error at key locations o How well the ASG was maintained o Arrival rates / throughput Participant aircraft o IM speeds o MCP selected speed o Distance to ABP o Frequency of IM terminations o Interactions with displays (e.g., TTF selection and data entry) 4-1 4 Results\\nThis section begins with a description of the analysis method for both subjective and objective\\ndata. It then describes baseline scenarios and the operations (terminal metering and RNP RF\\nturns) that form the operational foundation for IM. It then covers the objective data, including:\\nthe conduct of IM operations (mainly controller actions related to IM), IM speeds (for\\nparticipant pilot aircraft) and flight crew actions related to those speeds, and aircraft spacing\\nand separation results (for pseudo-pilot and participant pilot aircraft). Subjective data for both\\npilots and controllers is covered next (e.g., acceptability of IM, displays, responsibilities). Time\\non RNAV arrivals and communication results are then provided, followed by en route IM\\ninitiation results, as related to controllers. Finally, the section ends with results for the\\nparticipants assessments of the simulation. 4.1 Analysis Method 4.1.1 Subjective Data\\nThe subjective results are based on responses to the statements from both the post-scenario\\nand post-simulation questionnaires. The post-simulation questionnaires comprise most of the\\ndata so in these cases, the source will not be noted unless it helps for clarity. Any data from the\\npost-scenario questionnaires will be noted. Controller and pilot comments were included if they\\nwere enlightening or if a sufficient number of participants made similar comments. Controller results are based on nine participants while pilot results are based on 18\\nparticipants. Controller responses are divided by the independent variable of controller role.\\nPilot responses are usually combined (as role was not a planned independent variable), unless\\nthere was a clear reason to report the roles separately. Some questions in the questionnaires were yes / no with an opportunity for open-ended\\ncomments. Most response-scale items were statements with 100 hash marks (without numeric\\nlabels) and an opportunity to provide open-ended comments. The scale was anchored on the\\nleft with the label Strongly Disagree and on the right with the label Strongly Agree (Figure\\n4-1). Figure 4-1. 100-Point Agreement Scale Most items were presented as a statement, and participants were asked to rate their level of\\nagreement. Participants were told to draw a straight line anywhere on the scale, including\\nbetween the lines and right on the end points. During data reduction, responses were rounded\\nto the nearest single digit between 0 and 100. In the presentation of the results, any responses\\nbelow the midpoint (i.e., lower than 50) on the scale were considered to be on the disagree\\nside while any responses above the midpoint (i.e., higher than 50) on the scale were considered 4-2 to be on the agree side. Any responses at the midpoint (i.e., equal to 50) were considered to\\nbe neutral (Figure 4-2). Figure 4-2. 100-Point Agreement Scale Agreement Rating Breakdown When presenting results on the 100-point agreement scale in the post-simulation\\nquestionnaires, the following terminology / methodology is used to describe the levels of\\nagreement. All [controllers / pilots] [agreed / disagreed] o All of the participants are on the agree or disagree side of the scale The majority (n; %) of [controllers / pilots] [agreed / disagreed] o Low variability, e.g., SD of less than approximately 25 (unless one value is driving a\\nSD slightly higher) [Controller / Pilot] responses were variable but the majority (n; %) [agreed / disagreed] o Responses have a SD of greater than approximately 25 and distribution is relatively\\nbiased in",
  "updated_at": "4/8/2006 12:00:00 AM",
  "created_at": "8/23/2017 12:00:00 AM"
}