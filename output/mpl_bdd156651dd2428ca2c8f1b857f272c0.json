{
  "sdl_source_type": "MPL",
  "productName": "emptiness's faggot",
  "uploadedate": "2011-01-08T00:00:00",
  "productUrl": "http://disaffected.com",
  "creatorNames": "Mamudou l Kurtze;Ianos i Boden;Arayik b Onali;Sultan d Amjad",
  "uploaded": "2019-09-04T00:00:00",
  "sdl_extracted_summary": "and consistent with the psychological literature: depression has been shown to cause people to literally see the world through a colorless lens, and reduce their social activity. So while the results are not necessarily surprising, what is remarkable is that they hold even when we restrict model training to pictures posted prior to the date of diagnosis, demonstrating predictive skill. Furthermore, the model outperformed general practitioners average unassisted diagnostic success rate for depression. 9 Predicting Depression and PTSD from Twitter | # | \\n \\nIn a companion study, we investigated predictors of depression & PTSD using twitter messages. Linguistic style, sentiment, and meta-data were used to train supervised learning algorithms. Resulting models successfully discriminated between depressed and healthy content, and compared favorably to general practitioners average success rates. Again, the results held even when the analysis was restricted to content posted before first depression diagnosis, suggesting that onset of depression and PTSD may be detectable several months prior to formal diagnosis. Machine learning models. We trained supervised machine learning classifiers to discriminate between affected and healthy sample members observations. Classifiers were trained on a randomly-selected 70% of total observations, and tested on the remaining 30%. Out of several candidate algorithms, a 1200-tree Random Forests classifier demonstrated best performance. \\nWe trained a two-state Hidden Markov Model (HMM) to detect differential changes between affected and healthy groups over time. 10 Pilot Study at University of Vermont (UVM): \\nPredictors of Suicide Risk\\nUsing social media to identify predictors of suicide risk Extending from two previous studies (Instagram and Twitter) Leveraging unique dataset from UVM hopstial \\n1500 individuals who have died by suicide in VT, NH and northern NY\\nNames, cities, activity matched w/100 billion tweets in last decade | # | \\n \\nBig picture: The results of our work and that of colleagues suggest that early-warning signs of emerging mental health issues like depression and PTSD can be observed in social media, even before any clinical diagnosis is made. The goal here is to build technology that identifies early warning signs of mental health problems, and connects individuals to a doctor sooner. 11 Veteran ometer\\nApplication of Hedonometer specifically tuned for veterans\\nNearly ten years of Twitter data\\nSentiment analysis for tweets containing veteran \\nRelative count of tweets in the veteran set that contain opioids | # | \\n \\nRed: overall Hedonometer values Blue: veteran-ometer [ambient happiness for veterans] Green: relative count of of veteran-ometer tweets containing the word opioids Note: January 2017 was the date U.S. Secretary of Veterans Affairs Dr. David J. Shulkin announced that VA has begun publicly posting information on opioids dispensed from VA pharmacies, along with VAs strategies to prescribe these pain medications appropriately and safely. https://www.blogs.va.gov/VAntage/44376/va-becomes-first-hospital-system-release-opioid-prescribing-rates/ \\n12 Veteran ometer\\nZoom in to 2016-2018:\\nDates Highlighted Jan 2, 2018\\nMost mentions of opioids\\nVeterans Day\\nVeterans Day\\nOct 9, 2017\\nMemorial Day\\nNotes: We are working to add dates, events, VA press releases, and analysis around all the outlier datesextreme lows and highs in the data. | # | \\n \\nWe are working to add dates, events, VA press releases, and analysis around all the outlier datesextreme lows and highs in the data.\\n13 Veteran ometer: Explaining differences with Word Shifts. Nov. 14, 2017 (Veterans Day)\\nAverage Happiness: 6.40\\nWhats making this day happier than the previous seven days:\\nMay 29, 2017 (Memorial Day)\\nAverage Happiness: 6.05\\nWhats making this day happier than the previous seven days: October 9, 2017\\nAverage Happiness: 5.14\\nWhats making this day sadder than the previous seven days: | # | \\n \\nThe Happiness time series, along with the opioid frequency data, explain the What: What is average happiness in the veteran data on a given day?\\nWord Shifts explain the Why: Why is a particular day happy or sad relative do another day? 14 Potential Applications at VA\\nAssessing and understanding veteran sentiment\\nLeverage well established MITRE & UVM team of Data Scientists, Computational Social Scientists, and Psychiatrists\\nQuantifying and comparing analytics\\nRigorous emphasis on quantifiable sentiment, insight into longitudinal changes over time \\nInsight into the relevance of metadata relating location, TOD, relationships with other datasets, etc.\\nProof of concept studies increasingly suggest there are linguistic & other behavioral predictors of state of mental health state, including suicidal.\\nComplete pilot study initiated at UVM Medical Center and potentially extend to VA, given a training set of VA data \\nveterans w/wo mental health diagnosis who have died by suicide\\nveterans w/wo mental health diagnosis who have not attempted suicide\\nBuild algorithm to identify predictors in classification task, validate on separate set of data. | # | \\n 15 Thank you!\\nJosh Park\\njpark@mitre.org Lisa Tompkins-Brown\\nltompkins@mitre.org Technical POC: Matt McMahon\\nmcmahon@mitre.org | # | \\n 16 Follow up To download a copy of this presentation, visit: https://health.mitre.org/himss18 Improving the \\nPatient Experience\\nFollow us on social media: @MITREhealth | # | \\n 17 |12| www.Hedonometer.org | # | \\n presentgeneralpractitionersunassisteddiagnosticaccuracyasreportedinMitchell,Vaze,and \\nRao(MVR)(24) .6 Results BothAlldataandPrediagnosismodelsweredecisivelysuperiortoanullmodel\\n.Alldatapredictorsweresignificantwith99%probability.57.5(KAll = 1 K 49.8) Pre = 1 \\n7 PrediagnosisandAlldataconfidencelevelswerelargelyidentical,withtwoexceptions: \\nPrediagnosisBrightnessdecreasedto90%confidence,andPrediagnosispostingfrequency \\ndroppedto30%confidence,suggestinganullpredictivevalueinthelattercase. Increasedhue,alongwithdecreasedbrightnessandsaturation,predicteddepression.This \\nmeansthatphotospostedbydepressedindividualstendedtobebluer,darker,andgrayer(see \\nFig.2).ThemorecommentsInstagrampostsreceived,themorelikelytheywerepostedby \\ndepressedparticipants,buttheoppositewastrueforlikesreceived.IntheAlldatamodel,higher \\npostingfrequencywasalsoassociatedwithdepression.Depressedparticipantsweremorelikely \\ntopostphotoswithfaces,buthadaloweraveragefacecountperphotographthanhealthy \\nparticipants.Finally,depressedparticipantswerelesslikelytoapplyInstagramfilterstotheir \\npostedphotos. Fig.2.MagnitudeanddirectionofregressioncoefficientsinAlldata(N=24,713)andPrediagnosis(N=18,513) models.Xaxisvaluesrepresenttheadjustmentinoddsofanobservationbelongingtodepressedindividuals,per \\nstandardizedunitincreaseofeachpredictivevariable.Oddsweregeneratedbyexponentiatinglogisticregression \\nlogoddscoefficients. 6Comparingpointestimatesofaccuracymetricsisnotastatisticallyrobustmeansofmodelcomparison.However, \\nwefeltitwasmoremeaningfultoframeourfindingsinarealisticcontext,ratherthantobenchmarkagainstanaive \\nmodelthatsimplypredictedthemajorityclassforallobservations. \\n7K abbreviatestheBayesFactorratiobetweenthesubscriptedmodelandanullmodel.SeeSIAppendixIVforK \\nvaluelegend. 8 presentgeneralpractitionersunassisteddiagnosticaccuracyasreportedinMitchell,Vaze,and \\nRao(MVR)(24) .6 Results BothAlldataandPrediagnosismodelsweredecisivelysuperiortoanullmodel\\n.Alldatapredictorsweresignificantwith99%probability.57.5(KAll = 1 K 49.8) Pre = 1 \\n7 PrediagnosisandAlldataconfidencelevelswerelargelyidentical,withtwoexceptions: \\nPrediagnosisBrightnessdecreasedto90%confidence,andPrediagnosispostingfrequency \\ndroppedto30%confidence,suggestinganullpredictivevalueinthelattercase. Increasedhue,alongwithdecreasedbrightnessandsaturation,predicteddepression.This \\nmeansthatphotospostedbydepressedindividualstendedtobebluer,darker,andgrayer(see \\nFig.2).ThemorecommentsInstagrampostsreceived,themorelikelytheywerepostedby \\ndepressedparticipants,buttheoppositewastrueforlikesreceived.IntheAlldatamodel,higher \\npostingfrequencywasalsoassociatedwithdepression.Depressedparticipantsweremorelikely \\ntopostphotoswithfaces,buthadaloweraveragefacecountperphotographthanhealthy \\nparticipants.Finally,depressedparticipantswerelesslikelytoapplyInstagramfilterstotheir \\npostedphotos. Fig.2.MagnitudeanddirectionofregressioncoefficientsinAlldata(N=24,713)andPrediagnosis(N=18,513) models.Xaxisvaluesrepresenttheadjustmentinoddsofanobservationbelongingtodepressedindividuals,per \\nstandardizedunitincreaseofeachpredictivevariable.Oddsweregeneratedbyexponentiatinglogisticregression \\nlogoddscoefficients. 6Comparingpointestimatesofaccuracymetricsisnotastatisticallyrobustmeansofmodelcomparison.However, \\nwefeltitwasmoremeaningfultoframeourfindingsinarealisticcontext,ratherthantobenchmarkagainstanaive \\nmodelthatsimplypredictedthemajorityclassforallobservations. \\n7K abbreviatestheBayesFactorratiobetweenthesubscriptedmodelandanullmodel.SeeSIAppendixIVforK \\nvaluelegend. 8 Figure2.HiddenMarkovModelshowingprobabilityofdepression(N=74,990).Xaxisrepresentsdays fromdiagnosis.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Trendlines representcubicpolynomialregressionfitswith95%CIbands,pointsareaggregationsof14dayperiods, witherrorbarsindicating95%CIoncentraltendencyofdailyvalues. Figure3.HiddenMarkovModelshowingprobabilityofPTSD(N=54,197).Xaxisrepresentsdaysfrom traumaevent.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Thepurple verticallineindicatesmeannumberofdaystoPTSDdiagnosis,posttrauma,andthepurpleshaded regionshowstheaverageperiodbetweentraumaanddiagnosis.Trendlinesrepresentcubicpolynomial regressionfitswith95%CIbands,pointsareaggregationsof30dayperiods,witherrorbarsindicating 95%CIoncentraltendencyofdailyvalues. 13 Figure2.HiddenMarkovModelshowingprobabilityofdepression(N=74,990).Xaxisrepresentsdays fromdiagnosis.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Trendlines representcubicpolynomialregressionfitswith95%CIbands,pointsareaggregationsof14dayperiods, witherrorbarsindicating95%CIoncentraltendencyofdailyvalues. Figure3.HiddenMarkovModelshowingprobabilityofPTSD(N=54,197).Xaxisrepresentsdaysfrom traumaevent.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Thepurple verticallineindicatesmeannumberofdaystoPTSDdiagnosis,posttrauma,andthepurpleshaded regionshowstheaverageperiodbetweentraumaanddiagnosis.Trendlinesrepresentcubicpolynomial regressionfitswith95%CIbands,pointsareaggregationsof30dayperiods,witherrorbarsindicating 95%CIoncentraltendencyofdailyvalues. 13 Figure2.HiddenMarkovModelshowingprobabilityofdepression(N=74,990).Xaxisrepresentsdays fromdiagnosis.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Trendlines representcubicpolynomialregressionfitswith95%CIbands,pointsareaggregationsof14dayperiods, witherrorbarsindicating95%CIoncentraltendencyofdailyvalues. Figure3.HiddenMarkovModelshowingprobabilityofPTSD(N=54,197).Xaxisrepresentsdaysfrom traumaevent.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Thepurple verticallineindicatesmeannumberofdaystoPTSDdiagnosis,posttrauma,andthepurpleshaded regionshowstheaverageperiodbetweentraumaanddiagnosis.Trendlinesrepresentcubicpolynomial regressionfitswith95%CIbands,pointsareaggregationsof30dayperiods,witherrorbarsindicating 95%CIoncentraltendencyofdailyvalues. 13 Figure2.HiddenMarkovModelshowingprobabilityofdepression(N=74,990).Xaxisrepresentsdays fromdiagnosis.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Trendlines representcubicpolynomialregressionfitswith95%CIbands,pointsareaggregationsof14dayperiods, witherrorbarsindicating95%CIoncentraltendencyofdailyvalues. Figure3.HiddenMarkovModelshowingprobabilityofPTSD(N=54,197).Xaxisrepresentsdaysfrom traumaevent.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Thepurple verticallineindicatesmeannumberofdaystoPTSDdiagnosis,posttrauma,andthepurpleshaded regionshowstheaverageperiodbetweentraumaanddiagnosis.Trendlinesrepresentcubicpolynomial regressionfitswith95%CIbands,pointsareaggregationsof30dayperiods,witherrorbarsindicating 95%CIoncentraltendencyofdailyvalues. 13 \\nFigure2.HiddenMarkovModelshowingprobabilityofdepression(N=74,990).Xaxisrepresentsdays \\nfromdiagnosis.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Trendlines \\nrepresentcubicpolynomialregressionfitswith95%CIbands,pointsareaggregationsof14dayperiods, \\nwitherrorbarsindicating95%CIoncentraltendencyofdailyvalues. \\nFigure3.HiddenMarkovModelshowingprobabilityofPTSD(N=54,197).Xaxisrepresentsdaysfrom \\ntraumaevent.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Thepurple \\nverticallineindicatesmeannumberofdaystoPTSDdiagnosis,posttrauma,andthepurpleshaded \\nregionshowstheaverageperiodbetweentraumaanddiagnosis.Trendlinesrepresentcubicpolynomial \\nregressionfitswith95%CIbands,pointsareaggregationsof30dayperiods,witherrorbarsindicating \\n95%CIoncentraltendencyofdailyvalues. \\n13 \\nFigure2.HiddenMarkovModelshowingprobabilityofdepression(N=74,990).Xaxisrepresentsdays \\nfromdiagnosis.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Trendlines \\nrepresentcubicpolynomialregressionfitswith95%CIbands,pointsareaggregationsof14dayperiods, \\nwitherrorbarsindicating95%CIoncentraltendencyofdailyvalues. \\nFigure3.HiddenMarkovModelshowingprobabilityofPTSD(N=54,197).Xaxisrepresentsdaysfrom \\ntraumaevent.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Thepurple \\nverticallineindicatesmeannumberofdaystoPTSDdiagnosis,posttrauma,andthepurpleshaded \\nregionshowstheaverageperiodbetweentraumaanddiagnosis.Trendlinesrepresentcubicpolynomial \\nregressionfitswith95%CIbands,pointsareaggregationsof30dayperiods,witherrorbarsindicating \\n95%CIoncentraltendencyofdailyvalues. \\n13 \\nFigure2.HiddenMarkovModelshowingprobabilityofdepression(N=74,990).Xaxisrepresentsdays \\nfromdiagnosis.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Trendlines \\nrepresentcubicpolynomialregressionfitswith95%CIbands,pointsareaggregationsof14dayperiods, \\nwitherrorbarsindicating95%CIoncentraltendencyofdailyvalues. \\nFigure3.HiddenMarkovModelshowingprobabilityofPTSD(N=54,197).Xaxisrepresentsdaysfrom \\ntraumaevent.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Thepurple \\nverticallineindicatesmeannumberofdaystoPTSDdiagnosis,posttrauma,andthepurpleshaded \\nregionshowstheaverageperiodbetweentraumaanddiagnosis.Trendlinesrepresentcubicpolynomial \\nregressionfitswith95%CIbands,pointsareaggregationsof30dayperiods,witherrorbarsindicating \\n95%CIoncentraltendencyofdailyvalues. \\n13 \\nFigure2.HiddenMarkovModelshowingprobabilityofdepression(N=74,990).Xaxisrepresentsdays \\nfromdiagnosis.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Trendlines \\nrepresentcubicpolynomialregressionfitswith95%CIbands,pointsareaggregationsof14dayperiods, \\nwitherrorbarsindicating95%CIoncentraltendencyofdailyvalues. \\nFigure3.HiddenMarkovModelshowingprobabilityofPTSD(N=54,197).Xaxisrepresentsdaysfrom \\ntraumaevent.Healthydataareplottedfromaconsecutivetimespanofequivalentlength.Thepurple \\nverticallineindicatesmeannumberofdaystoPTSDdiagnosis,posttrauma,andthepurpleshaded \\nregionshowstheaverageperiodbetweentraumaanddiagnosis.Trendlinesrepresentcubicpolynomial \\nregressionfitswith95%CIbands,pointsareaggregationsof30dayperiods,witherrorbarsindicating \\n95%CIoncentraltendencyofdailyvalues. \\n13 \",",
  "sdl_date": "2020-12-03T00:00:00",
  "countryPublished": "Saudi Arabia",
  "conference": "lambasting farrowing io Cristofor",
  "originalAuthorName": "Kandeh n Michler",
  "title": "restraint's seeks",
  "declaredTags": "Department of Transportation|MITRE Innovation Program|mobile devices|cybersecurity|shared aviation information",
  "releaseReason": "neckline's/garrotted",
  "docName": "QG_81_3816",
  "fundingCenter": 21,
  "resourceURL": "https://Vivaldi's.com",
  "fundingDepartment": "jo13",
  "caseNumber": "70-9929",
  "publicationDate": "1/10/2020 12:00:00 AM",
  "releaseYear": 2019,
  "releaseStatement": "Public Collaboration/Benchmarking/Standards Body",
  "approver": "$Victorio $Wolfsohn",
  "handCarry": 4,
  "authorDivision": "wq11",
  "copyrightOwner": "Neonila Weixl",
  "lastModifiedDate": "6/1/2002 12:00:00 AM",
  "releaseDate": "1/20/2010 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "3476HMTK81",
  "materialType": "Paper",
  "publicationType": "Paper",
  "authorCenter": 79,
  "originalAuthorID": "Sanaa",
  "mitrePublicServer": 0,
  "subjectTerminology": "Modeling and Simulation",
  "dateEntered": "7/22/2000 12:00:00 AM",
  "documentInfoURL": "https://babushka's schoolmistresses carpetbag tastiness's bronzed.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE2: 17-2450|CASE2: 17-1719",
  "organization": "xj42",
  "authorDepartment": "ic68",
  "publicationYear": 1991,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "Non-Sponsored",
  "level1": "HR, Strat Comm & BD Ops/Dev",
  "fundingDivision": "deepened humanitarians elegance's sublease Edwin's",
  "publishedOutsideUSA": 0,
  "level3": "oo62",
  "level2": "ni63",
  "sdl_id": "bdd156651dd2428ca2c8f1b857f272c0",
  "text": "being examined by some heuristic or analytical method. Edges are constructed between a vertex and all vertices in its neighborhood. Vertices with edges between them are said to be adjacent. A multitude of methods exist to define this neighborhood and the size of the edge set, ||, can vary \\ngreatly between methods for the same set of vertices. Vertex adjacency may be represented by multiple means, e.g., matrices, linked lists, or arrays of arrays. Of particular interest in spectral imaging is the adjacency matrix, ! =\\n\\\", where \\\" = 1 if an edge exists between and , and zero \\notherwise. ! is therefore an n x n symmetric matrix for \\nundirected graphs, where n is the number of pixels in the image. An HSI image with 10% pixels has an adjacency matrix \\nof size 10&'. The need for efficient encoding and/or \\napproximation is evident. Edges may also be encoded with the strength of the relationship between vertices by replacing the unitary edge contributions in !, with the value of a distance (similarity) \\nmetric1. This weighted adjacency (affinity) matrix, ( =\\n), is composed of non-negative scalar values where \\n , , and zero otherwise. A graph may therefore be \\ndefined as = , , ), where w is a mapping associating \\neach edge of unitary contribution in ! with a positive number \\nrepresenting vertex distance (similarity), i.e., ): 0,, or \\nsimply = ,.. In the subsections to follow, we will first focus on basic graph construction techniques and then migrate to those influenced by data density because it leads to the concept of clusters, i.e., community structure. \\n1 Distance measures quantify the separation between two objects such that 0,, where = 0 indicates identical objects. Similarity measures \\nquantify the similarity, s, between two objects where larger values indicate objects that are more alike, where typically 0 1 0,1. Edge weight matrices for \\ndistance and similarity matrices are called adjacency and affinity matrices respectively. IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. xx, NO. yy, MONTH 201x. DOI:xxxx Approved for Public Release; Distribution Unlimited. Case Number 16-3868 \\n 3 A. -Threshold Graphs (aka -NN Graphs) The threshold graph is very simple to construct and the fastest of all described methods. Given all pairwise distances (or similarities), an undirected edge is placed between two nodes if the distance, , , (or similarity) between them is \\nless (greater) than, 1, where 1 > 0 is a user defined constant. \\nSpecifically, , is added to the edge set, , if lies \\nwithin the hypersphere of radius 1 constructed about in \\nspectral space. Mathematically, this may be stated , \\n 33 , 1 and . The similarity based equation \\nis obtained by replacing , with 0 , and switching \\nthe direction of the inequality. This technique is not adaptive to scale or density because 1 is a fixed global threshold [15], \\ngenerally producing unconnected graphs. Hyperspectral data typically displays a multitude of densities hence this technique is not widely used, but is instructional in understanding other graph construction techniques. B. k Nearest Neighbor (k-NN) Graphs Nearest neighbor graphs are very common graphical construction techniques. An edge is placed between and if \\n is among the k nearest neighbors of . The user defined \\nparameter, k, is a global parameter indicating the number of edges exiting , i.e., its out degree. Given each node has its \\nown set of k nearest neighbors, this relationship is not symmetric and therefore produces directed edges leading to an asymmetric adjacency matrix [16]. These graphs are also called directed k-NN graphs for this reason. Construction of the k-NN graph is conceptually very simple, but computationally expensive due to the evaluation of all pairwise distances and subsequent sorting. Many traditional indexing methods (e.g., R-tree, k-d tree) fail in high dimensional spaces such that exhaustive searching for nearest neighbors can outperform even the most complex indexing scheme [17]. As such, several fast nearest neighbor methods have been developed to generate approximate or exact k-NN lists [18]. k-NN graphs are locally adaptive to both density and scale which makes them particularly well suited to model clusters of varying density [19] or follow mixing trends between clusters. Unfortunately, a global k-NN construction tends to over connect vertices in low density regions since the nearest neighbor may span a significant distance (or similarity). Another difficulty is selection of the user defined parameter k; values from 5 to 60 are common [13][19]. Selecting k too high tends to over connect the graph, whereas selecting k too low leaves the graph disjoint. This sensitivity to k is true of many k- NN variants and has promoted the development of adaptive algorithms that provide node-specific values, 8. \\nWe are restricting analysis to simple graphs and therefore need to modify the asymmetric adjacency matrix resulting from the directed k-NN relationship. There are two means of creating simple graphs from directed k-NN graphs, producing symmetric adjacency matrices: symmetry and mutuality. An added benefit of symmetric adjacency matrices is that memory requirements are cut in half because only the upper or lower triangular portions of the adjacency matrix need be stored. This can be quite substantial for even average sized HSI cubes. Generation of the symmetric k-NN graph is a trivial extension of the directed k-NN graph, where all vertex pairs, , , are connected if 9 orororor \\n 9, where 9 represents the k nearest neighborhood \\nof vertex . As a result, each node will have at least k \\nneighbors with the node degree being proportional to the nodes local density. Forcing a symmetric adjacency in this manner (bidirectional extension) may connect clusters of varying density and edges can still span large regions of spectral space to overly connect outlier nodes. Stringers, long chains of single nodes, can also extend from virtually anywhere if the conditions are right. This can produce a larger subgraph diameter if they extend from cluster edges. Generation of the mutual k-NN graph is also a trivial extension of the directed k-NN graph, where all node pairs , , are connected if 9 and 9, i.e., \\nonly existing bidirectional edges are retained. The resultant adjacency matrix is symmetric and a subset (subgraph) of the directed k-NN adjacency matrix (graph). Forcing adjacency symmetry in this manner reduces the possibility of connecting clusters of varying density; hence edges typically do not span large regions of feature space, leaving outlier nodes unconnected from denser regions [15]. C. Density Weighted k-NN (DW k-NN) Graphs Dense groupings of points in feature (spectral) space share similar attributes of similar magnitude and are therefore related. It makes intuitive sense these similar intracluster nodes should be more heavily connected than extracluster nodes. HSI data clusters exist at varying scales and density, so adaptive algorithms are desired. The definition of density itself may also need to be change because traditional Euclidean density becomes meaningless in high dimensional spaces due to the exponential growth in d-D volume [20]. Kameshwaran and Malarvizhi [21] state density based measures are the key to finding nonlinear structure, and we will find they are used extensively in HSI graph generation. Mercovich [13] introduced the concept of density weighted k-nearest neighbors to encode stronger relationships between similar nodes and promote more effective clustering by minimizing the impact of extracluster pixels. All pixels are assigned a codensity (distance) score given by ,\\n1 1\\n)( max minminmax \\n= \\n +\\n= k kk iki w\\nkk v (1) where k represents the indices of node <0 nearest neighbors \\nprovided in non-decreasing order, and kmin and kmax define the range of k values to average. Mercovich set 8=> = 1 such \\nthat ? represents the average distance of the pixels \\n8=@A neighbors. A node-specific number of neighbors, 8, is \\nassigned to each node based on its position in the histogram of IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. xx, NO. yy, MONTH 201x. DOI:xxxx Approved for Public Release; Distribution Unlimited. Case Number 16-3868 \\n 4 codensity scores (indicated by the nodes z-score). Mercovich found assignments based on integral z-score values from the range [-3,3] worked well for HSI clustering. Fig. 1a shows a representative Gaussian codensity distribution and the node specific connectivity count, 8 assigned by this method. \\nAssigning 8 in this manner results in a large number of nodes with ~8=@A/2 edges and much fewer nodes with 1 or \\n8=@A edges as shown in Fig. 1(b). The mapping is a quantized \\ninverse function of codensity where pixels residing in lower density regions (higher codensity) receive few edges (towards 1) and pixels in high density regions (low codensity) receive more edges (towards kmax). Mercovich notes that codensity distributions can take on non-normal forms based on scene content, but the normal assumption (through use of z-score) worked well for clustering2. HSI codensity distributions can approach normal \\n2 While some codensity distributions may appear relatively normal, they are never rigorously normal. We verified this by testing 56 chips of varying scene content, from different sensors, and GSDs via the Komolgorov-Smirnov test with increasing number of bands as seen in Fig.",
  "updated_at": "8/21/2009 12:00:00 AM",
  "created_at": "11/2/2018 12:00:00 AM"
}