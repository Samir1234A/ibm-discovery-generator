{
  "sdl_source_type": "MPL",
  "productName": "yucca tanneries",
  "uploadedate": "2010-04-26T00:00:00",
  "productUrl": "http://opossum's.com",
  "creatorNames": "Sondra u Otamendi;Karamat v Etxezarreta;Isilda g Derks",
  "uploaded": "2019-02-08T00:00:00",
  "sdl_extracted_summary": "forest \\nwas made up of 10 sub-forests, each with 10 trees, giving 100 trees total. Each sub-forest was \\ntrained on a different balanced subset of the training data, to increase the total amount of data that \\nthe forest is exposed to, while still keeping the outputs unbiased. Figure 8 shows the results. ALERTairportbenign boatingboatingboisterous crowdclimbing/jumpingcrowddancingdangerous boatingdemonstration/protestdestructiveemergency responsefightinginjurymarchingparadingpolice standoffrunningscreamingshootingtraffic-vehiculartrain stationmAP\\n0 0.2 0.4 0.6 0.8 1 Av\\ner ag\\ne Pr\\nec isi\\non (A\\nP) 0 0.5 1 1.5 2 m\\nea n \\nAv er\\nag e \\nPr ec\\nisi on\\n (m AP\\n)Comparison of ContextNet and Random Forest Classifiers ContextNet Random Forest Classifiers Placeholder Figure 8: Average Precision and Mean Average Precision: ContextNet Versus Random \\nForests The graph shows Average Precision (AP) scores for both ContextNet and Random Forests in the indivual \\ncontexts of the LBM Dataset, along with Alert. The mean Average Precision (mAP) is also displayed on the far \\nright. We found that there was a wide range of difficulty for the various contexts, with boating being \\none of the easiest to detect, and destructive being one of the hardest to detect. The hardest \\ncontexts to detect were typically ones with a very limited number of positive examples in the \\ntraining set, and we believe performance in these categories would be significantly improved by \\nobtaining additional training examples. Of the 22 labels (including Alert), ContextNet achieved a \\nhigher AP score in 13 labels with a 3% higher overall mAP score than Random Forests. \\nContextNet and Random Forests were approximately tied in 5 labels and Random Forests \\nperformed better in only 4 labels. In these 4 labels, Random Forests had significantly higher AP \\nscores than ContextNet, the most notable of which was a rare context benign boating. One \\npossible explanation for this result is the simpler way that decision trees handle features. Decision xv trees always split along a feature dimension, thus, if there are a few simple but effective scene \\nfeatures that can instantly distinguish a context (rather than a more complex interaction among \\nfeatures), then the forests may do a better job than ContextNet in selecting these features and \\nignoring noise from irrelevant features. This is especially likely for contexts with few positive \\nexamples. For example, the Video Entity Detection module can detect boats, cars, buses and other \\nvehicles, which could be instantly useful to identify benign boating or traffic-vehicular. \\nAdditionally, there are audio entities for screaming that would be directly useful to identifying the \\nscreaming context. These features are available for ContextNet to use too; however, with few \\npositive training examples, it is harder to train ContextNet because it is a more complex model \\nthan Random Forests. The results also suggest that ContextNet is better at detecting more complex \\nand nuanced contexts, such as injury, destructive and most importantly Alert. These are \\nmore abstract concepts that require more complex fusion from multiple domains to detect. \\nContextNet is capable of combining features in this way, while Random Forests is limited to \\nsplitting along one feature at a time. Alert: ContextNet on the LBM Dataset5.2\\nIn this section, we give special attention to the performance of the Alert of ContextNet. \\nMechanically speaking, the Alert label is handled identically to any other context, but for the \\nLinkBioMan system as a whole, it has a special importance because of the systems overall aim \\nat generating near-real-time alerts for complex real-world situations. Alert is also unique because it \\nis extremely open-ended in its definition. The other context labels generated by ContextNet \\nrepresent fairly concrete concepts such as airport, crowd, screaming or fighting. Such \\nlabels can have a broad range of characteristics, but they are all still much more specific than \\nAlert which could represent many different scenarios, such as, riots, injuries, fires, car accidents, \\nrobberies, floods, and more. For this reason, ContextNets understanding of alert-ability is highly \\ndependent on the data on which it is trained and on the annotations provided by human annotators. When trained with all possible input modules, ContextNet achieved 81.5% accuracy at detecting \\nalert-able situations. However, because the LBM Dataset has a highly unbalanced number of \\npositive and negative examples for most labels, accuracy is not a good metric for measuring \\nperformance. Instead, the preferred metric is AP, for which ContextNet achieved a score of 0.567. \\nFor perspective, the expected AP of a fully random system on a large data pool can be \\napproximated as the ratio of true positive examples to total documents [66]. In this case, the testing \\nsegment of our LBM Dataset had 214 clips annotated as alert-able out of the 1094 clips total, \\ngiving an expected AP of only 0.196. Thus, the performance of ContextNet in detecting alert-able \\nsituations far exceeds random chance. We can also examine the true positive rate (TPR) and false \\npositive rate (FPR) for Alerts when using a fixed threshold to convert alert probability scores into a \\nbinary decision. In this case we will use the simple threshold of 0.5. At this threshold, ContextNet \\nhas a 0.678 TPR and a 0.151 FPR. While these results are far from ideal, they are promising given \\nthe domain complexity, the open-endedness of what the system is expected to detect, and the \\nmany challenging factors included in the LBM Dataset, such as, poor image quality, wide \\nvariability in viewing angles, moving cameras, and missing sound. xvi Context Classifier: ContextNet, Random Forests on the TRECVid 5.3\\nEvaluation Dataset NIST made available the numerical scores for the TRECVid 2015 SI Task, which we used to \\ncompare with our context classifier performance of both ContextNet and Random Forests. These \\ntests are performed on seven topics described in Section 3.3. Note that for TRECVid 2015, NIST \\nused inferred Average Precision (infAP) [67] to estimate precision from their rank-ordered search \\nand retrieval results based on random sampling theory. This infAP metric gives results that closely \\napproximate standard AP, and thus, it should provide a fair comparison with our results, which are \\nmeasured in AP. Table 3 presents the infAP scores and mAP score for the best performing \\nTRECVid team, alongside the AP and mAP scores for ContextNet and Random Forests. Figure 9 \\npresents the scores of all TRECVid teams as well as those of ContextNet and Random Forests on \\nthe seven topics. Both approaches (ContextNet and Random Forests) outperform many of the \\nTRECVid performing teams scores. Table 3: Comparison of ContextNet (CN), Random Forests (RF) \\nand Best TRECVid Team on the TRECVid Evaluation Dataset The table compares the AP of ContextNet and Random Forests when applied to seven of the NIST-annotated \\ntopics on the TRECVid 2015 dataset. The mAP scores across all 7 topics are displayed in the bottom row of the \\ntable. The three colored columns indicate three different testing configurations, which are elaborated in more \\ndetail in the paragraphs below the table. The scores of Best TRECVid Team are reported as Inferred Average \\nPrecision (infAP), which approximates AP. Average Precision (AP) All Data\\nAll Data (No Audio Used) \\nClips with Audio TRECVid Topic Best \\nTRECVid Team CN CN RF CN RF Boat Ship 0.569 0.626 0.608 0.529 0.684 0.604 Cheering 0.229 0.205 0.235 0.097 0.245 0.409 Dancing 0.072 0.175 0.168 0.166 0.234 0.170 Demonstration or Protest 0.397 0.513 0.435 0.423 0.618 0.237 Explosion Fire 0.135 0.308 0.315 0.272 0.379 0.261 Running 0.280 0.241 0.185 0.299 0.259 0.400 Throwing 0.226 0.204 0.235 0.096 0.235 0.100 mAP 0.273 0.325 0.312 0.269 0.379 0.312 We performed experiments in three distinct configurations, which are highlighted in different xvii colors in Table 3. In the first configuration, All Data, we used all the clips minus the few that \\ncould not be processed due to technical issues (see Section 3.3 for details). For this configuration, \\nwe compared only ContextNets results to the best TRECVid performing teams results because, \\nunlike the experiments comparing ContextNet and Random Forests using the LBM Dataset, \\nwe did not attempt to accommodate data of varying feature vector length for the random \\nforest implementation (i.e., training and testing ContextNet and Random Forests with the \\nsame clips of missing audio and with audio). In three of the seven topics tested, we achieved \\nhigher individual topic AP scores than any other TRECVid performing teams (see Figure 9), and \\nwe achieved a 0.325 mAP score, far surpassing the best TRECVid performing teams score of \\n0.273. These results are very promising, but they lead to two questions which we addressed with \\nthe other two configurations: First, considering that audio is missing in many clips, will \\nContextNet perform better if the audio features are just completely removed? Second, if we restrict \\nthe dataset down to only the clips with audio, will that make the problem easier and will \\nContextNet and Random Forests have better performance? Note that because many clips lack audio, we used zero-padding to replace the missing inputs to \\nContextNet. This partially missing data could have adverse effects on ContextNet and even lead to \\nworse performance than if audio was completely ignored. The second configuration, All Data \\n(No Audio Used), was used to answer the first question posed above. In All Data (No Audio \\nUsed), we used all the clips, but simply ignored all",
  "sdl_date": "2020-09-21T00:00:00",
  "countryPublished": "Comoros",
  "conference": "truncheon's philosophy's bs Emelia",
  "originalAuthorName": "Wenyuan m Kalishevsky",
  "title": "irremediably goldfinch jellyfish's casuistry",
  "declaredTags": "avalanche photodiodes|satellite constellation|PDMP|data analytics|Department of Defense",
  "releaseReason": "edicts/schematic",
  "docName": "HI_15_9307",
  "fundingCenter": 66,
  "resourceURL": "https://ineligible.com",
  "fundingDepartment": "jk15",
  "caseNumber": "44-4892",
  "publicationDate": "6/8/2019 12:00:00 AM",
  "releaseYear": 2016,
  "releaseStatement": "Conference/Workshop",
  "approver": "$Lawanda $Duckworth",
  "handCarry": 8,
  "authorDivision": "tm43",
  "copyrightOwner": "Xueping Kola",
  "lastModifiedDate": "4/17/2004 12:00:00 AM",
  "releaseDate": "6/13/2002 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "9424IVWO92",
  "materialType": "Book",
  "publicationType": "Paper",
  "authorCenter": 64,
  "originalAuthorID": "Klaus",
  "mitrePublicServer": 0,
  "subjectTerminology": "Data (General)",
  "dateEntered": "3/27/2009 12:00:00 AM",
  "documentInfoURL": "https://septettes decriminalize bowler Amoco's plumpest.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE2: 12-1089|CASE2: 17-2450|CASE1: 18-0469",
  "organization": "jv77",
  "authorDepartment": "yk32",
  "publicationYear": 2007,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "NSEC MOIE",
  "level1": "MITRE Public Sector",
  "fundingDivision": "rationality leanness incision snapping optometrists",
  "publishedOutsideUSA": 0,
  "level3": "kx33",
  "level2": "gf26",
  "sdl_id": "5d0c78d3fc384b3191b919da8707a51c",
  "text": "Management from Nova Southeastern University. Her \\nemail is sheila@mitre.org. CHARLES SCHMIDT is a Group Lead at the MITRE corporation. He has over 17 years experience in \\ncybersecurity, security automation and standards development. He holds a B.S. in Mathematics and \\nComputer Science from Carleton College and an M.S. in Computer Science from the University of \\nUtah. His email is cmschmidt@mitre.org. JOHN TUFAROLO is a principal engineer at the Modeling, Simulation, Experimenation and Analytics \\n(MSEA) Technical Center, MITRE. He is a NTSA Certified Modeling and Simulation Professional \\n(CMSP) and has more than 32 years of experience providing systems engineering project work, \\nplanning, and leadership in complex distributed systems. His email is tufarolo@mitre.org. RICHARD B. HARRIS is a principal cybersecurity policy engineer for the Homeland Security Center, \\nMITRE. He has over 14 years of experience in cybersecurity with the Department of Homeland \\nSecurity and MITRE, and a perspective on complex risk environments that was seasoned by a 26 year \\ncareer in the U.S. Marine Corps. His email is rbharris@mitre.org. Mittal, Cane, Schmidt, Tufarolo and Harris Approved for Public Release; Distribution Unlimited. Case Number 18-1212 / DHS reference number 17-J-00100-01 NOTICE This (software/technical data) was produced for the U. S. Government under Contract Number \\nHSHQDC-14-D-00006, and is subject to Federal Acquisition Regulation Clause 52.227-14, Rights in \\nDataGeneral. As prescribed in 27.409(b)(1), insert the following clause with any appropriate alternates: Rights in DataGeneral (Deviation May 2014). No other use other than that granted to the U. S. Government, or to those acting on behalf of the U. S. \\nGovernment under that Clause is authorized without the express written permission of The MITRE Cor-\\nporation. For further information, please contact The MITRE Corporation, Contracts Management Office, 7515 \\nColshire Drive, McLean, VA 22102-7539, (703) 983-6000. \",",
  "updated_at": "11/25/2007 12:00:00 AM",
  "created_at": "5/7/2019 12:00:00 AM"
}