{
  "sdl_source_type": "MPL",
  "productName": "cribbage ramshackle",
  "uploadedate": "2010-11-27T00:00:00",
  "productUrl": "http://pontoon.com",
  "creatorNames": "Amalia p Ugartechea;Redha o Beckmann",
  "uploaded": "2019-04-26T00:00:00",
  "sdl_extracted_summary": "    \"text\": \" Microsoft Word 289292 i MITREs mission-driven teams are dedicated to solving problems for a safer world. Through our federally funded R&D centers and public-private partnerships, we work across government to tackle challenges to the safety, stability and well-being of our nation. ii Table of Contents Background \\nhttps://accelerate.mitre.org/\\nhttps://www.mitre.org/publications/technical-papers/defense-agile-acquisition-guide-tailoring-dod-it-acquisition-program\\nhttps://playbook.cio.gov/techfar/\\nhttps://playbook.cio.gov/ 10 RFP Patterns and Techniques for Successful Contracting Contracting Guidance to Support Modular Development SAFe 4.6 GAOs (12-681) Software Development: Effective Practices and Federal Challenges in Applying Agile Methods SEIs RFP Patterns and Techniques for Successful Agile Contracting Defense Innovation Board (DIB) Software Acquisition and Practices (SWAP) Study Familiarization with these publications equips the workforce with all of the foundational knowledge needed to achieve acquisition flexibility and prevent them from having to reinvent the wheel. 8. Inject Training and Experience to Support a Flexible Agile Approach: Agile coaches, with deep experience in managing agile programs, provide the needed expertise to guide program teams. Observations/Challenges: It is an understatement to say that, applying an Agile approach to software development is not easy. The need for an experienced, co-located Agile coach in a clearly defined visible role is made more and more apparent every day. Programs frequently encounter difficult or unique situations that require training, experience, and guidance. Recommendation: The Defense Acquisition University (DAU) offers some short courses that provide acquisition and source selection teams with tools to help understand the different methodologies of Agile software development. One important continuous learning course, Continuous learning module CLE076 Introduction to Agile Software Development for Defense, includes Agile approaches, and benefits and risks of Agile development. DAU also offers a series of short videos produced by DAUs Agile subject matter experts (SMEs), Mr. Chris Collins and Mr. Robert Thomas. Additional videos on the site include Agile Software Development and Scrum 101. These videos cover basic principles of Agile software development. By investing time in these course offerings and understanding principles of Agile development, an organization will be better positioned to write requirements or define higher level objectives for a SOO with flexible and open parameters. Moreover, understanding the intricacies of standing up a large Agile development program, particularly one with many levels and comprising a large solution, requires a Sherpa who has done similar work in the past. It is not sufficient to learn on the job with so much at stake. The coach can also have bigger impact with a clearly defined role on the program. https://resources.sei.cmu.edu/asset_files/SpecialReport/2016_ 63.pdf\\nhttps://obamawhitehouse.archives.gov/sites/default/files/omb/procurement/guidance/modular-approaches-for-information-technology.pdf\\nhttps://www.scaledagileframework.com/agile-contracts/\\nhttps://www.gao.gov/assets/600/593091.pdf\\nhttps://www.gao.gov/assets/600/593091.pdf\\nhttps://resources.sei.cmu.edu/asset_files/SpecialReport/2016_ 63.pdf\\nhttps://media.defense.gov/2019/Apr/30/ /-1/-1/0/SOFTWAREISNEVERDONE_REFACTORINGTHEACQUISITIONCODEFORCOMPETITIVEADVANTAGE_FINAL.SWAP.REPORT.PDF\\nhttps://media.defense.gov/2019/Apr/30/ /-1/-1/0/SOFTWAREISNEVERDONE_REFACTORINGTHEACQUISITIONCODEFORCOMPETITIVEADVANTAGE_FINAL.SWAP.REPORT.PDF 11 Agile purists are not able to deliver success or positive change. The best Agile development implementations tailor the methods to the needs of the organization. The best coaches understand the principles and know what to tailor to help the organization progress and become more efficient, while not tailoring away the goodness that is enabled by the agile methods. Additionally, program teams should reply upon a Sherpa who can guide the program and keep it on a path to success. Agile coaches, with deep experience in managing agile programs, provide the needed expertise to guide program teams; they should be integrated into the Governments program management team and empowered. Programs must adopt and include a co-located agile coach in a clearly defined, visible role. This role should not be provided by the contractor who is leading the development effort. 9. Institute a Change Management Process: Change management needs to continue throughout the lifecycle of the program, and programs should continuously look to adopt such practices at a larger scale. Observations/Challenges: In many cases, a program team establishes a clear and disciplined roadmap to achieving program success but are less disciplined when an unexpected change disrupts the teams plans. This, in turn, could result in extended delays and additional unnecessary costs. Implementing Agile Development requires change in organizations. This includes change to their planning processes, change to their deliverables, change in the way testing is accomplished, and change in how deployments are managed. It is insufficient to enable changes in one area and expect each of the other areas to change themselves. This change should be accomplished as part of the DevSecOps Tactics, Techniques, and Procedures (TTPs). Recommendation: The introduction of flexibility into acquisition processes does not mean that the processes should be undisciplined. Brown and Hegarty provide a sound recommendation and caution the Government to continuously Anticipate the unexpected and include language to govern contract extensions.7 This not only ensures that contractors are fully prepared to adapt to evolving needs, but also encourages them to build a plan in response to the Government request for proposal (RFP) describing how the contractor will deal with inevitable changes to avoid having to re-negotiate the contract. 7 Brown, A. a requirements backlog meets this need. Finally, the established change management process must include repeatable processes for evaluating and reprioritizing the backlog (or equivalent) on a regular basis. Establishing this normalized rhythm of reprioritization will minimize many of the negative effects that evolving requirements generate. Change management needs to continue throughout the lifecycle of the program, and programs should continuously look to adopt such practices at a larger scale. Furthermore, it is imperative that programs are aware and build relationships with supporting organizations to help make and support change throughout the programs lifecycle. 10. Enforce a Robust Requirements Elicitation and Development Process: It is imperative to time-box requirement elicitation, and most importantly, to have all of the stakeholders in the room, working collaboratively on defining these requirements. 8 RFP Patterns and Techniques for Successful Agile Contracting, Software Engineering Institute, November 2016. https://resources.sei.cmu.edu/asset_files/SpecialReport/2016_ 63.pdf 9 Ibid. 10 Brown, A. however, if the Government does not want to create this definition upfront, it should require offerors to include that definition as part of their Agile proposal or the proposed process. Ultimately, this definition, tied with a well-defined vision, will be used to accept or reject the contractors output at the end of each sprint and will maintain boundaries to promote greater flexibility within. 12. Identify the Right Agile Development Metrics: Practitioners must define metrics to determine value to the end user. Observations/Challenges: Pursuant to Section 872 of the 2018 NDAA, the Defense Innovation Board (DIB) conducted a study on Software Acquisition Practices (SWAP). The first of three (3) fundamental themes discussed how Speed and cycle time are the most important metrics for managing software.13 This aligns to the 2018 National Defense Strategy to ensure that the U.S. Government executes acquisition and development faster than its adversaries. However, finding the right metrics to use for evaluating a program can be challenging and must progress beyond estimating complexity based on Source Lines of Code and in terms of programmer productivity.14 Recommendation: The DIB Metrics for Software Development report stipulates that different software types (e.g., commercial, custom, or blends) will drive different metrics to evaluate program success. Consequently, the report recommends deployment rate, response rate, code quality, and program management as four (4) necessary categories of metrics, while providing discrete examples that can be incorporated into both acquisition strategies and plans to maintain accountability for program success. Finally, practitioners must define metrics to determine value to the end user in relation to the aforementioned four (4) general categories to align programmatic and mission success. As with the Vision, and definition of Done, strong metrics help to communicate clear reasonable boundaries, allowing for greater variability of activities and flexibility within those limits. 13. Develop an Overall Vision: The Product Vision, coupled with the use of a SOO, provides flexibility. 13 Defense Innovation Board (DIB) Software Acquisition and Practices (SWAP) Report. https://innovation.defense.gov/software/ 14 Defense Innovation Board (DIB) Metrics for Software Development. https://media.defense.gov/2019/May/02/ /-1/-1/0/DEFENSEINNOVATIONBOARDMETRICSFORSOFTWAREDEVELOPMENT.PDF 15 Observations/Challenges: FAR 15.203(a)(1) requires Requests for Proposals (RFPs) to describe the Governments requirements; however, it is significantly more difficult to define tasks and develop a work statement for an Agile-like requirement due the evolving nature of solution development and employment. Recommendation: In 2014, Scrum.inc hosted a presentation on Agile Contracts: The Foundation of Successful Partnering, led by Alex Brown and Christopher Hegarty, which suggested that agencies specify overall vision and context in the Statement of Objectives (SOO) versus stipulating the required process.15 This approach enables the Government to articulate a framework and award flexible contracts that meet the Governments vision. The TechFAR also recommends the use of a Product Vision, which establishes a high-level definition of the scope of the project, specifies expected outcomes, and produces high level budgetary estimates. This Product Vision, coupled with the use of a SOO, provides flexibility and allows vendors to develop innovative solutions.16 14. When in Agile, Act Like an 874: Tailor programs to reduce contract requirements for DoD software development. Observations/Challenges: The role of these 874 pilots is to write the book on how Agile can be best integrated into the current Department of Defense (DoD) acquisition system to help streamline delivery of software intensive developments. Just because a program or project is not explicitly designated as a formal 874 project doesnt mean that agencies cannot utilize 874 Agile best practices to build a better solution. Recommendation: If a program",
  "sdl_date": "2020-06-14T00:00:00",
  "countryPublished": "Burkina",
  "conference": "schussing annual mj Din",
  "originalAuthorName": "Aixa a Vishnuraman",
  "title": "racial canal's",
  "declaredTags": "Marine Corps|Special Publication|IAPs",
  "releaseReason": "molder's/abductor",
  "docName": "LR_90_6454",
  "fundingCenter": 95,
  "resourceURL": "https://snack.com",
  "fundingDepartment": "ik18",
  "caseNumber": "28-9666",
  "publicationDate": "9/2/2019 12:00:00 AM",
  "releaseYear": 2018,
  "releaseStatement": "Publicity/Promotion",
  "approver": "$Gardenia $Rkiouak",
  "handCarry": 3,
  "authorDivision": "xp75",
  "copyrightOwner": "Matutina Kempen",
  "lastModifiedDate": "10/8/2012 12:00:00 AM",
  "releaseDate": "5/14/2013 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "3516TWRP83",
  "materialType": "Article",
  "publicationType": "Book",
  "authorCenter": 81,
  "originalAuthorID": "Marsel",
  "mitrePublicServer": 0,
  "subjectTerminology": "Human Resources Management",
  "dateEntered": "8/11/2000 12:00:00 AM",
  "documentInfoURL": "https://value's butternut's spares extirpation's expropriating.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE3  17-2945|CASE3  17-2945|CASE1: 18-0296|CASE1: 17-2739",
  "organization": "au45",
  "authorDepartment": "wc14",
  "publicationYear": 2016,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "Non-Sponsored",
  "level1": "Programs & Technology",
  "fundingDivision": "menswear's charlatan's pettiness saprophytes yessed",
  "publishedOutsideUSA": 0,
  "level3": "jt22",
  "level2": "le19",
  "sdl_id": "29ff4dc0ba444e2da2031855b3b56b80",
  "text": "to better their video scene analysis. Some image and video \\ncaptioners also attempt to include semantic context [2], [19][21]. In this paper, we extend the \\ntraditional semantic context to actionable context, where a preliminary interpretation of events \\n(What is happening?) through semantic captioning and alerting (Is something happening that \\ndemands an immediate response?) can be automatically generated for a timely response to an \\nobserved event from a video scene. To increase the accuracy of such context detection, we believe \\nthat it is key to exploit the multiple modalities present in the sensor feed. Leveraging and fusing \\nmultiple modalities in signal processing has been previously explored [3], [15], [16], [22][30], \\n[62]. However, with the possible exception of [26], which fuses audio and video to detect violent \\nscenes in movies, most of the fusion deals with benign, simple and often trivial contexts. Our goal \\nis to develop a decision framework capable of processing complex topics and extracting an \\nactionable context to aid in public area safety and in post-event investigation for law enforcement \\nor national/international security. Datasets3\\nPublicly available, well annotated multimedia datasets that include video and audio recordings of \\nthe activities of interest are scarce. To get started, we assembled an initial dataset reflecting the \\nactivities of highest relevance to LinkBioMan and annotated and labeled the data ourselves as \\nexplained in Section 3.1. Recognizing the need to expand our data corpus, we continued to search \\nfor external sets of annotated data and were able to acquire two datasets: the Youtube2Text iii 2 http://www.youtube.com\\n3 http://webscope.sandbox.yahoo.com Dataset, described in Section 3.2, which was used for evaluating LinkBioMans video captioner \\nperformance and compared against existing video captioners, and the TRECVid Evaluation \\nDataset, described in Section 3.3, which was used to compare LinkBioMans ContextNet \\nperformance with the TRECVid performer teams video context models in selected topics relevant \\nto LinkBioMan. LinkBioMan (LBM) Dataset3.1\\nFor system development testing and training ContextNet and the LinkBioMan Captioner we \\nassembled our own dataset consisting of 4404 short video clips extracted from 2541 source \\nvideos; each clip is 2 seconds in duration, totaling 84.7 minutes of video. We used freely available \\nvideo recordings from a variety of uncontrolled sources, including mobile sensors and surveillance \\ncameras, rather than from orchestrated data collections. This dataset henceforth referred to as the \\nLBM Dataset includes a mixture of videos, both with and without audio, downloaded from \\nYouTube2 under the Creative Commons licenses in combination with videos from existing video \\ndatasets: WWW Crowd Dataset [31], YFCC100M Dataset3 [32], Violent Scenes Dataset [33] and \\nViolent-Flows Database [34]. Source videos encompass a wide range of real-world scenes, as well \\nas scenes from sporting events and films. These videos were recorded on a variety of devices \\nranging from personal cell phones to professional film equipment. \\nBecause selected videos did not come with captioning, we annotated the clips and generated \\nground truth labels. First, a basic motion-based keyframe extractor [36] was used to identify \\nportions of each video where activity was occurring. Next, these portions were segmented into 2-\\nsecond clips to create the corpus. Using a simple interface we created in-house, annotators \\n(MITRE staff including some of our team members) reviewed each 2-second clip and provided: \\n(1) a brief (ideally one sentence) description of the activity that they saw and heard in the clip, (2) \\nan indication of which modality carried the most relevant information (audio/visual/both) and (3) a \\nbinary alert/no alert determination to indicate whether an unusual activity meriting attention \\noccurred in the clip. Each clip was annotated for its own content, not the content of preceding or \\nsubsequent clips. The descriptions provided by the annotators adhered to a general style of \\nactor/object followed by action, object and then a prepositional phrase containing key contextual \\ninformation. Guidance was provided to annotators in the form of the following examples: OBJECT ACTION OBJECT ENVIRONMENT Policemen are pushing the people publicly protesting against NATO\\nA man is fighting a man in front of a store at the mall\\nA policeman has pinned down a man on the ground in front of a store at the mall\\nA man is throwing bottles to the store window on the street\\nPeople are standing on top of a car that is turned upside down\\nA man started a fire near a car on the street iv The annotations were reviewed, and any typos corrected before labeling occurred. Annotators \\nwere not constrained to a set dictionary but did exhibit overall self-consistency across annotations \\nand in the case of multi-way annotation, annotators were unanimous in identifying the key activity \\ncontent. While the initial batch of the LBM Dataset was annotated five-ways (i.e., five annotators \\nwith each clip), more recent portions were annotated one-way due to time constraints. Figure 2 \\nshows frames from two different video clips, along with the associated annotations, in the format \\nof <caption>|<modality>|<alert>. Figure 2: Sample Frames of LBM Dataset Video Clips\\nGround truth labels (i.e., semantic context) were derived exclusively from the annotations. Each \\nclips annotations were searched for descriptive terms of the activities sought. For example, the \\npresence of words including smash, bash, shatter, break and/or destroy in annotations would \\nindicate that destructive behavior is evident in the clip. In the case of five-way annotations of the \\nsame clips, synonymous terms that different annotators used to describe the same actions, objects \\nand actors were added to the list of keywords associated with an activity label. A general rule was \\nthat if one or more annotators identified an activity, then the clip was labeled as positive for that \\nactivity, although in every instance there was unanimous agreement on the presence of an activity. \\nThe semantic context labels included in this dataset cover a range of topics and may pertain to the \\nvisual surroundings (e.g. Airport, Train Station), human actors (e.g. Crowd, Fighting, Running), \\nnon-human actors (e.g. Benign Boating, Dangerous Boating), or auditory cues (e.g. Screaming, \\nShooting). YouTube2Text Dataset3.2\\nWe selected YouTube2Text or Microsoft Video Description Corpus (MSVD) [37] to compare \\nLinkBioMans Captioner performance with other captioners that published their performance on \\nthe YouTube2Text Dataset. The official YouTube2Text Dataset is comprised of 1970 clips \\nwithout audio, with multiple descriptions per video. Annotation of the dataset was crowd-sourced \\nvia Amazons Mechanical Turk. The clips contain a wide variety of scenes and actions performed \\nby both humans and animals, ranging from cooking to playing to riding a bike. Each clip averages \\nabout 41 reference captions, and the average clip length is approximately 9.6 seconds. While the \\nofficial YouTube2Text Dataset contains descriptions in multiple languages, we used only the v English descriptions. Although the official YouTube2Text Dataset does not contain audio, it is \\ndistributed with uniform resource locators (URLs) to the original video clips that contain \\naudio, some of which have been dubbed with music. For additional experiments, we followed \\n[3] in downloading and extracting clips from the original video clips via the included URLs. \\nFor our experiments, we used both the official YouTube2Text Dataset without audio and the \\nYouTube2Text clips with audio. TRECVid Evaluation Dataset3.3\\nWe decided to leverage the outcomes from the National Institute of Standards and Technology \\n(NIST) TRECVid evaluations [38] to compare the ContextNets performance with other video \\ncontext models. We selected the TRECVid 2015 Semantic Indexing (SI) Task evaluation [8]. \\nPublicly-available NIST-generated annotations were downloaded from the following datasets: \\nIACC.1A, IACC.1B, IACC.1C and IACC.2C [39]. Of the 30 topics for which NIST created \\nground truth labels, only seven topics were deemed relevant to the usage scenarios envisioned for \\nLinkBioMan. This resulted in a downsized set chosen for experimentation: Boat_Ship, \\nCheering, Dancing, Demonstration_Or_Protest, Explosion_Fire, Running and Throwing. Table 1 \\nprovides detailed breakdowns of the positive and negative instances for these topics. Table 1: Number of Positive and Negative Instances for the TRECVid Data Topic Number of Positive Clips Number of Negative Clips \\nCClipsClipClipsBoat_Ship 950 13,963 Cheering 484 14,192\\nDancing 705 14,576\\nDemonstration_Or_Protest \\nPorotest 643 13,395\\nExplosion_Fire 899 16,116\\nRunning 637 14,677\\nThrowing 272 16,363 Some videos were excluded due to encoding errors that prevented processing. In addition, a large \\nportion of the videos did not contain audio, rendering the audio modules unusable. Therefore, \\nmultiple models were trained, some containing audio features and some not (see Appendix B for \\nfurther details on the training datasets used). Design Overview4\\nLinkBioMan consists of five main components: (1) the decision framework which contains the \\nmodules functioning as the analytic powerhouse of the system, (2) the scheduler which acts as a \\nstreaming platform, orchestrating information passing and cueing for the decision framework, (3) \\na user feedback loop which is integrated into the systems user interface, producing data that can \\nlater be aggregated and used to improve module performance, (4) a database for module result \\nstorage and retrieval and (5) the video handler which transcodes input video streams for vi processing by the modules. With the ContextNet module generating (semantic) context and alert \\nevery 3 seconds and LinkBioMan Captioner module running every 5 seconds based upon \\nprocessed input from all associated modules for ContextNet and LinkBioMan Captioner, the \\nLinkBioMan system can process videos in near-real-time. A sample 10-second clip was \\nprocessed at 30 frames per second (fps) using the above parameters in 10.006 seconds, which",
  "updated_at": "9/25/2014 12:00:00 AM",
  "created_at": "10/7/2016 12:00:00 AM"
}