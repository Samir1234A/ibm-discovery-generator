{
  "sdl_source_type": "PWS",
  "source_library": "SRC-supplicate",
  "file_name": "squadrons.ext",
  "document_url": "http://denseness'sSurabaya.com",
  "uploaded_by": "Aparecida f Larson",
  "last_modified": "10/1/2020",
  "sdl_date": "2020-02-29T00:00:00",
  "countryPublished": "Dominica",
  "conference": "Scrooge goatherd cn Wander",
  "originalAuthorName": "Fahad m Eckoldt",
  "title": "kayaked synthesis's mailing's",
  "declaredTags": "computational auditory perception|Critical Infrastructures|acquisition",
  "releaseReason": "airport/reorganization's",
  "docName": "GK_32_4154",
  "fundingCenter": 86,
  "resourceURL": "https://ductility.com",
  "fundingDepartment": "hf53",
  "caseNumber": "26-6723",
  "publicationDate": "8/9/2017 12:00:00 AM",
  "releaseYear": 2015,
  "releaseStatement": "Peer-reviewed Publication/Journal",
  "approver": "$Aizeti $Visser",
  "handCarry": 4,
  "authorDivision": "uy51",
  "copyrightOwner": "Visita Espirito-Santo",
  "lastModifiedDate": "8/29/2001 12:00:00 AM",
  "releaseDate": "12/18/2016 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "2840LNTO87",
  "materialType": "Book",
  "publicationType": "Article",
  "authorCenter": 71,
  "originalAuthorID": "Karamo",
  "mitrePublicServer": 0,
  "subjectTerminology": "Aviation Economic Analysis",
  "dateEntered": "5/25/2000 12:00:00 AM",
  "documentInfoURL": "https://flashiness garnish outsold newsreel predilections.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 17-3768|CASE1: 18-1446|CASE1: 17-3245",
  "organization": "go93",
  "authorDepartment": "je56",
  "publicationYear": 2014,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "MSR",
  "level1": "HR, Strat Comm & BD Ops/Dev",
  "fundingDivision": "fluoroscopes larboard's pesticides Elul's Venn's",
  "publishedOutsideUSA": 0,
  "level3": "jq53",
  "level2": "ep13",
  "sdl_id": "fd1cbd97177d4feaa7c0dad86ec3d7d9",
  "text": "    \"text\": \" Semi-Supervised Methods for Explainable Legal Prediction Semi-Supervised Methods for Explainable Legal Prediction Branting\\nPfaff\\nWeiss\\nBrown The MITRE Corporation McLean, VA, USA {lbranting,mpfaff,bweiss}@mitre.org Pfeifer\\nThe MITRE Corporation Ann Arbor, MI, USA cpfeifer@mitre.org Chakraborty\\nFerro\\nYeh The MITRE Corporation Bedford, MA, USA {achakraborty,lferro,asy}@mitre.org ABSTRACT Legal decision-support systems have the potential to improve ac- cess to justice, administrative efficiency, and judicial consistency, but broad adoption of such systems is contingent on development of technologies with low knowledge-engineering, validation, and maintenance costs. This paper describes two approaches to an important form of legal decision supportexplainable outcome predictionthat obviate both annotation of an entire decision cor- pus and manual processing of new cases. The first approach, which uses an Attention Network for prediction and attention weights to highlight salient case text, was shown to be capable of predict- ing decisions, but attention-weight-based text highlighting did not demonstrably improve human decision speed or accuracy in an evaluation with 61 human subjects. The second approach, termed SCALE (Semi-supervised Case Annotation for Legal Explanations), exploits structural and semantic regularities in case corpora to iden- tify textual patterns with predictable relationships to case decisions. ACM Reference Format: Branting, Pfaff, Weiss, Brown, Pfeifer, Chakraborty, Ferro, and Yeh. 2019. Semi-Supervised Methods for Explainable Legal Prediction. In Proceedings of ICAIL (ICAIL19). ACM, New York, NY, USA, 9 pages. https://doi.org/10. 1145/nnnnnnn.nnnnnnn 1 INTRODUCTION Recent advances in Artificial Intelligence (AI) and Human Language Technology (HLT) have created new opportunities to automate rou- tine aspects of case management and adjudication, freeing human experts to focus on aspects of these tasks that most require human judgment and knowledge. An important application of this technol- ogy is decision support for routine administrative decision-making and adjudication. Globally, benefits adjudications, resolution of commercial conflicts, criminal defense, and other forms of access to justice are often impeded by the opacity of legal processes, short- ages of affordable legal assistance, and growing case backlogs [18]. Effective decision-support systems could potentially improve access Permission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org. ICAIL19, June 2019, Montreal, QB, CA \\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn to justice for significant numbers of citizens by improving trans- parency, compensating for lack of affordable human legal assistance, and speeding case decisions. A particularly simple but useful form of decision support is explainable legal decision prediction. For example, benefits appli- cants could make better-informed decisions if they knew (1) the likelihood of success of an application and (2) the strengths or weaknesses of their application, i.e., the reasons for the predicted likelihood. Similarly, adjudicators and decision processes could be more productive and consistent if the strengths or weaknesses of each application could be automatically identified and presented along with the application itself. Inherent in explainable outcome prediction systems is a trade-off between explanation quality and development effort. At one extreme, purely machine-learning-based systems typically have little or no explanatory capability but require relatively little development ef- fort. At the other extreme, systems with case facts expressed in man- ually engineered features and legal rules expressed in executable logic may be capable of generating explanations with considerable fidelity to human explanations but require prohibitively high levels of development effort for application at scale. A key requirement for explainable decision predictions systems is therefore optimizing the trade-off between explanation quality and development effort, that is, identifying approaches that can produce useful and comprehensible predictions but forwhich the en- gineering effort is low enough to permit development, verification, and maintenance at scale. A particularly important requirement for large-scale adoption is the ability to accept free text rather than manually-engineered features as input. This paper describes two approaches to explainable legal deci- sion prediction that operate on textual inputs. Each system was prototyped on a collection of 16,092 World Intellectual Property Organization (WIPO) domain name dispute cases. The first system, NFE (No Feature Engineering), uses an Atten- tion Network [26] for prediction and highlights salient case text based on attention weights for decision support. NFE was evaluated as a decision aide for attorney and non-attorney subjects to predict the case decisions. While the case decisions themselves were found to be predictable using this approach, attention-weight-based text highlighting was not shown to improve decision speed or accu- racy. This negative result motivated a second approach, termed SCALE (Semi-supervised Case Annotation for Legal Explanations), in which the justification structure of a representative set of cases is annotated, and tags for factual and legal findings are propagated https://doi.org/10.1145/nnnnnnn.nnnnnnn\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn ICAIL19, June 2019, Montreal, QB, CA Branting, Pfaff, Weiss, Brown, Pfeifer, Chakraborty, Ferro, and Yeh Figure 1: Paradigms of explainable decision prediction. to sentences in unannotated cases that share a high degree of simi- larity to the annotated sentences in a semantic embedding space. This approach exploits the structural and semantic regularities in case corpora to identify fact patterns with predictable relationships to case decisions. The remainder of this paper is structured as follows. Section 2 discusses the inherent trade-off between explanation quality and development effort. A minimalist approach to feature engineering, NFE, is discussed in Section 3, and Section 4 discusses a semi- supervised approach, SCALE, motivated by the negative results of the evaluation of NFE. The annotation scheme is described in Sec- tion 7, and Section 6 discusses how annotations of a representative set of cases are mapped onto similar sentences from other cases in semantic embedding space for use in prediction and decision support. The paper concludes with related and future work. 2 THE TRADE-OFF BETWEEN EXPLANATION QUALITY AND DEVELOPMENT EFFORT Until the recent rise in popularity of legal applications of data sci- ence and deep learning, the primary focus of research in the AI and law community was on computational models of argumentation [8]. These models typically operated on manually engineered rep- resentations of case facts and produced argument trees or other formal justifications for legal conclusions [25]. Even when the ob- jective was simply prediction, rather than justification, case features were typically assigned to each case manually [19] [2]. In general, the only circumstance under which manual representation of the facts of each new case was obviated was when the user interface queried users for the value of each feature [28] [23] or when pre- diction was based on factors unrelated to the merits of the case, such as the attorneys, judge, cause of action, etc., [31]. When the features closely corresponded to users conception of relevant case facts, this approach was workable. However, this approach often requires a significant knowledge-engineering effort to identify a cognitively-plausible feature set [1]. Advances in machine learning and corpus-based techniques have made outcome prediction increasingly feasible, even in the absence of domain-specific features. Several projects have demonstrated the feasibility of predicting case outcomes from unprocessed text, using either deep learning techniques or applying symbolic ma- chine learning techniques to n-grams or other domain-independent lexical or linguistic text features, provided that there are sufficient training examples [3] [12] [30]. However, such systems have very limited inherent explanatory capability. Reducing the opacity of the machine learning algorithms that perform best for many purposes neural network models, often referred to as Deep Learningis the focus of very active current research, such as the DARPA Explain- able AI (XAI) program [17]. However, there has been little XAI work directed at the particular forms of explanation characteristic of legal discourse.1 Legal justifications and explanations differ from typical XAI applications in that they must make explicit reference to authoritative legal sources to be persuasive. One way to characterize the dependence of explanation qual- ity on prior engineering effort is by identifying the knowledge- engineering artifacts intermediary between the textual expression of the facts of a case and the output (e.g., a prediction with some degree of explanation or justification). Figure 1 sets forth a notional division of explainable prediction systems. In this figure, the term feature is intended to mean a fact pattern of potential legal rele- vance,2 whereas a legal predicate is a term or concept occurring as an antecedent or consequent a legal rule or norm. Paradigm 1 is a pure machine-learning approach requiring no knowledge engineering other than construction of an output-labeled corpus and producing little explanation beyond the prediction itself and the internal model weights associated with that prediction (e.g., attention weights on input subtexts). At the opposite extreme, Paradigm 7 can generate hybrid case-based and rule-based argu- mentation but requires manually engineered feature, issue, and rule sets and execution-time representation of new cases in terms of that feature set [24] [11]. The objective of the research described in this paper is to de- velop techniques for",
  "updated_at": "12/21/2011 12:00:00 AM",
  "created_at": "10/5/2014 12:00:00 AM"
}