{
  "sdl_source_type": "PRC",
  "sdl_date": "2020-11-20T00:00:00",
  "countryPublished": "Cambodia",
  "conference": "drivelled womanlike xi Marica",
  "originalAuthorName": "Conrad d Bozzo",
  "title": "cowslips pediatrist's cellophane's chimpanzee",
  "declaredTags": "Australian Signals Directorate|architecture|automate",
  "releaseReason": "insanity's/trouping",
  "docName": "LP_65_7401",
  "fundingCenter": 47,
  "resourceURL": "https://acrylic.com",
  "fundingDepartment": "lj56",
  "caseNumber": "83-8573",
  "publicationDate": "12/11/2018 12:00:00 AM",
  "releaseYear": 2008,
  "releaseStatement": "Peer-reviewed Publication/Journal",
  "approver": "$Orencio $Bruma",
  "handCarry": 6,
  "authorDivision": "jj72",
  "copyrightOwner": "Hermosinda Talavan",
  "lastModifiedDate": "2/21/2006 12:00:00 AM",
  "releaseDate": "3/18/2006 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "2307UYJT68",
  "materialType": "Paper",
  "publicationType": "Book",
  "authorCenter": 83,
  "originalAuthorID": "Abdelhakim",
  "mitrePublicServer": 0,
  "subjectTerminology": "Artificial Intelligence",
  "dateEntered": "4/15/2015 12:00:00 AM",
  "documentInfoURL": "https://affiliation outmanoeuvres openings colleen Springsteen's.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE3  17-2945",
  "organization": "kf42",
  "authorDepartment": "hj68",
  "publicationYear": 2009,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "CAASD Non-Direct Project",
  "level1": "HR, Strat Comm & BD Ops/Dev",
  "fundingDivision": "kerosene's browses complication Christina's Egyptology",
  "publishedOutsideUSA": 0,
  "level3": "yc28",
  "level2": "sl89",
  "sdl_id": "11bfc2659cc54116b1b9de793309931e",
  "text": "RDNH (Reverse Domain NameHijacking) and Harass (complaint brought primarily to harass DN holder). Each level of annotation also has an \\\"Other\\\" option to be used when none of the predefined tags is appropriate, and there is a free-form Comment field which the annotator can use to capture ad hoc labels and enter notes. 5.4 Attributes A Citation attribute is used to capture the paragraph citation of Policy and Case Rule argument elements. A polarity attribute is used to capture positive/negative values for issues and factors. Figure 7 shows four typical annotations. We anticipate that this annotated data set will be made available to researchers in 2019. 6 MAPPING ANNOTATIONS FOR PREDICTION AND EXPLANATION Linguistic annotation is an expensive and arduous task, but neces- sary to train and evaluate analytics. From a small set of 23 annotated documents, we are able to project the annotations to similar sen- tences throughout the entire corpus of documents. This projection is accomplished through the use of word and sentence embeddings to find text that is semantically similar to the annotated text. The projection method is as follows. https://www.icann.org/resources/pages/policy-2012-02-25-en#4\\nhttps://www.icann.org/resources/pages/udrp-rules-2015-03-11-en Semi-Supervised Methods for Explainable Legal Prediction ICAIL19, June 2019, Montreal, QB, CA First, word embeddings are trained on the tokenized corpus using FastText[9]. This yields a vector per token which captures the semantics of the word through the surrounding context. Next, these word embeddings are used to compute sentence embeddings by averaging the vectors of the words in each sentence for each of the 2.64 million sentences in our corpus. Semantically similar sentences are close to each other in semantic-embedding space. A notable limitation of this approach is sentences that are lexicallhy very similar but have opposite meaning end up very close in this embedding space. An example is simple negation via \\\"not\\\", for example \\\"the panel finds that it was properly constituted\\\" and \\\"the panel finds that it was not properly constituted\\\" differ by a single word but have completely different meanings. The sentences are then clustered into 512 clusters by their embeddings. The clusters establish neighborhoods of similar sentences. Once the word embeddings have been trained, embeddings for the annotation spans of text can be trained. The same method used to compute sentence embeddings is used to compute embeddings of the annotation spans. While the annotation spans are not strictly sentences, the sentence embedding method can be used to compute embeddings of arbitrary spans of text. Once the word embeddings, corpus sentence embeddings, an- notation span embeddings and clusters have been computed the tags can be projected. For each annotation label of interest for the specific experiment, we retrieve the top 10,000 sentences in the corpus ranked by cosine similarity to the annotated spans. Then, the annotation label is projected to each cluster associated with each retrieved sentence. For these prediction tasks, we do not use the words of the sen- tences. Instead, we use the cluster label of each sentence in the document. The sentences are selected according to task-specific cri- teria. XGBoost[16], an efficient implementation of gradient boosted machines, is used in all prediction tasks in this work. These are preliminary results, and we continue to iterate to improve the out- comes. As the transfer decision labels are highly skewed, 91% transfer (14,591 cases), 9% nontransfer (1,407 cases), we do not create a dedicated test set. Instead, we opt for 10 random test/train splits and report the average area under the curve (AUC), and per class precision, recall and F1 score aggregated over the 10 trials. 6.1 Predicting Decisions from Mapped Tags The accuracy of mapped tags as predictive features depends on both the annotation conventions and the details of the clustering. An initial evaluation of adequacy and correctness of these initial two steps can performed by determining the predictive accuracy of the mapped tags. If the tags are capturing the actual decision, then a high degree of accuracy should be achievable by training a model that predicts overall case decisions, or decisions for individual is- sues, from the mapped tags. This experiment used the tag projection method described in section 6, and retrieve sentences based on all annotation types. This method selected 1.8M sentences out of the total corpus of 2.6M. Predicting overall case outcome with the annotated data gave strong results with an average AUC of 80.8% and a standard devi- ation of 0.01. The transfer class, the majority class in this dataset, earned a 91.9% F1 (97.3% precision and 87.1% recall). The non- transfer class was lower with a 48.2% F1 (35.7% precision, 74.5% recall). This experiment indicates that tags mapped from a modest set of 23 annotated cases (0.14 percent of the entire corpus) are sufficient to express the decisions in the Findings section. 6.2 Predicting Decisions from Factual Findings Tags The next experiment involved restricting the prediction to just those tags that represent factual findings. The purpose of this ex- periment is to determine whether the factual findings are sufficient to determine the outcome of individual issues and of the case as a whole. This is important because the factual findings may be predictable from the case text, e.g., in the case of WIPO, all the sections preceding the panels discussion and findings. This experiment used the tag projection method described in section 6, and retrieved sentences only of factual finding annotation types. This method selected 1.3M sentences out of the total corpus of 2.6M sentences. Predicting overall case outcome with the annotated data gave strong results with an average AUC of 89.6% and a standard devia- tion of 0.008. The transfer class, the majority class in this dataset, earned a 94.3% F1 (98.8% precision, 90.2% recall). The non-transfer class was lower with a 61.2% F1 (46.7% precision, 89% recall). 6.3 Predicting Findings from Case Text In our next experiment, we measured our ability to predict tags in the Findings section from the case text, i.e., the first 5 sections of cases. We selected 3 findings: Identical or Confusingly Similar (ICS)- Complainant owns trademark, No Rights or Legitimate Interest (NRLI)-prima facie, and Bad Faith-disrupt business. These were the most frequently occurring findings in our corpus. To create the training data for this experiment, we used the tag projection method described in section 6. The projected sentence annotations were then interpreted as labels for a binary document classification task. Each finding label is skewed towards the negative class. The ICS label distribution is 75.8% negative and 24.2% positive, NRLI is 80.3% negative and 19.7% positive, finally bad faith label distribution is 74.2% negative and 25.8% positive. The prediction accuracy is similar for all labels. Over 10 trials, bad faith and NRLI earned an average AUC of 63.5% and 62.9% respectively, while ICS was a bit lower with 60.1% AUC. Examining the per class precision, recall and F1 metrics, bad faith had the strongest result with 79.8% F1 for the negative class (81.5% precision, 78.2% recall) and 46.3% F1 for the positive class (43.9% precision, 48.9% recall). ICS and NRLI had similar results. ICS had 79.5% F1, for the negative class (80.9% precision, 78.2% recall) and 40.2% F1 for the positive class (38.3% precision, 42.3% recall). Results for NRLI prediction are 78.1% for the negative class (86.5% precision, 71.1% recall) and 40.1% F1 for the positive class (31.7% precision, 54.6% recall). This section has described a methodology for inducing case features for decision prediction by exploiting regularities in case corpora. The use of these features for justification and explana- tion is beyond the scope of this paper. However, over 30 years of ICAIL19, June 2019, Montreal, QB, CA Branting, Pfaff, Weiss, Brown, Pfeifer, Chakraborty, Ferro, and Yeh scholarship has been directed at techniques, including case-based reasoning (CBR), dialectical argumentation, and rule-based justifi- cation, for justification, explanation, and argumentation in cases represented in terms of outcome-relevant features. The SCALE approach is agnostic as to which of these techniques should be ap- plied. The key research contribution of this work is enabling these techniques to apply to cases represented as text without requiring manual feature processing. 7 RELATEDWORK Several previous projects have addressed SCALEs goal of extracting factors from case texts for use in prediction or for other purposes. Ashley et al. 2009 [6] extracted CATO factors [5] from squibs, a form of case summary, using text classification [14] then predicted case outcomes by applying a machine learning algorithm, Issue-Based Prediction [15], to cases represented using those factors. SCALE differs from this approach in two important respects. First, SCALE uses features that arise from a linguists annotations of the Deci- sion portions of a representative sample of cases, whereas CATO factors were engineered and refined by multiple experts over a period of many years. Moreover, SCALE extracts factors from unre- stricted free text from cases, unlike [6], which processed only squibs, which were produced by experts. Accordingly, SCALE has both much less onerous feature development and broader applicability. Features defined and extracted by SCALEs much more highly au- tomated approach are almost certain to be less precise than CATO factors. However,",
  "updated_at": "9/2/2014 12:00:00 AM",
  "created_at": "3/20/2000 12:00:00 AM"
}