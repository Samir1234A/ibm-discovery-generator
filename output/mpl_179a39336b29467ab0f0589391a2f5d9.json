{
  "sdl_source_type": "MPL",
  "productName": "unreliable hooted",
  "uploadedate": "2010-08-19T00:00:00",
  "productUrl": "http://Communist.com",
  "creatorNames": "Derrick p Karras;Ignacia a Nata;Cristian i Timmer;Julio w Lasquibar",
  "uploaded": "2013-01-27T00:00:00",
  "sdl_extracted_summary": "is \\napproximately 0.9994x real-time. Note that the run time does not include module training time, \\nmodule load time or system startup time. Figure 3 shows an overview of the LinkBioMan \\nsystem. Refer to Figure 1 for a more detailed view of the decision framework (B in Figure 3). Decision Framework\\nAlert Caption Video/Audio \\nBuffer Database Scheduler Module Cues\\nt(i+1) HISTORY\\nModule Results t(0i-1) CURRENT\\nModule Results t(i)\\nPre-trained Models\\n(CNNs, SVMs, etc.) ALL\\nModule Results t(0i) Contexts User \\nFeedback B C D E Video \\nHandler A Figure 3: LinkBioMan Design \\n(A) The video handler transcodes the input video stream. (B) The decision framework then ingests the input \\nframes and audio buffers, producing results which are saved to (C) the database. (D) The scheduler ensures that \\nthe results from the decision framework are generated at the user-specified rates and synchronized across the \\nframework by controlling the video stream to each of the modules. (E) At run time, users can provide feedback \\non the decision framework output (alert, context and caption) for future module fine-tuning. At system startup, the scheduler dynamically builds the decision framework based on the user-\\nspecified system configuration, assigning separate worker processes to each module. Once \\ninitialization is complete, the video handler transcodes the input video stream and passes the \\ntranscoded frames and audio buffers to the scheduler. The scheduler then brokers and \\nsynchronizes the data streams for the modules, each of which typically processes the data at \\ndifferent rates. Any data queries or stores to the database are also handled by the scheduler, which \\nacts as the mediator among the decision framework, the database and the individual decision \\nframework modules.\\nThe decision framework itself is highly modular and houses the analytical capabilities of the vii system. Individual modules are categorized as either primary or secondary depending on their \\ninput. The input of primary modules is limited to the source data stream, while secondary modules \\nmay receive input from the data stream and other modules. This leads to a hierarchical module \\nstructure where modules further up-stream tend to extract low-level features from the data and \\ndown-stream modules work to fuse these low-level features in meaningful ways, ultimately \\nteasing out higher-level semantic concepts that can be used to alert, provide context to, or describe \\na scene to an operator. The modules, for the most part, rely on pre-trained deep neural network \\nmodels, the details of which are described in Table 2. These modules extract information ranging \\nfrom raw features which are interpretable to a machine (i.e. a vector of floats) to more semantic \\nfeatures such as entities (i.e. detected objects or attributes of objects in a frame) that are more \\neasily interpreted by human beings. \\nIn one of the system user interfaces, there is also an option for users to provide feedback on the \\nsystem performance. The feedback is automatically recorded in the system database and can be \\nextracted later by developers to fine-tune the models. Table 2: LinkBioMan Modules\\nThe table includes information on the input, output, and general description of each of the modules in the \\nLinkBioMan decision framework, which is graphically displayed in Figure 1. The right-most column of the \\ntable, titled Pre-Trained, indicates if the decision framework module is using a pre-trained model. If the \\ndecision framework module is not using a pre-trained model, then the model has been either fine-tuned or \\ntrained from scratch on an external dataset, as noted in the Description column. Module Description Input Output Pre-Trained\\nVideo Entity \\nClassification Performs image recognition, assigning class \\nprobabilities for the entire image. Using the \\nframework from [40] and trained on \\nImageNet [41]. Video buffer Image classification \\nprobabilities, raw Inception \\nfeatures Yes Video Entity \\nDetection Identifies multiple objects in an image \\nusing the application programing interface \\n(API) from [42], assigning a bounding box \\nand a class label for each object detected. \\nUsing the framework from [43] and trained \\non Open Images [44]. Video buffer List of detected objects, \\naggregated object \\nprobabilities Yes Video Scene \\nSegmentation Assigns labels to every pixel in the image, \\nresulting in masks for each region of the \\nimage. Using the framework from [12] and \\ntrained on [11]. Optimized framework for \\nspeed. Video buffer Scene areas, normalized by \\ntotal image area and \\naggregated object/scene \\nprobabilities No Video Scene \\nDetection Classifies the scene into one of 365 scene \\ntypes. Using the framework from [45] and \\nthe training set from [46]. Video buffer Scene classification \\nprobabilities Yes TVNet Optical Generates optical flow using a new Video buffer Optical flow frames Yes viii Flow alternative flow-computing deep net from \\n[47]. Two Streams \\nSpatial Spatial branch of a Two-Stream video \\nprocessing architecture for human activity \\nrecognition as described by [48], [49] and \\nimplemented in [50]. The spatial branch \\nprocesses visual information in the form of \\nRGB video frames. Network pre-trained on \\nthe UCF101 dataset provided by [51]. Video buffer Human activity \\nclassification scores, raw \\nResNet Features Yes Two Streams \\nMotion Motion branch of a Two-Stream video \\nprocessing architecture, which takes in \\nmotion information in the form of stacked \\noptical flow frames. Pre-trained network \\nprovided by [50]. TVNet \\nOptical Flow Human activity \\nclassification scores, raw \\nResnet features Yes Two Streams \\nCombined Module to fuse outputs of the Two-Stream \\nbranches using temporal aggregation \\nbased on [49]. Two Streams \\nSpatial/ \\nMotion Human activity \\nclassification scores Yes Audio Entity \\nClassification Detects 73 audio events spanning human \\nactivities, transportation, machinery, and \\nweather events. Uses [52], [53]. Audio \\nfeatures Audio entity probabilities No Audio Feature \\nExtraction Extracts feature embeddings from 1-\\nsecond windows of acoustic waveform. \\nUses [52], [54]. Audio buffer Audio feature embeddings Yes Video Entity \\nTracking Tracks objects through a video scene by \\nusing a combination of temporal tracking \\nand a color-based appearance model. Uses \\nKernelized Correlation Filters (KCF) [55]. Video buffer, \\nvideo entity \\ndetection List of tracked objects Yes Soft Biometrics Classifies individuals identified by video \\nentity detection modules using the \\nDeepMAR neural network [56] pretrained \\non the pedestrian attributes dataset [57]. \\nOutputs soft biometric descriptions of each \\ndetected person. Video buffer, \\nvideo entity \\ndetection Human soft biometric \\nclassification scores No Context and Alert: \\nContextNet Fusion module that combines inputs from \\nmany modules to predict probabilities for \\nalert and high-level semantic contexts. \\nContextNet is based on a framework for \\ntransfer-learning with multi-label problems \\n[58]. Multiple \\nmodules (see \\nFigure 1) Probabilities for semantic \\ncontexts and alert No Context: Random \\nForest Context \\nActivity Classifier Individual models trained to classify \\nsemantic contexts using video and audio \\nentities. Implemented with [53]. Video entity, \\naudio entity Class labels, probabilities \\nfor semantic contexts No ix Captioner Fusion module that combines inputs from \\nother modules to generate a natural \\nlanguage description of a given scene. \\nBased on Googles Show and Tell caption \\nframework, TensorFlow implementation \\n[4], [5], [59]. Multiple \\nmodules (see \\nFigure 1) Semantic scene \\ndescriptions listed by net \\nconfidence scores No Audio Feature Extraction and Audio Entity Classification 4.1\\nFigure 4 depicts audio processing for Audio Feature Extraction and Audio Entity Classification. \\nThe incoming audio stream is decoded from the MPEG-4 (MP4) file, converted to mono, down-\\nsampled to 8 kilohertz (kHZ) (if necessary) and added to a circular buffer. All audio processing \\nuses a 1-second analysis window, which is typical for non-speech audio event detection [52]. \\nWhen audio processing is initiated by the Scheduler, the Audio Buffer sends a 1-second audio \\nframe to the Audio Feature Extraction module, that converts the acoustic waveform into feature \\nvectors.\\nTwo feature sets are calculated from the acoustic waveform: VGGish embeddings and cortical \\nfeatures. These two feature sets capture different information from the acoustic waveform, with \\nthe former engineered using deep learning and the latter designed after the responses from cortical \\nneurons in the brain. We incorporate both feature sets as shown in Figure 4. Figure 4: Audio Processing \\n(A) Audio is extracted from the input video, pre-processed, and stored in a 1-second circular buffer. (B) Two \\nfeature sets, VGGish and cortical, are extracted from the audio waveform and sent, along with audio entity \\nprobabilities, to later stages of processing in the LinkBioMan Decision Framework (Refer to Figure 1). The VGGish embeddings are provided by Google [52], so named because their architecture is \\ninspired by comparable features in the visual domain known as VGG. These features are learned x by a Convolutional Neural Network (CNN) trained on a large corpus of audio to capture the \\nvariability present in real-world sound events. The result is a 128-feature vector for 1-second of \\naudio. The cortical features are spectro-temporal features based on the receptive fields of cortical \\nneurons measured in ferret [60]. These features (described in detail in [54]) are calculated by first \\ncomputing the cochleagram, a time-frequency representation (similar to a spectrogram) that \\nincorporates non-linearities present in the mammalian cochlea and auditory periphery. The \\ncochleagram is then filtered using a large bank of two-dimensional Gabor filters spanning different \\ntemporal and spectral modulations. The resulting representation is a 7680-dimensional feature \\nvector.\\nThe VGGish embeddings are sent to the Audio Entity Classification module; VGGish features \\nwere selected in lieu of cortical features after comparison of cross-validation performance. This \\nmodule is composed of a support vector machine (SVM) that outputs the probability that each of \\n73 audio entities is",
  "sdl_date": "2020-04-29T00:00:00",
  "countryPublished": "Morocco",
  "conference": "Suwanee's perspicacity's bc Venice",
  "originalAuthorName": "Eudosia l Arance",
  "title": "reprisal snow's",
  "declaredTags": "spectral clustering|Fusion Centers|data analytics|TAF|vulnerability",
  "releaseReason": "educated/halt",
  "docName": "ET_63_3612",
  "fundingCenter": 56,
  "resourceURL": "https://compatibility.com",
  "fundingDepartment": "lk63",
  "caseNumber": "34-6301",
  "publicationDate": "6/5/2018 12:00:00 AM",
  "releaseYear": 2014,
  "releaseStatement": "Public Collaboration/Benchmarking/Standards Body",
  "approver": "$Lanfen $Berolegui",
  "handCarry": 8,
  "authorDivision": "xe52",
  "copyrightOwner": "Nasima Yardin",
  "lastModifiedDate": "4/5/2009 12:00:00 AM",
  "releaseDate": "6/24/2002 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "5834HKVM25",
  "materialType": "Article",
  "publicationType": "Article",
  "authorCenter": 18,
  "originalAuthorID": "Temur",
  "mitrePublicServer": 0,
  "subjectTerminology": "Communications Technology (General)",
  "dateEntered": "7/12/2007 12:00:00 AM",
  "documentInfoURL": "https://mortar's observational lavatories township carbines.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 17-2739|CASE2: 17-2450",
  "organization": "xd67",
  "authorDepartment": "qr65",
  "publicationYear": 1994,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "CAMH FFRDC Contracts",
  "level1": "MITRE Public Sector",
  "fundingDivision": "normal's jag's trounced Greenland's amorality's",
  "publishedOutsideUSA": 0,
  "level3": "rs80",
  "level2": "hu83",
  "sdl_id": "179a39336b29467ab0f0589391a2f5d9",
  "text": "    \"text\": \" 'Approved for Public Release; Distribution Unlimited. Case Number 17-4389' Students Tackle Missile Defense and More (a Student Voices article) Have you ever analyzed space-based infrared systems and defense systems? Identified early warning \\nsigns of political or social unrest around the world? Used spectral clustering to reroute air traffic? \\nEmployed cognitive assistance techniques to attract co-op candidates? MITRE interns have. Several of them showcased their research at a student-focused event during the \\nsummer of 2017a highlight for the students and full-time employees alike. Improving Space-Based Infrared Systems Joshua Christ, a senior majoring in computer science with an emphasis on cybersecurity at California \\nState Fullerton, worked on a project that analyzed space-based infrared systems and early warning \\nmissile defense systems. His work supported MITREs efforts to improve our nations ability to detect and \\ndefend against incoming attacks. Part of that solution involved converting current documents to visual representations and aggregating \\nthem into one constructive diagram. The simpler visualization makes them easier to track at one time. \\\"We want to establish a good understanding of the system we're working with, Christ said during the \\nevent. And then we want to move on and create good baseline behavior and detection capabilities as \\nwell as improve the resiliency of the system. The end goal is to better identify the attack surface and \\nreduce some of the vectors of attack that bad actors or attackers could use to compromise the system \\nwe're working with.\\\" Recognizing Early Warning Indicators of Global Unrest Lindsey Lozoskie, a sophomore at the Hume Center for National Security at Virginia Tech, and Sophie \\nFaaborg-Andersen, a senior at the School of Foreign Service at Georgetown University, analyzed data \\nlooking for non-traditional early-warning indicators of political or social unrest around the world. The team worked with senior MITRE staff to develop a cross-disciplinary warning framework, integrating \\nfinancial, economic, political, and social data. The project outlined how data analysis of such indicators as \\noil prices, economic policies, investments, and media campaigns could be used to better anticipate \\nconflict or instability in a country or region. Improving Air Traffic Flows via Enhanced Spectral Clustering These days, theres increased demand for access to airspace, especially with the addition of commercial \\nspace operators. The need for methods to rapidly and accurately assess the impact of blocking the \\nairspaceas well as finding ways to quickly reroute air trafficis more important than ever. Nate Vollbrecht, a junior at the University of Michigan, developed a method, using a spectral clustering \\nalgorithm, to accurately assess heavily blocked traffic routes and quickly come up with alternative \\nroutings. Using techniques such as dynamic time warping and variable cluster sizing, he improved the \\naccuracy of the modeling. https://www.mitre.org/careers/student-programs/co-ops-interns 'Approved for Public Release; Distribution Unlimited. Case Number 17-4389' Using Cognitive Assistance to Connect Students to MITRE How do you effectively direct students to MITRE for co-op opportunities? Job fairs are great, but what do \\nyou do if the person staffing the booth is busy? Are there ways to engage a student after the fair is over? \\nAnd are there leave-behinds to help potential co-op candidates understand whether MITRE is right for \\nthem? Three of our co-ops spent the summer developing a solution. The teamPietari Sulkava, a senior at \\nRochester Institute of Technology, majoring in computer science; Paul Galatic, a junior majoring in \\ncomputer science at the Rochester Institute of Technology; and Ian Gross, a graduate student in \\ncomputer science at Rensselaer Polytechnic Institutethought cognitive assistance could help. Using IBM Bluemix and Watson and back-end discovery cognitive tools, the three developed programs \\nthat allow students to have conversations about MITRE and what it offers using ChatBot Client, the NAO \\ninteractive robot, or the Amazon Echo Show interface. The Amazon Echo Show, for example, allowed the student to discuss MITRE by saying \\\"Alexa, start a \\nconversation with MITRE.\\\" Students could also scan in a resume. The goal of the devices was to make co-op candidates learn whether the MITRE environment would be a \\ngood fit for them before stepping onto the campus or a MITRE site. by Tom Nutile If these or any of our many intern opportunities interest you, check out our current Job Openings or read \\nmore about our Student Programs. https://www.mitre.org/careers/job-openings/\\nhttps://www.mitre.org/careers/student-programs/student-voices 'Approved for Public Release; Distribution Unlimited. Case Number 17-4389' Paul Galatic explains how his team developed ways to connect students to MITRE for \\nco-op opportunities using cognitive assistance. (Photo by Michael Baker) http://info.mitre.org/people/app/person/26277#Phonebook _GoBack\\n _top\\n _Hlk497220109\\n _Hlk497220467\\n _Hlk497220677\\n _GoBack\\n _Hlk497220846\\n _Hlk496613984\\n _GoBack \",",
  "updated_at": "9/11/2020 12:00:00 AM",
  "created_at": "2/29/2000 12:00:00 AM"
}