{
  "sdl_source_type": "MPL",
  "productName": "pilot Chatterley",
  "uploadedate": "2018-05-12T00:00:00",
  "productUrl": "http://pyromania's.com",
  "creatorNames": "Andriu b Balanza;Yahima c Angelescu;Heradio e Vea Murguia;Neyen r Genswurger",
  "uploaded": "2014-08-15T00:00:00",
  "sdl_extracted_summary": "3-35 deviations from the slot markers in the feeders airspace. They reported that their goal was to\\nget all aircraft into their slot markers so that the handoff to the final controller would be\\nacceptable and that they felt uncomfortable when aircraft were not in the slot markers. Based on this feedback, it was decided that the slot marker color modification (shown in Figure\\n3-25) was a good initial change to test. Therefore, the chosen modification to test was the trail\\naircraft slot marker color change from white to blue during some scenarios when controller\\nmade the keyboard entry to indicate IM was active. The slot markers for trail aircraft remained\\nblue when an IM trail aircraft was in the suspended state but returned to white when controller\\nmade the keyboard entry to indicate IM was terminated. The slot markers for trail aircraft and\\nall other aircraft remained white. 3.1.3.3.4 Controller Tool Sets for Evaluation\\nThe following three IM tool sets were developed for the simulation to examine the controller\\ninformation needs: Basic: TSAS features, the IM clearance window, as well as the IM trail and lead aircraft\\nstatus fields in the data blocks Basic+ cue: The basic tool set plus the slot marker color change (cue) Basic+ cue and prediction: The basic+ cue tool set plus the spacing prediction value\\n(shown in parentheses after the lead aircraft identification in the IM clearance window) 3.1.4 Pseudo-Pilot Workstation\\nPseudo-pilots acted as pilots for all (some IM capable and some not IM capable) aircraft other\\nthan the participant flight crews aircraft. This allowed the controller to interact normally with\\nthe traffic. It also allowed aircraft to maneuver based on controller instructions, which is\\nreflected on both the controller and flight crew displays. The pseudo-pilots used an interface\\ntermed Simpilot which allowed users to simultaneously control multiple simulated aircraft\\n(Figure 3-29). It provided basic information about the aircraft (e.g., aircraft call sign, type) and\\nallowed the pseudo-pilot to control various aspects of the aircraft (e.g., heading, airspeed,\\naltitude, route, communications frequency) and respond to controller instructions by entering\\ncommands. The pseudo-pilots also initiated, suspended, resumed, and terminated IM per instructions from\\nthe controller. When the controller entered the command to accept the IM clearance, a pop-up\\nwindow with the clearance information was displayed to the pseudo-pilot. Once the controller\\nissued the IM clearance via a voice communication, the pseudo-pilot acknowledged the\\nclearance and then pressed ok in the window to engage the IM sample algorithm. This\\nimplementation was to assist the pseudo-pilot (who was managing several aircraft) in entering\\nall the information associated with the IM clearance. However, it made the acceptance and\\nentry of the clearance easier than it was for the participant aircraft flight crews and is more like\\na CPDLC implementation that allowed for loading of the IM clearance into the fight deck IM\\nequipment without the need to memorize or write down the information and then type it in. 3-36 Figure 3-29. Pseudo-Pilot Interface (Simpilot) 3-37 3.1.5 Airspace\\nThe airspace modeled for this simulation was based on Phoenix International Airport (KPHX).\\nNorth RNAV operations were run and aircraft landed to the west on runway 26, which was\\noperated as a landing only runway. Aircraft also landed on runway 25L but as independent\\noperations (even though real-world operations at KPHX are not run independently). The airport\\nenvironment is shown in Figure 3-30. The weather was visual meteorological conditions. Figure 3-30. PHX Airport. The two RNAV arrival procedures (BRUSR1 and EAGUL6) that were used were heavily based on\\ncurrent arrival procedures. Minor modifications were made to have the arrival connect to the\\ninstrument approach procedure. Modifications were also made to accommodate the RNP RF\\nturns. RNP RF turns onto final approach can be challenging for controllers but are expected to\\nbe supported by TSAS and will continue to be part of arrival and approach procedures in the\\nterminal metering environment (Wynnyk and Kopald, 2013). Therefore, they were examined to\\ndetermine the impact of IM. The airspace had one feeder (Apache airspace) and one final (Freeway airspace) position. Figure\\n3-31 provides an overview of the airspace. The waypoints and the associated altitude and\\nspeed constraints for the arrival procedures as shown in Figure 3-32. The TSAS constraint points\\nused in the simulation were RHYAN (RNP RF turn start), DERVL (merge point outside final), and\\nYOKXO (FAF). 3-38 Figure 3-31. PHX-Like Airspace Overview: Arrival Procedures and Controller Airspace Figure 3-32. Arrival Procedures 3-39 Based on the arrival and approach geometries, a mix of different types of aircraft pairings could\\noccur. For example, aircraft flying RNAV arrivals could follow aircraft flying the same or a\\ndifferent RNAV arrival. An aircraft flying the BRUSR1 arrival with the RNP RF turn could follow\\nanother aircraft flying the RNP RF turn, or it could follow an aircraft flying either RNAV arrival\\nwithout the RNP RF turn. Aircraft flying either arrival could follow an aircraft flying the BRUSR1\\narrival with the RNP RF turn. The geometries are shown in Figure 3-34 on the next page (with\\npotential IM pairings noted). One of the more interesting geometries is shown in Figure 3-33,\\nwhere an aircraft flying the BRUSR1 RNAV arrival is following an aircraft flying the BRUSR1 with\\nthe RNP RF turn. The sequence to the runway designed by TBFM can initially appear confusing if\\nthe RNP RF turn is not considered. Figure 3-33. Geometry Where RNAV Aircraft is Following RNP RF Turn Aircraft on the Same\\nArrival 3-40 Figure 3-34. Traffic Pairing Geometries 3-41 The IM operations were Multi-Stream Arrivals with Metering, as defined in Hicok and Barmore\\n(2016) (Figure 3-35). Figure 3-35. Sample IM Operations with the PTP and ABP Options 3.1.6 Traffic\\nSimulation traffic included the participants aircraft along with other aircraft arriving from the\\nnorth for the RNAV approach to Runway 26 (as shown previously in Figure 3-31). Aircraft also\\nlanded on runway 25L, but as independent operations. Attempts were made to develop traffic\\nflows that were high density and kept the controller workload at a high but reasonable level.\\nThe arrival rate was approximately 40 aircraft per hour to Runway 26. The higher workload\\nenvironment was desirable to keep the controllers engaged, but not so much that disturbances\\nin the traffic flow occurred (and vectoring became necessary). The intent was to examine the\\nintegration of IM operations in the terminal metering environment without disturbances. The\\ndensity of the flow was modified several times prior to selecting the final density for data\\ncollection based on controller and ground expert inputs during the concept evaluations.\\nEquivalent traffic levels were used through the entire simulation to avoid workload differences\\nbetween scenarios. Each traffic file was derived from real world traffic files with arrivals to\\nKPHX. At scenario start, traffic started arriving from the two flows and gradually built to the full\\ndensity of traffic that lasted until the data collection was complete for the scenario and the 3-42 scenario was terminated by one of the researchers (typically at the point where the participant\\naircraft landed and / or when the final off-nominal event occurred). All aircraft in the simulation were capable of flying the RNAV arrivals. There was a mixture of\\naircraft that were capable of flying the RNP RF turn (approximately 20%) and aircraft that were\\nnot capable of flying the RNP RF turn (approximately 80%). This RNP RF turn capability split was\\ncoordinated with key stakeholders and believed to represent a reasonable number of aircraft\\ncapable in the future of flying the more challenging routing. All aircraft in the simulation were ADS-B out and capable of as acting as an IM lead aircraft.\\nThere was a mixture of aircraft that were capable as acting as an IM trail aircraft (approximately\\n60%) and aircraft that were not capable as acting as IM aircraft (approximately 40%). This IM\\ncapability split was also coordinated with key stakeholders and believed to represent a\\nreasonable mix of aircraft. It was also an increase over past work done under ATD-1 (as\\nreviewed in Section 2.4.2.2). However, it is reasonably short of 100% equipage, which is unlikely\\nto be realistic in the near term and may be easier to manage based on not having to deal with\\nthe difficulties of mixed equipage. The participant aircraft was always IM capable (except in one\\nbaseline condition). Aircraft were delivered from the en route environment to the TRACON boundary with a set\\ndeviation around the center of the slot markers, which simulated the management of aircraft\\nby an en route controller prior to them entering the TRACON. Aircraft arrived no earlier than 30\\nseconds and no later than 15 seconds, with a distribution between those maximum values. The\\nexpected delivery for terminal metering operations is approximately +/- 30 seconds (Robinson\\net al., 2015). However, based on aircraft being able to more easily decelerate than accelerate in\\nthe TRACON, and the chosen arrival procedures, the 15-second threshold was chosen for this\\nactivity. This is consistent with a simulation reported in Callatine et al. (2014) in which en route\\ncontrollers were asked to deliver aircraft +/- 30 seconds with a preference for the early side. Pseudo-piloted aircraft were mainly large but also included heavy category aircraft. Aircraft\\ntypes included: Airbus A306, Airbus A319, Airbus A320, Airbus A321, Boeing 717-200, Boeing\\n737-300, Boeing B737-700, Boeing 737-800, Boeing 737-900, Boeing 767-300, Bombardier\\nCRJ200, Bombardier CRJ700, Bombardier CRJ900, McDonnell Douglas DC-10, McDonnell\\nDouglas MD-11, and McDonnell Douglas MD-90. Pseudo-piloted aircraft flew final approach\\nspeeds of either 140 or 150 kts6. The aircraft started to slow to the final approach speed at the\\nFAF. Pseudo-piloted aircraft that performed",
  "sdl_date": "2020-03-08T00:00:00",
  "countryPublished": "Bahrain",
  "conference": "Lebesgue's creeper dm Gilberta",
  "originalAuthorName": "Brittny d Strassmeier",
  "title": "relocated",
  "declaredTags": "continuous-wave|service|and economic flow of inbound traffic",
  "releaseReason": "disintegrated/tick's",
  "docName": "PY_70_7038",
  "fundingCenter": 95,
  "resourceURL": "https://glowworm's.com",
  "fundingDepartment": "wp50",
  "caseNumber": "61-9713",
  "publicationDate": "3/5/2017 12:00:00 AM",
  "releaseYear": 2002,
  "releaseStatement": "Other",
  "approver": "$Antonin $Usikov",
  "handCarry": 9,
  "authorDivision": "rv30",
  "copyrightOwner": "Momna Lyvin",
  "lastModifiedDate": "8/22/2012 12:00:00 AM",
  "releaseDate": "10/11/2008 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "5874SSXG55",
  "materialType": "Article",
  "publicationType": "Book",
  "authorCenter": 39,
  "originalAuthorID": "Haron",
  "mitrePublicServer": 0,
  "subjectTerminology": "Sensing and Signal Processing (General)",
  "dateEntered": "2/18/2014 12:00:00 AM",
  "documentInfoURL": "https://nongovernmental grommet suspending centaur's comradeship.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE4  14-0961|CASE1: 18-0296|CASE1: 17-4550",
  "organization": "bn81",
  "authorDepartment": "yq82",
  "publicationYear": 2004,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "NIST FFRDC Contracts",
  "level1": "Programs & Technology",
  "fundingDivision": "stylizes zestful restauranteurs Katowice's wiriness",
  "publishedOutsideUSA": 0,
  "level3": "dm89",
  "level2": "jx81",
  "sdl_id": "a65e438f4f4a4a5482955ed880992b88",
  "text": "present in the 1-second audio window. The SVM was trained on a subset of the \\nAudioset database [52], [61]; classes span many types of audio events occurring in public spaces, \\nsuch as, transportation and vehicles (e.g., aircraft, police siren, car horn), explosions (e.g., \\nfireworks, gunshot), weather sounds (e.g., rain, thunder, wind) and human activities (e.g., speech, \\nclapping, walking, shouting, laughter). Audio entity probabilities and labels from the Audio Entity \\nClassification module, along with both VGGish and cortical features, become inputs to \\nContextNet and the LinkBioMan Captioner, which incorporate these features with those from \\nother modules to derive higher-level contextual information. ContextNet4.2\\nThe ContextNet module is a fully connected network that is used to perform feature fusion on the \\noutputs of many primary and secondary modules. The ContextNet produces probabilities for alert \\nand semantic contexts. Alert probabilities are generated for experiments performed on the LBM \\nDataset only because it is the only dataset with Alert annotations. Figure 5 shows the ContextNet \\ndesign. The network architecture includes a dropout layer with the drop probability of 0.2 directly \\nafter the feature input layer. Using the dropout layer encourages ContextNet to explore \\nredundancies across the input feature space provided by different modules and not to overly rely \\non a small subset of the inputs. Following the dropout layer is a fully connected hidden layer with \\n1024 units and a Rectified Linear Unit (ReLU) activation function. The fully connected hidden \\nlayer is followed by another dropout layer with a 0.1 drop probability. This second dropout layer is \\nlastly followed by a fully connected layer for the output logits and a sigmoid activation function \\nproducing the final probabilities. We performed tests with additional hidden layers but found that \\nthey did not provide better performance on any of the datasets tested. L2 regularization was \\napplied to all weights. In both the LBM Dataset and the TRECVid Evaluation Dataset, most labels \\nhad a severe imbalance of positive and negative examples. To address this and create an unbiased \\nestimator, inverse-frequency weighting was used for each labels loss function. In all experiments, \\nContextNet was trained using TensorFlows Adam Optimizer for 2000 training steps, with a batch \\nsize of 100 random samples per step. xi Figure 5: ContextNet Design The ContextNet design shows that input modules can be easily swapped out for experimentation. Dropout \\nlayers are included only during training. The logits layer is sent through a sigmoid activation function to \\ngenerate the output probabilities. LinkBioMan Captioner4.3\\nThe LinkBioMan Captioner model modifies the Show and Tell Captioners [5] CNN-Long Short \\nTerm Memory (LSTM) layers to incorporate LinkBioMans multimodal features. The motivation \\nfor the modification is based on results from [3], [20], which demonstrated that incorporating \\nadditional information from multiple modalities (image, motion, audio) as well as semantic \\ninferences from other classifiers improved the overall captioner accuracy. The Show and Tell \\nCaptioner uses an encoder-decoder framework, where an (Inception v3) CNN is used to extract \\nsalient features from an image to encode, followed by a LSTM decoding the extracted features \\ninto a natural language representation. In comparison, the LinkBioMan Captioner (first version) \\nuses the extracted features from LinkBioMans primary and secondary modules plus their \\ninference scores, concatenates them with the image features from the pre-trained CNN of the \\nShow and Tell Captioner and run them through the LSTM decoder. Figure 6 shows the \\ncomparison between the Show and Tell Captioner and LinkBioMans Captioner (first version). \\nNote that the LSTM vector sizes and the combined CNN-LinkBioMan vector sizes are \\nproportional to the number of features that can be extracted from the LinkBioMan modules. As the \\ncombined CNN-LinkBioMan features are large and sparse, incorporating them directly into the \\nLSTM would drastically expand its cell size, risking overfitting. Therefore, only the LinkBioMan \\ninference scores that are more compact and summarize the detected entities extracted from each \\nLinkBioMan module are included in the combined CNN-LinkBioMan vector. The combined xii CNN-LinkBioMan vector is then fed into the LSTM of an expanded cell size. The LSTM decoder \\nis pre-trained on the original MSCOCO caption dataset [63] before being fine-tuned on our \\ncustomized LBM Dataset, using the following training parameters: sparse softmax cross entropy \\nloss, stochastic gradient descent with a learning rate of 2.0, decay rate of x0.5 per 8.0 epochs and \\nan LSTM dropout rate of 0.3. The result is a captioner that processes and translates the multimodal \\nfusion of video, audio, two streams (temporal, motion) and soft biometrics in less than 2 seconds. Inception v3\\nCNN Image Encoder LSTM Language \\nDecoder Embedding Layer Combined LinkBioMan \\nFeature Vector Audio \\nEncoder Attribute \\nEncoder Context \\nEncoder Combined CNN-\\nLinkBioMan Features Vector size:\\n2048 Vector size:\\n512 Vector size:\\n*1014 Vector size:\\n*1526 Cell size:\\n*1526 Vector size:\\n*Varies for each CNN Inception v3\\nCNN Image Encoder LSTM Language \\nDecoder Embedding Layer Vector size:\\n2048 Vector size:\\n512 Cell size:\\n512 Show and Tell Captioner LinkBioMan Captioner (First Version) LinkBioMan Multimodal Encoders Figure 6: Show and Tell Captioner vs. LinkBioMan Captioner (First Version) \\nThe diagram shows an overview of the original Show and Tell Captioner model compared with the \\nLinkBioMan Captioner (first version) model based off the Show and Tell Captioner [5]. The LinkBioMan \\nCaptioner (first version) uses LinkBioMans multimodal features and inference scores, that are concatenated \\nwith the pre-trained Show and Tell Captioners (Inception v3) CNN features before being fed into the LSTM. \\n*The LSTM cell size increases to support the combined CNN-LinkBioMan features (exact dimensions depend \\non LinkBioMan feature vector size: for an LinkBioMan vector size of 1014, the LSTM cell size expands from \\n512 to 1526). Preliminary experiments showed that the LinkBioMan Captioner (first version) model with \\nconcatenation did outperform the original Show and Tell Captioner in BLEU [64], ROUGE-L \\n[65] and METEOR [6] metrics (see Section 5.5 for details), but also suggested it may run the risk \\nof overfitting. This is partly due to our LBM Dataset being limited in size and the LSTM cell size \\nbeing expanded to incorporate the LinkBioMan features, resulting in a more complex LSTM \\ntrained with a more limited dataset than in the Show and Tell Captioners training scenario. To \\nresolve this issue, modifications to the LinkBioMan Captioner (first version) were made. First, the \\nfeatures extracted from each LinkBioMan module are concatenated and then fed through an \\nembedding layer. This embedding layer, consisting of a fully connected layer with a reduced cell \\nsize, is trained to compress and reduce the sparsity of the LinkBioMan features. Afterwards the \\nembedded features are then combined with the Show and Tell Captioners CNN features via \\nelement-wise summation before being fed into the LSTM, allowing the system to learn joint xiii relationships between the LinkBioMan features and the Show and Tell Captioners CNN features. \\nThis change allows us to preserve the original dimensions of the LSTM, further reducing the risk \\nof overfitting during the fine-tuning of the LSTM. Figure 7 shows the second version of \\nLinkBioMan Captioner. The LSTM cell size and the size of the input vector to the LSTM remain \\nthe same as those in the Show and Tell Captioner. Inception v3\\nCNN Image Encoder LSTM Language \\nDecoder Embedding Layer Inception v3\\nCNN Image Encoder LSTM Language \\nDecoder Embedding Layer Embedding Layer CNN \\nAudio Encoder CNN \\nAttribute \\nEncoder CNN \\nContext \\nEncoder LinkBioMan Multimodal Encoders Vector size:\\n2048 Vector size:\\n512 Cell size:\\n512 Vector size:\\n2048 Combined LinkBioMan Feature Vector Vector size:\\n512 Vector size:\\n1014 Vector size:\\n512 Cell size:\\n512 Vector size:\\n512 Element-wise Sum Show and Tell Captioner LinkBioMan Captioner (Second Version) Figure 7: Show and Tell Captioner vs. LinkBioMan Captioner (Second Version) \\nThe diagram shows an overview of the original Show and Tell Captioner model compared with the \\nLinkBioMan Captioner (second version) model based off the Show and Tell Captioner [5]. The LinkBioMan \\nCaptioner (second version) first compresses LinkBioMan features in an embedding layer before combining with \\nthe Show Tell Captioners CNN features via summation. The resulting feature vector size and LSTM cell size \\nremain the same as those in the Show and Tell Captioner [5]. Experiments and Analysis5\\nTo assess the efficacy of the LinkBioMan system, we conducted several experiments, which are \\ndetailed in the following subsections. Section 5.1 compares two approaches to classifying \\n(semantic) contexts using the LBM Dataset: ContextNet and Random Forests. Section 5.2 \\ndescribes the performance of LinkBioMans Alert with ContextNet. Section 5.3 compares the \\nperformance of ContextNet and Random Forests on the TRECVid Evaluation Dataset with the \\nreported results of the TRECVid performer teams. Section 5.4 conducts ablative tests with the \\nContextNet module to measure the contributions of each of its input modules. Sections 5.5 and 5.6 \\nmeasure the performance of the LinkBioMan Captioner using the LBM Dataset and the \\nYouTube2Text Dataset, respectively. Context Classifier: ContextNet Versus Random Forests5.1\\nUsing the LBM Dataset, we compared the performance of ContextNet against more traditional xiv random forests (ensemble of decision tree classifiers) for classifying contexts. For Random \\nForests, we trained separate forests for each context and alert using scikit-learns Random Forest \\nClassifier but found that its weight-based balancing system was unable to handle the high \\npositive/negative imbalance for some of our contexts. For this reason, it was necessary to train \\neach forest on a randomly sampled, balanced subset of the data. In this experiment, each",
  "updated_at": "8/1/1999 12:00:00 AM",
  "created_at": "5/11/2006 12:00:00 AM"
}