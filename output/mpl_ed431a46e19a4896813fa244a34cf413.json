{
  "sdl_source_type": "MPL",
  "productName": "sidelight impalement's",
  "uploadedate": "2017-09-04T00:00:00",
  "productUrl": "http://internist.com",
  "creatorNames": "Olinta d Krumsee;Es o Forsythe",
  "uploaded": "2012-07-23T00:00:00",
  "sdl_extracted_summary": "processes and activities, Fourth Ed., INCOSETP200300204. Project Management Institute (PMI), 2013, Standard for Program Management, Third Ed. U.S. Government Accountability Office, December 22, 2015, GAO Schedule Assessment Guide: Best Practices for Project Schedules, GAO-16-89G. http://www.gao.gov/products/GAO-16-89G\\nhttp://www.gao.gov/products/GAO-16-89G Integrated Master Schedule (IMS)/Integrated Master Plan (IMP) Application \",",
  "sdl_date": "2020-09-08T00:00:00",
  "countryPublished": "Madagascar",
  "conference": "exploitative megabyte's lm Sharice",
  "originalAuthorName": "Aliona c Chelminia",
  "title": "resonator glints",
  "declaredTags": "Jaya Tripathi interview|system capacity|MITRE Asia Pacific Singapore",
  "releaseReason": "Diem/filmmakers",
  "docName": "QL_66_3348",
  "fundingCenter": 22,
  "resourceURL": "https://bourgeois.com",
  "fundingDepartment": "bx34",
  "caseNumber": "78-9316",
  "publicationDate": "6/10/2017 12:00:00 AM",
  "releaseYear": 2015,
  "releaseStatement": "Public Collaboration/Benchmarking/Standards Body",
  "approver": "$Xianyong $Exon",
  "handCarry": 9,
  "authorDivision": "ar77",
  "copyrightOwner": "Ariam Jeuther",
  "lastModifiedDate": "5/21/2009 12:00:00 AM",
  "releaseDate": "9/6/2017 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "2970ESEH88",
  "materialType": "Article",
  "publicationType": "Article",
  "authorCenter": 53,
  "originalAuthorID": "Ferhat",
  "mitrePublicServer": 0,
  "subjectTerminology": "Computer Security",
  "dateEntered": "7/3/2015 12:00:00 AM",
  "documentInfoURL": "https://afternoon prepays courtliness's cognition's sublimation's.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE2: 16-1196",
  "organization": "sx90",
  "authorDepartment": "po26",
  "publicationYear": 1992,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "International Contracts",
  "level1": "Corporate Ops & Transformation",
  "fundingDivision": "Barbarella Kit's triggered profligacy's holidayed",
  "publishedOutsideUSA": 0,
  "level3": "qu14",
  "level2": "mr64",
  "sdl_id": "ed431a46e19a4896813fa244a34cf413",
  "text": "audio features. In this configuration we were \\nable to compare ContextNet with Random Forests and found that ContextNet achieved higher AP \\nscores in six out of seven topics. Our results also showed that removing audio led to a 1.3% \\ndecrease in the mAP score for ContextNet, showing that overall, audio is helping even though it is \\npartially missing. Note that there were some topics that had actually improved AP scores once the \\naudio was removed, such as Cheering, a topic that has clear connections to audio. This suggests \\nthat for more audio-centric topics, the partially missing audio is a larger hinderance than help. It \\nalso suggests that there are consistent visual indicators for Cheering that ContextNet can learn to \\nuse instead of audio features and that these visual indicators give more stable performance than the \\npartially missing audio features. In the next configuration Clips with Audio we saw that when \\naudio was consistently present, ContextNet performed better in Cheering, supporting the theory \\nthat the partially missing audio is to blame. In the final configuration Clips with Audio, we performed training and testing on only the subset \\nof video clips containing audio. The goal of Clips with Audio was to answer the second question \\nposed above; how well ContextNet and Random Forests could perform when audio was \\nconsistently available for all clips. We found that again ContextNet outperformed Random Forests \\nin all but two topics: Cheering and Running. It is interesting to note that Random Forests \\nachieved a significantly higher AP score on each of these two topics than any other system when \\nrunning on this subset of the data. In the previous section we theorized that, because of their \\nsimple splitting rules, Random Forests may excel in limited data situations when there is a small \\nnumber of very reliable features that can instantly distinguish the context. This theory may explain \\nthe result as well if there are certain audio features that instantly distinguish the Cheering and xviii Running topics. Both ContextNet and Random Forests had a significantly higher mAP score \\nwhen training and testing in Clips with Audio than in All Data (No Audio Used), achieving \\nmAP scores of 0.379 and 0.312, respectively. This shows that the consistent presence of audio \\nmakes the prediction of topics much easier. Note that because these experiments were done on a \\ndifferent subset of the TRECVid Evaluation Dataset, they may not provide an apples-to-\\napples comparison with the other results. For this reason, these results are marked with a \\ndifferent shape in Figure 9. We believe the results above show that ContextNet is more able to handle complex topics/contexts \\nthan Random Forests or TRECVid 2015 performing teams. Considering the All Data \\nconfiguration, ContextNet was able to greatly outperform any TRECVid 2015 performing teams \\nmethod for the complex topics, such as, Demonstration_Or_Protest and Explosion_Fire. \\nThese topics are more demanding than simple object or activity recognition topics like \\nBoat_Ship or Running, as they require a complex fusion of features to detect. Furthermore, \\nboth topics saw even further improvement when testing on the Clips with Audio configuration, \\nshowing that ContextNet can very effectively fuse audio into its decisions. Figure 9: AP for ContextNet Versus AP for Random Forest Versus infAP for TRECVid \\nTeams on the TRECVid Evaluation Dataset The diagram compares individual topic AP performance for ContextNet, Random Forests, and 86 other \\nTRECVid 2015 participating teams methods on seven of the NIST-annotated TRECVid 2015 topics. \\nNote that the scores reported for the 86 other methods were reported as infAP which approximates AP. \\nAlso note that the data points marked with diamonds were trained and tested only on clips including \\naudio; therefore, they may not provide an exact apples-to-apples comparison with the other data points. xix Ablation: ContextNet5.4\\nWe performed a series of ablative tests with the ContextNet module to measure the contributions \\nof each of its input modules. In these ablative tests, we gradually removed or corrupted parts of the \\ninputs of ContextNet, starting with individual modules and then moving on to groups of modules. \\nThe results are reported as mAP scores across all labels, including Alert. A fixed 75-25 Training-\\nTesting split was used in all experiments. Note that some clips in the LBM Dataset are sourced \\nfrom the same original video, meaning that the two samples are not independent. We took extra \\ncaution to ensure that clips from the same video were not spread between the training and testing \\nset. Additionally, some context labels in the LBM Dataset were very rare, having fewer than 50 \\npositive examples total. For this reason, we were careful to select a split that had a somewhat \\nbalanced distribution of positive instances of all labels in both the training and testing sets.\\nIn the first ablative experiment, we tested the performance of ContextNet when it was given inputs \\nfrom only a subset of the modules at both training and testing time. We will refer to these as \\nConstructive Ablative Tests as the modules are ablated before model construction. For \\nconsistency, the architecture and training protocol of ContextNet was kept constant regardless of \\nthe number of input features. We tested with individual modules and with groupings of modules, \\nsuch as RGB Video Only, Optical Flow Only, Audio Only, Semantic Features Only, and Non-\\nSemantic Features Only. The last two categories are a comparison between features with known \\nhuman-readable semantic labels (e.g. the logits of a CNN) and raw deeply-learned features, \\nrespectively. These deeply learned features are taken from the last fully connected layer directly \\nbefore the final output layer of a deep network. These feature vectors are much larger in size \\n(typically 2048) and may contain more general-purpose feature information than the semantically-\\nloaded output layers. The Semantic Features are: Video Scene Detection, Video Scene \\nSegmentation, Video Entity Classification, Video Entity Detection, Soft Biometrics, Two Streams \\nSpatial, Two Streams Motion, Two Streams Combined and Audio Entity. The Non-Semantic \\nFeatures are: Raw Inception Features, Two Streams (TS) Spatial Raw ResNet Features, TS \\nMotion Raw ResNet Features, Audio Feature VGGish and Audio Feature Cortical. Note that the \\nRaw Inception Features are extracted as part of the Video Entity Classification module. Figure 10 \\nshows the results. xx ALLRGB Video OnlyOptical Flow OnlyAudio onlyRGB Video + Optical FlowRGB Video + AudioOptical Flow + AudioSemantic Features OnlyNon-Semantic Features OnlyVideo Scene DetectionVideo Scene SegmentationVideo Entity ClassificationVideo Entity DetectionSoft BiometricsRaw Inception FeaturesTwo Streams SpatialTwo Streams MotionTwo Streams CombinedTS Spatial Raw ResNet FeaturesTS Motion Raw ResNet FeaturesAudio EntityAudio Feature VggishAudio Feature Cortical Modules Included 0 0.1 0.2 0.3 0.4 0.5 0.6 m\\nAP Constructive Ablative Tests mAP Scores Figure 10: Constructive Ablative Tests of ContextNet with the LBM Dataset \\nThe graph shows results for our Constructive Ablative Tests with ContextNet on the LBM Dataset. In these \\ntests, only a subset of the possible input features to ContextNet were given both at training and testing time. The \\nscores reported are mAP scores across all contexts and Alert. The x-axis states which modules or group of \\nmodules were included in the test. The color coding of bars represents the following: Orange = All Modules \\nIncluded, Blue = Group of Modules Included, Green = Single Module Included. Refer to Table 2 for further \\ndetails on the individual modules. The highest mAP score was achieved when all modules were included; however, some module \\nsubsets nearly matched this performance. When using RGB Video Only, the system achieved a \\nmAP score almost equal to the performance with all modules. This reflects human intuition about \\nthe reliance on vision. For example, even just a single still image of a protest would be sufficient \\nfor a human to recognize the demonstration/protest context. However, when using RGB Video \\nOnly, ContextNet had a much lower AP score in contexts that relied on auditory information (e.g. \\nscreaming) or motion information (e.g. running). The Non-Semantic Features also gave nearly \\nequal mAP, and a significantly higher score than the Semantic Features, showing that the raw \\ndeeply learned features may provide more information for ContextNet to use. This is further \\nsupported by the fact that the Raw Inception Features and Two Stream Spatial Raw ResNet \\nFeatures both achieved the highest mAP scores in isolation.\\nThere are some cases where adding more features led to a degradation of performance. For xxi example, the set of RGB Video + Optical Flow performed worse than RGB Video Only. This \\nsuggests that, despite the counter-measures taken, ContextNet is still susceptible to some \\noverfitting when increasing the size of the feature input space. More features make distances in \\nhigher-dimensional spaces become more similar, increasing the demand for a precise decision \\nboundary and making good classifications more difficult. We can see that among the three main \\ngroups of information (RGB video, optical flow, and audio), the most useful one by far is the RGB \\nvideo. After RGB video, the optical flow channel provides the second most information. Finally, \\nusing audio alone yields the lowest scores, however this is to be expected, as many videos in the \\ndataset include either no audio or super-imposed audio that is not relevant to the original scene.\\nThe second ablative tests were Corruptive",
  "updated_at": "4/30/2010 12:00:00 AM",
  "created_at": "12/5/1990 12:00:00 AM"
}