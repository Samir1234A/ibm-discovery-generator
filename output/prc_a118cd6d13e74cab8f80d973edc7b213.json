{
  "sdl_source_type": "PRC",
  "sdl_date": "2020-01-19T00:00:00",
  "countryPublished": "Luxembourg",
  "conference": "counterespionage's streamer ft Gema",
  "originalAuthorName": "Salvia s Vilken",
  "title": "pretentiousness's SVN's",
  "declaredTags": "Capital Markets|Area Navigation|operations|service levels",
  "releaseReason": "handkerchiefs/Ulster's",
  "docName": "IT_41_7073",
  "fundingCenter": 35,
  "resourceURL": "https://lallygag.com",
  "fundingDepartment": "rj23",
  "caseNumber": "12-9212",
  "publicationDate": "4/15/2019 12:00:00 AM",
  "releaseYear": 2011,
  "releaseStatement": "Advertising/Recruiting",
  "approver": "$Chakira $Cacace",
  "handCarry": 6,
  "authorDivision": "yu76",
  "copyrightOwner": "Galilea Stingaciu",
  "lastModifiedDate": "2/6/2011 12:00:00 AM",
  "releaseDate": "7/28/2019 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "7381RFQN55",
  "materialType": "Paper",
  "publicationType": "Book",
  "authorCenter": 23,
  "originalAuthorID": "Zouhra",
  "mitrePublicServer": 0,
  "subjectTerminology": "Program Development/Management",
  "dateEntered": "9/11/2004 12:00:00 AM",
  "documentInfoURL": "https://jujitsu's malingerer's Hobart's steward tippers.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 18-0935|CASE1: 16-2039",
  "organization": "mh36",
  "authorDepartment": "mu89",
  "publicationYear": 1995,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "DoD FFRDC Contracts",
  "level1": "Programs & Technology",
  "fundingDivision": "gluts irresolution's beauty vetoing Rayburn's",
  "publishedOutsideUSA": 0,
  "level3": "hp28",
  "level2": "ki31",
  "sdl_id": "a118cd6d13e74cab8f80d973edc7b213",
  "text": "is \\napproximately 0.9994x real-time. Note that the run time does not include module training time, \\nmodule load time or system startup time. Figure 3 shows an overview of the LinkBioMan \\nsystem. Refer to Figure 1 for a more detailed view of the decision framework (B in Figure 3). Decision Framework\\nAlert Caption Video/Audio \\nBuffer Database Scheduler Module Cues\\nt(i+1) HISTORY\\nModule Results t(0i-1) CURRENT\\nModule Results t(i)\\nPre-trained Models\\n(CNNs, SVMs, etc.) ALL\\nModule Results t(0i) Contexts User \\nFeedback B C D E Video \\nHandler A Figure 3: LinkBioMan Design \\n(A) The video handler transcodes the input video stream. (B) The decision framework then ingests the input \\nframes and audio buffers, producing results which are saved to (C) the database. (D) The scheduler ensures that \\nthe results from the decision framework are generated at the user-specified rates and synchronized across the \\nframework by controlling the video stream to each of the modules. (E) At run time, users can provide feedback \\non the decision framework output (alert, context and caption) for future module fine-tuning. At system startup, the scheduler dynamically builds the decision framework based on the user-\\nspecified system configuration, assigning separate worker processes to each module. Once \\ninitialization is complete, the video handler transcodes the input video stream and passes the \\ntranscoded frames and audio buffers to the scheduler. The scheduler then brokers and \\nsynchronizes the data streams for the modules, each of which typically processes the data at \\ndifferent rates. Any data queries or stores to the database are also handled by the scheduler, which \\nacts as the mediator among the decision framework, the database and the individual decision \\nframework modules.\\nThe decision framework itself is highly modular and houses the analytical capabilities of the vii system. Individual modules are categorized as either primary or secondary depending on their \\ninput. The input of primary modules is limited to the source data stream, while secondary modules \\nmay receive input from the data stream and other modules. This leads to a hierarchical module \\nstructure where modules further up-stream tend to extract low-level features from the data and \\ndown-stream modules work to fuse these low-level features in meaningful ways, ultimately \\nteasing out higher-level semantic concepts that can be used to alert, provide context to, or describe \\na scene to an operator. The modules, for the most part, rely on pre-trained deep neural network \\nmodels, the details of which are described in Table 2. These modules extract information ranging \\nfrom raw features which are interpretable to a machine (i.e. a vector of floats) to more semantic \\nfeatures such as entities (i.e. detected objects or attributes of objects in a frame) that are more \\neasily interpreted by human beings. \\nIn one of the system user interfaces, there is also an option for users to provide feedback on the \\nsystem performance. The feedback is automatically recorded in the system database and can be \\nextracted later by developers to fine-tune the models. Table 2: LinkBioMan Modules\\nThe table includes information on the input, output, and general description of each of the modules in the \\nLinkBioMan decision framework, which is graphically displayed in Figure 1. The right-most column of the \\ntable, titled Pre-Trained, indicates if the decision framework module is using a pre-trained model. If the \\ndecision framework module is not using a pre-trained model, then the model has been either fine-tuned or \\ntrained from scratch on an external dataset, as noted in the Description column. Module Description Input Output Pre-Trained\\nVideo Entity \\nClassification Performs image recognition, assigning class \\nprobabilities for the entire image. Using the \\nframework from [40] and trained on \\nImageNet [41]. Video buffer Image classification \\nprobabilities, raw Inception \\nfeatures Yes Video Entity \\nDetection Identifies multiple objects in an image \\nusing the application programing interface \\n(API) from [42], assigning a bounding box \\nand a class label for each object detected. \\nUsing the framework from [43] and trained \\non Open Images [44]. Video buffer List of detected objects, \\naggregated object \\nprobabilities Yes Video Scene \\nSegmentation Assigns labels to every pixel in the image, \\nresulting in masks for each region of the \\nimage. Using the framework from [12] and \\ntrained on [11]. Optimized framework for \\nspeed. Video buffer Scene areas, normalized by \\ntotal image area and \\naggregated object/scene \\nprobabilities No Video Scene \\nDetection Classifies the scene into one of 365 scene \\ntypes. Using the framework from [45] and \\nthe training set from [46]. Video buffer Scene classification \\nprobabilities Yes TVNet Optical Generates optical flow using a new Video buffer Optical flow frames Yes viii Flow alternative flow-computing deep net from \\n[47]. Two Streams \\nSpatial Spatial branch of a Two-Stream video \\nprocessing architecture for human activity \\nrecognition as described by [48], [49] and \\nimplemented in [50]. The spatial branch \\nprocesses visual information in the form of \\nRGB video frames. Network pre-trained on \\nthe UCF101 dataset provided by [51]. Video buffer Human activity \\nclassification scores, raw \\nResNet Features Yes Two Streams \\nMotion Motion branch of a Two-Stream video \\nprocessing architecture, which takes in \\nmotion information in the form of stacked \\noptical flow frames. Pre-trained network \\nprovided by [50]. TVNet \\nOptical Flow Human activity \\nclassification scores, raw \\nResnet features Yes Two Streams \\nCombined Module to fuse outputs of the Two-Stream \\nbranches using temporal aggregation \\nbased on [49]. Two Streams \\nSpatial/ \\nMotion Human activity \\nclassification scores Yes Audio Entity \\nClassification Detects 73 audio events spanning human \\nactivities, transportation, machinery, and \\nweather events. Uses [52], [53]. Audio \\nfeatures Audio entity probabilities No Audio Feature \\nExtraction Extracts feature embeddings from 1-\\nsecond windows of acoustic waveform. \\nUses [52], [54]. Audio buffer Audio feature embeddings Yes Video Entity \\nTracking Tracks objects through a video scene by \\nusing a combination of temporal tracking \\nand a color-based appearance model. Uses \\nKernelized Correlation Filters (KCF) [55]. Video buffer, \\nvideo entity \\ndetection List of tracked objects Yes Soft Biometrics Classifies individuals identified by video \\nentity detection modules using the \\nDeepMAR neural network [56] pretrained \\non the pedestrian attributes dataset [57]. \\nOutputs soft biometric descriptions of each \\ndetected person. Video buffer, \\nvideo entity \\ndetection Human soft biometric \\nclassification scores No Context and Alert: \\nContextNet Fusion module that combines inputs from \\nmany modules to predict probabilities for \\nalert and high-level semantic contexts. \\nContextNet is based on a framework for \\ntransfer-learning with multi-label problems \\n[58]. Multiple \\nmodules (see \\nFigure 1) Probabilities for semantic \\ncontexts and alert No Context: Random \\nForest Context \\nActivity Classifier Individual models trained to classify \\nsemantic contexts using video and audio \\nentities. Implemented with [53]. Video entity, \\naudio entity Class labels, probabilities \\nfor semantic contexts No ix Captioner Fusion module that combines inputs from \\nother modules to generate a natural \\nlanguage description of a given scene. \\nBased on Googles Show and Tell caption \\nframework, TensorFlow implementation \\n[4], [5], [59]. Multiple \\nmodules (see \\nFigure 1) Semantic scene \\ndescriptions listed by net \\nconfidence scores No Audio Feature Extraction and Audio Entity Classification 4.1\\nFigure 4 depicts audio processing for Audio Feature Extraction and Audio Entity Classification. \\nThe incoming audio stream is decoded from the MPEG-4 (MP4) file, converted to mono, down-\\nsampled to 8 kilohertz (kHZ) (if necessary) and added to a circular buffer. All audio processing \\nuses a 1-second analysis window, which is typical for non-speech audio event detection [52]. \\nWhen audio processing is initiated by the Scheduler, the Audio Buffer sends a 1-second audio \\nframe to the Audio Feature Extraction module, that converts the acoustic waveform into feature \\nvectors.\\nTwo feature sets are calculated from the acoustic waveform: VGGish embeddings and cortical \\nfeatures. These two feature sets capture different information from the acoustic waveform, with \\nthe former engineered using deep learning and the latter designed after the responses from cortical \\nneurons in the brain. We incorporate both feature sets as shown in Figure 4. Figure 4: Audio Processing \\n(A) Audio is extracted from the input video, pre-processed, and stored in a 1-second circular buffer. (B) Two \\nfeature sets, VGGish and cortical, are extracted from the audio waveform and sent, along with audio entity \\nprobabilities, to later stages of processing in the LinkBioMan Decision Framework (Refer to Figure 1). The VGGish embeddings are provided by Google [52], so named because their architecture is \\ninspired by comparable features in the visual domain known as VGG. These features are learned x by a Convolutional Neural Network (CNN) trained on a large corpus of audio to capture the \\nvariability present in real-world sound events. The result is a 128-feature vector for 1-second of \\naudio. The cortical features are spectro-temporal features based on the receptive fields of cortical \\nneurons measured in ferret [60]. These features (described in detail in [54]) are calculated by first \\ncomputing the cochleagram, a time-frequency representation (similar to a spectrogram) that \\nincorporates non-linearities present in the mammalian cochlea and auditory periphery. The \\ncochleagram is then filtered using a large bank of two-dimensional Gabor filters spanning different \\ntemporal and spectral modulations. The resulting representation is a 7680-dimensional feature \\nvector.\\nThe VGGish embeddings are sent to the Audio Entity Classification module; VGGish features \\nwere selected in lieu of cortical features after comparison of cross-validation performance. This \\nmodule is composed of a support vector machine (SVM) that outputs the probability that each of \\n73 audio entities is",
  "updated_at": "9/16/2011 12:00:00 AM",
  "created_at": "7/25/1994 12:00:00 AM"
}