{
  "sdl_source_type": "MIP Projects",
  "chargeCode": "wjr",
  "longName": "qecojfewlk-1453",
  "endDate": "2002-12-22T00:00:00",
  "phonebookDisplayName": "Garoa Water",
  "startDate": "2007-02-25T00:00:00",
  "status": "Cancelled",
  "portfolios": "Fuji|tailpipe's|noisiness's|pyre's|webcasting",
  "name": "wrangled ascendents",
  "public_url": null,
  "project_url": "http://gusseting.com",
  "sdl_date": "2020-07-10T00:00:00",
  "countryPublished": "Chad",
  "conference": "esthete Charmin's ch Itxaro",
  "originalAuthorName": "Todorova x Hergger",
  "title": "schizoid's prickle's Subaru millrace's",
  "declaredTags": "APM framework|sUAS|CyCS",
  "releaseReason": "foretell/oilfields",
  "docName": "RI_11_7549",
  "fundingCenter": 61,
  "resourceURL": "https://surrealists.com",
  "fundingDepartment": "uv31",
  "caseNumber": "82-8841",
  "publicationDate": "4/2/2018 12:00:00 AM",
  "releaseYear": 2015,
  "releaseStatement": "Publicity/Promotion",
  "approver": "$Teena $Talikov",
  "handCarry": 8,
  "authorDivision": "pl71",
  "copyrightOwner": "Ralf El Miri",
  "lastModifiedDate": "4/19/2014 12:00:00 AM",
  "releaseDate": "5/1/2015 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "6791VYAP95",
  "materialType": "Paper",
  "publicationType": "Paper",
  "authorCenter": 91,
  "originalAuthorID": "Nayla",
  "mitrePublicServer": 0,
  "subjectTerminology": "Sensing and Signal Processing (General)",
  "dateEntered": "9/24/2014 12:00:00 AM",
  "documentInfoURL": "https://outpost's Adrian continent tensile laptop.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 16-4196",
  "organization": "we69",
  "authorDepartment": "pa76",
  "publicationYear": 2000,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "FAA FFRDC Contracts",
  "level1": "Programs & Technology",
  "fundingDivision": "gravitational musicologists Ukraine diabetic Bantu's",
  "publishedOutsideUSA": 0,
  "level3": "qs59",
  "level2": "ni61",
  "sdl_id": "0385adfcb9354a9d8e095c6c98b0c4b9",
  "text": "    \"text\": \" APPROVED FOR PUBLIC RELEASE; DISTRIBUTION UNLIMITED. \\nPUBLIC RELEASE CASE NUMBER 18-3508 MITRE TECHNICAL REPORT: MTR180617 LinkBioMan: Delivering Actionable Context to \\nMultimodal Video Understanding October 2018 Dr. Monica Carley-Spencer\\nAdithya Dattatri\\nHoward Huang\\nChongeun Lee\\nBenjamin M. Skerritt-Davis\\nAmanda C. Vu\\nMatthew R. Walmer\\nMarc S. Zebrowitz The views, opinions and/or findings contained in this report are those of \\nThe MITRE Corporation and should not be construed as an official \\nGovernment position, policy, or decision, unless designated by other \\ndocumentation. 2018 The MITRE Corporation. All Rights Reserved. Corporate Headquarters\\nMcLean, Virginia ii iii Abstract\\nSince most multimedia context extraction models deal with benign, simple and often trivial \\ncontexts, there is a need for a decision framework capable of processing complex topics and \\nextracting an actionable context that can aid in public area safety and in post-event investigation \\nfor law enforcement or national/international security. The framework should be able to process \\nmultiple data streams, combining lower-level sensory inferences into a human-level interpretation \\nof events. In this paper, we introduce Linking Soft Biometrics to Semantic Description of an Event \\n(LinkBioMan), a multimodal near-real-time decision framework capable of producing \\nactionable context, a preliminary interpretation of events and automated alerting. With the \\nYouTube2Text dataset, LinkBioMan's Captioner achieves a maximum METEOR score of 0.3183 \\nand a maximum CIDEr score of 0.7061, which is in line with the performance of other video \\ncaptioners. We also demonstrate the effectiveness of LinkBioMan's context classification module, \\nContextNet, which outperforms NISTs 2015 TRECVid evaluation team scores for the Semantic \\nIndexing challenge. LinkBioMans ContextNet achieves a mean Average Precision (mAP) score \\nof 0.325 (0.379 using a subset of TRECVid data with audio) in relevant categories, compared with \\nthe TRECVid 2015 teams' maximum mAP score of 0.273 in the same categories. Ablative tests on \\nContextNet demonstrate the critical contribution of raw visual features and show that motion and \\naudio boost performance in certain categories of semantic concepts depending on the nature of the \\ncontext. LinkBioMans multimodal fusion strategy at the decision level allows it to process \\ncomplex, multimodal data streams to generate an understanding of scenes ranging from everyday \\nevents to life-threating emergencies. iv v Table of Contents \\n1 INTRODUCTION \\n3.2 YOUTUBE2TEXT DATASET \\n3.3 TRECVID EVALUATION DATASET \\n4.2 CONTEXTNET \\n4.3 LINKBIOMAN CAPTIONER \\n5.2 ALERT: CONTEXTNET ON THE LBM DATASET \\n5.3 CONTEXT CLASSIFIER: CONTEXTNET, RANDOM FORESTS ON THE TRECVID EVALUATION DATASET \\n5.4 ABLATION: CONTEXTNET \\n5.5 CAPTIONER: ON THE LBM DATASET \\n5.6 CAPTIONER: ON THE YOUTUBE2TEXT DATASET \\nAppendix B Dataset Details \\nB.2 All Data (Minus Unreadable Clips) \\nB.3 Clips with Audio \\nAppendix D Acronyms and Abbreviations \\nFIGURE 1: LINKBIOMAN DECISION FRAMEWORK \\nFIGURE 2: SAMPLE FRAMES OF LBM DATASET VIDEO CLIPS \\nFIGURE 3: LINKBIOMAN DESIGN \\nFIGURE 4: AUDIO PROCESSING \\nFIGURE 5: CONTEXTNET DESIGN \\nFIGURE 6: SHOW AND TELL CAPTIONER VS. LINKBIOMAN CAPTIONER (FIRST VERSION) \\nFIGURE 7: SHOW AND TELL CAPTIONER VS. LINKBIOMAN CAPTIONER (SECOND VERSION) \\nFIGURE 8: AVERAGE PRECISION AND MEAN AVERAGE PRECISION: CONTEXTNET VERSUS RANDOM FORESTS \\nFIGURE 9: AP FOR CONTEXTNET VERSUS AP FOR RANDOM FOREST VERSUS INFAP FOR TRECVID TEAMS ON THE TRECVID EVALUATION DATASET \\nFIGURE 10: CONSTRUCTIVE ABLATIVE TESTS OF CONTEXTNET WITH THE LBM DATASET \\nFIGURE 11: CORRUPTIVE ABLATIVE TESTS OF CONTEXTNET WITH THE LBM DATASET \\nTABLE 1: NUMBER OF POSITIVE AND NEGATIVE INSTANCES FOR THE TRECVID DATA \\nTABLE 2: LINKBIOMAN MODULES \\nTABLE 3: COMPARISON OF CONTEXTNET (CN), RANDOM FORESTS (RF) AND BEST TRECVID TEAM ON THE TRECVID EVALUATION DATASET \\nTABLE 4: CAPTIONER PERFORMANCE ON THE LBM DATASET \\nTABLE 5: CAPTIONER PERFORMANCE ON THE YOUTUBE2TEXT DATASET AND YOUTUBE2TEXT CLIPS WITH AUDIO \\nA surge of compact and affordable multimedia recording devices has enabled virtually anyone to \\ngenerate images, audio recordings and videos anywhere and anytime. This media has usages \\nranging from personal memorabilia, to more critical matters, such as, real-time monitoring for \\nphysical property, surveillance for border protection, public area safety and post-event \\ninvestigations for law enforcement or national/international security. However, the amount of data \\nproduced by such everyday sensors has far surpassed the available human resources needed to \\nprocess, analyze and formulate a timely response. For example, a command center with dozens of \\nscreens playing surveillance video feeds would need to staff a designated operator for each \\nindividual monitor, and even for trained operators, the effectiveness would be limited by their \\nability to maintain the required level of alertness, especially when a majority of the content is \\nbenign/unactionable. Such limitations call for an automated assistive system that can not only \\nprocess the data at the low sensory level (e.g., object detection), but also derive the global semantic \\ncontext from the sensory output. Many analytics tools exist that can detect, classify, query, \\ndescribe, summarize or even supply answers to asked questions (e.g., visual Q&A), but their \\ncontexts are limited to the low sensory levels (e.g., whether the image contains a car), rather than \\nthe actual meaning of the scene. Even those with soft or hard attention mechanisms are limited to \\nthe prominent foreground objects and fall short in connecting to unattended entities, thus not \\ndelivering the actual context of the scene [1][3]. \\nIn this paper, we propose Linking Soft Biometrics to Semantic Description of an Event, \\nLinkBioMan a multimodal near-real-time decision framework capable of producing actionable \\ncontext. It ingests uncontrolled collections of video1 (with or without audio) from a variety of \\nmobile or fixed sensors and generates a preliminary interpretation of events (What is happening?) \\nand automated alerting (Is something happening that demands an immediate response?). The \\nsystem is currently trained in the domain of peoples behaviors and activities in public gathering \\nplaces (e.g., riots, demonstrations, sporting events, concerts, parades) and public transportation \\nareas (e.g., seaports, airports, train stations), and can be expanded to interpret concepts in other \\ndomains. It is an assistive, automated framework that continually incorporates human feedback to \\nimprove its accuracy. Video \\nBuffer Captioner Alert Contexts Video Scene \\nDetection Video Scene \\nSegmentation Video Entity \\nDetection Video Entity \\nClassification Soft \\nBiometrics ContextNet Video Entity \\nTracking Caption Two Streams \\nSpatial TVNet \\nOptical \\nFlow Two Streams\\nCombined Two Streams \\nMotion Audio \\nBuffer Audio \\nFeature Extraction\\nAudio Entity Classification Input OutputDecision Framework Figure 1: LinkBioMan Decision Framework \\nThe diagram shows a hierarchical module structure of LinkBioMan decision framework. The gray modules \\nrepresent data streams as input to subsequent modules. The modules that are different hues of green and blue \\n(e.g., TVNet Optical Flow, Two Streams Spatial, Audio Feature Extraction) are primary modules extracting low-\\nlevel features from the source data stream. The apricot modules (e.g., Two Streams Combined) are secondary \\nmodules that fuse the low-level features from primary modules for semantic features, which are more easily \\ninterpreted by human beings. The orange modules produce output to be displayed on the user interface. See \\nTable 2 for further details on modules. The biggest differentiator between LinkBioMan and existing video analytics is that it is \\ndesigned to derive the semantic context from a video scene beyond the low sensory level and \\nrender a near-real-time alert, hereafter referred to as actionable context. Our goal was to develop a \\nsystem that can detect, classify and connect objects, people, activities and environments to provide \\na relevant, meaningful and actionable interpretation of multimodal sensor feeds. LinkBioMan \\nachieves this goal by using a modular approach, as shown in Figure 1, allowing maximum \\nflexibility to accommodate diverse types of sensors, domains and goals. For example, if the \\ndomain is port security and the goal is to detect vessels and to alert when they approach a port \\nfrom a particular direction, then some of the processing modules would not be necessary, such as \\nSoft Biometrics or Two Streams Combined. This modularity makes the LinkBioMans decision \\nframework flexible and customizable to allow for tackling a wide variety of use cases. In this \\npaper, we quantitatively demonstrate how each module contributes to deriving the overall \\nsemantic context present in the input feed.\\nThe LinkBioMan Captioner is a video captioner that augments the image captioning tool from ii Show and Tell [4], [5], henceforth called Show and Tell Captioner, allowing fusion with \\nadditional sensor feeds and semantic contexts from other domains. We were able to achieve a \\nmaximum METEOR [6] score of 0.3183 and a maximum CIDEr [7] score of 0.7061 on the \\nYouTube2Text dataset, putting our system in line with or exceeding the performance of other \\nvideo captioners. ContextNet is used to successfully demonstrate LinkBioMans capability of \\nclassifying contexts, with a mean Average Precision (mAP) score of 0.325 on relevant topics from \\nthe TRECVid 2015 evaluation [8], in comparison to the maximum mAP score of 0.273 from the \\nTRECVid performer teams in the same categories. The contribution of LinkBioMan is as follows. A modular near-real-time (alert) decision framework that accommodates different \\nmodalities of sensory inputs and is trainable for various domain concepts Deriving semantic context and alert (actionable context) from the fusion of audio and \\nvideo from uncontrolled data in domains of interest Demonstrating the quantitative impact of each low-level sensory module to the \\nclassification of the actionable context Related Work2\\nContext can represent a variety of concepts as categorized in [9]. Studies [10][14] have shown \\nthat leveraging context can enhance the performance of their image analytics goals. [15][18] \\nexplore and apply semantic context",
  "updated_at": "2/6/1999 12:00:00 AM",
  "created_at": "3/27/2008 12:00:00 AM"
}