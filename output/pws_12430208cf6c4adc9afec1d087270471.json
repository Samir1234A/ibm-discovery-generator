{
  "sdl_source_type": "PWS",
  "source_library": "SRC-rider",
  "file_name": "tealight's.ext",
  "document_url": "http://menuHohenzollern.com",
  "uploaded_by": "Llacolen s Rieu",
  "last_modified": "9/14/2013",
  "sdl_date": "2020-03-04T00:00:00",
  "countryPublished": "Turkey",
  "conference": "repatriated nitpicker ph Bo",
  "originalAuthorName": "Bina b Leniz",
  "title": "ethnics shaves",
  "declaredTags": "CUI|mentoring|NSEL|spectral clustering",
  "releaseReason": "introduction's/sideswiping",
  "docName": "FU_57_9207",
  "fundingCenter": 29,
  "resourceURL": "https://securely.com",
  "fundingDepartment": "ho56",
  "caseNumber": "27-4602",
  "publicationDate": "3/7/2020 12:00:00 AM",
  "releaseYear": 2014,
  "releaseStatement": "Peer-reviewed Publication/Journal",
  "approver": "$Cesar $Geerz",
  "handCarry": 6,
  "authorDivision": "ie64",
  "copyrightOwner": "Yunlong Keay",
  "lastModifiedDate": "2/27/2009 12:00:00 AM",
  "releaseDate": "7/6/2016 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "7747FRXA72",
  "materialType": "Article",
  "publicationType": "Article",
  "authorCenter": 33,
  "originalAuthorID": "Najib",
  "mitrePublicServer": 0,
  "subjectTerminology": "Education and Training (General)",
  "dateEntered": "3/21/2005 12:00:00 AM",
  "documentInfoURL": "https://gorgeously Tomsk knockwurst Elliot's windowsill's.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 16-2039|CASE2: 16-1196",
  "organization": "dd66",
  "authorDepartment": "xm30",
  "publicationYear": 1992,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "Non-Sponsored",
  "level1": "Corporate Ops & Transformation",
  "fundingDivision": "overshoot krÃ³na's gibe Madagascar frailer",
  "publishedOutsideUSA": 0,
  "level3": "eh41",
  "level2": "jg30",
  "sdl_id": "12430208cf6c4adc9afec1d087270471",
  "text": "being examined by some heuristic or analytical method. Edges are constructed between a vertex and all vertices in its neighborhood. Vertices with edges between them are said to be adjacent. A multitude of methods exist to define this neighborhood and the size of the edge set, ||, can vary \\ngreatly between methods for the same set of vertices. Vertex adjacency may be represented by multiple means, e.g., matrices, linked lists, or arrays of arrays. Of particular interest in spectral imaging is the adjacency matrix, ! =\\n\\\", where \\\" = 1 if an edge exists between and , and zero \\notherwise. ! is therefore an n x n symmetric matrix for \\nundirected graphs, where n is the number of pixels in the image. An HSI image with 10% pixels has an adjacency matrix \\nof size 10&'. The need for efficient encoding and/or \\napproximation is evident. Edges may also be encoded with the strength of the relationship between vertices by replacing the unitary edge contributions in !, with the value of a distance (similarity) \\nmetric1. This weighted adjacency (affinity) matrix, ( =\\n), is composed of non-negative scalar values where \\n , , and zero otherwise. A graph may therefore be \\ndefined as = , , ), where w is a mapping associating \\neach edge of unitary contribution in ! with a positive number \\nrepresenting vertex distance (similarity), i.e., ): 0,, or \\nsimply = ,.. In the subsections to follow, we will first focus on basic graph construction techniques and then migrate to those influenced by data density because it leads to the concept of clusters, i.e., community structure. \\n1 Distance measures quantify the separation between two objects such that 0,, where = 0 indicates identical objects. Similarity measures \\nquantify the similarity, s, between two objects where larger values indicate objects that are more alike, where typically 0 1 0,1. Edge weight matrices for \\ndistance and similarity matrices are called adjacency and affinity matrices respectively. IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. xx, NO. yy, MONTH 201x. DOI:xxxx Approved for Public Release; Distribution Unlimited. Case Number 16-3868 \\n 3 A. -Threshold Graphs (aka -NN Graphs) The threshold graph is very simple to construct and the fastest of all described methods. Given all pairwise distances (or similarities), an undirected edge is placed between two nodes if the distance, , , (or similarity) between them is \\nless (greater) than, 1, where 1 > 0 is a user defined constant. \\nSpecifically, , is added to the edge set, , if lies \\nwithin the hypersphere of radius 1 constructed about in \\nspectral space. Mathematically, this may be stated , \\n 33 , 1 and . The similarity based equation \\nis obtained by replacing , with 0 , and switching \\nthe direction of the inequality. This technique is not adaptive to scale or density because 1 is a fixed global threshold [15], \\ngenerally producing unconnected graphs. Hyperspectral data typically displays a multitude of densities hence this technique is not widely used, but is instructional in understanding other graph construction techniques. B. k Nearest Neighbor (k-NN) Graphs Nearest neighbor graphs are very common graphical construction techniques. An edge is placed between and if \\n is among the k nearest neighbors of . The user defined \\nparameter, k, is a global parameter indicating the number of edges exiting , i.e., its out degree. Given each node has its \\nown set of k nearest neighbors, this relationship is not symmetric and therefore produces directed edges leading to an asymmetric adjacency matrix [16]. These graphs are also called directed k-NN graphs for this reason. Construction of the k-NN graph is conceptually very simple, but computationally expensive due to the evaluation of all pairwise distances and subsequent sorting. Many traditional indexing methods (e.g., R-tree, k-d tree) fail in high dimensional spaces such that exhaustive searching for nearest neighbors can outperform even the most complex indexing scheme [17]. As such, several fast nearest neighbor methods have been developed to generate approximate or exact k-NN lists [18]. k-NN graphs are locally adaptive to both density and scale which makes them particularly well suited to model clusters of varying density [19] or follow mixing trends between clusters. Unfortunately, a global k-NN construction tends to over connect vertices in low density regions since the nearest neighbor may span a significant distance (or similarity). Another difficulty is selection of the user defined parameter k; values from 5 to 60 are common [13][19]. Selecting k too high tends to over connect the graph, whereas selecting k too low leaves the graph disjoint. This sensitivity to k is true of many k- NN variants and has promoted the development of adaptive algorithms that provide node-specific values, 8. \\nWe are restricting analysis to simple graphs and therefore need to modify the asymmetric adjacency matrix resulting from the directed k-NN relationship. There are two means of creating simple graphs from directed k-NN graphs, producing symmetric adjacency matrices: symmetry and mutuality. An added benefit of symmetric adjacency matrices is that memory requirements are cut in half because only the upper or lower triangular portions of the adjacency matrix need be stored. This can be quite substantial for even average sized HSI cubes. Generation of the symmetric k-NN graph is a trivial extension of the directed k-NN graph, where all vertex pairs, , , are connected if 9 orororor \\n 9, where 9 represents the k nearest neighborhood \\nof vertex . As a result, each node will have at least k \\nneighbors with the node degree being proportional to the nodes local density. Forcing a symmetric adjacency in this manner (bidirectional extension) may connect clusters of varying density and edges can still span large regions of spectral space to overly connect outlier nodes. Stringers, long chains of single nodes, can also extend from virtually anywhere if the conditions are right. This can produce a larger subgraph diameter if they extend from cluster edges. Generation of the mutual k-NN graph is also a trivial extension of the directed k-NN graph, where all node pairs , , are connected if 9 and 9, i.e., \\nonly existing bidirectional edges are retained. The resultant adjacency matrix is symmetric and a subset (subgraph) of the directed k-NN adjacency matrix (graph). Forcing adjacency symmetry in this manner reduces the possibility of connecting clusters of varying density; hence edges typically do not span large regions of feature space, leaving outlier nodes unconnected from denser regions [15]. C. Density Weighted k-NN (DW k-NN) Graphs Dense groupings of points in feature (spectral) space share similar attributes of similar magnitude and are therefore related. It makes intuitive sense these similar intracluster nodes should be more heavily connected than extracluster nodes. HSI data clusters exist at varying scales and density, so adaptive algorithms are desired. The definition of density itself may also need to be change because traditional Euclidean density becomes meaningless in high dimensional spaces due to the exponential growth in d-D volume [20]. Kameshwaran and Malarvizhi [21] state density based measures are the key to finding nonlinear structure, and we will find they are used extensively in HSI graph generation. Mercovich [13] introduced the concept of density weighted k-nearest neighbors to encode stronger relationships between similar nodes and promote more effective clustering by minimizing the impact of extracluster pixels. All pixels are assigned a codensity (distance) score given by ,\\n1 1\\n)( max minminmax \\n= \\n +\\n= k kk iki w\\nkk v (1) where k represents the indices of node <0 nearest neighbors \\nprovided in non-decreasing order, and kmin and kmax define the range of k values to average. Mercovich set 8=> = 1 such \\nthat ? represents the average distance of the pixels \\n8=@A neighbors. A node-specific number of neighbors, 8, is \\nassigned to each node based on its position in the histogram of IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. xx, NO. yy, MONTH 201x. DOI:xxxx Approved for Public Release; Distribution Unlimited. Case Number 16-3868 \\n 4 codensity scores (indicated by the nodes z-score). Mercovich found assignments based on integral z-score values from the range [-3,3] worked well for HSI clustering. Fig. 1a shows a representative Gaussian codensity distribution and the node specific connectivity count, 8 assigned by this method. \\nAssigning 8 in this manner results in a large number of nodes with ~8=@A/2 edges and much fewer nodes with 1 or \\n8=@A edges as shown in Fig. 1(b). The mapping is a quantized \\ninverse function of codensity where pixels residing in lower density regions (higher codensity) receive few edges (towards 1) and pixels in high density regions (low codensity) receive more edges (towards kmax). Mercovich notes that codensity distributions can take on non-normal forms based on scene content, but the normal assumption (through use of z-score) worked well for clustering2. HSI codensity distributions can approach normal \\n2 While some codensity distributions may appear relatively normal, they are never rigorously normal. We verified this by testing 56 chips of varying scene content, from different sensors, and GSDs via the Komolgorov-Smirnov test with increasing number of bands as seen in Fig.",
  "updated_at": "4/4/2005 12:00:00 AM",
  "created_at": "8/28/1995 12:00:00 AM"
}