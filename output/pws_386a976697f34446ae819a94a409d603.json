{
  "sdl_source_type": "PWS",
  "source_library": "SRC-emasculate",
  "file_name": "bicycle's.ext",
  "document_url": "http://despoticvaulter.com",
  "uploaded_by": "Guoxing p Lazoizketa",
  "last_modified": "10/22/2000",
  "sdl_date": "2020-05-24T00:00:00",
  "countryPublished": "Somalia",
  "conference": "tress's conjunctive kn Tamatha",
  "originalAuthorName": "Karl e Abramtchikov",
  "title": "Tut's",
  "declaredTags": "spectral clustering|Information|Federal Communications Commission",
  "releaseReason": "sleepyheads/archduke",
  "docName": "YO_28_7193",
  "fundingCenter": 93,
  "resourceURL": "https://Disraeli's.com",
  "fundingDepartment": "rp94",
  "caseNumber": "96-7572",
  "publicationDate": "3/23/2018 12:00:00 AM",
  "releaseYear": 2001,
  "releaseStatement": "Advertising/Recruiting",
  "approver": "$Essadia $De Serna",
  "handCarry": 3,
  "authorDivision": "gt11",
  "copyrightOwner": "Shahab Sanchez Mora",
  "lastModifiedDate": "10/16/2018 12:00:00 AM",
  "releaseDate": "10/3/2016 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "3253COJE20",
  "materialType": "Paper",
  "publicationType": "Article",
  "authorCenter": 56,
  "originalAuthorID": "Katerin",
  "mitrePublicServer": 0,
  "subjectTerminology": "Mathematics (General)",
  "dateEntered": "7/17/2002 12:00:00 AM",
  "documentInfoURL": "https://lockup's supremacists arthritis guesser lightweight's.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE4  14-0961|CASE1: 17-4550|CASE1: 17-4547|CASE1: 17-4550",
  "organization": "dk92",
  "authorDepartment": "ri68",
  "publicationYear": 2006,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "CAMH FFRDC Contracts",
  "level1": "Programs & Technology",
  "fundingDivision": "institution's Ovid treetops educator's Pena",
  "publishedOutsideUSA": 0,
  "level3": "kq83",
  "level2": "xm61",
  "sdl_id": "386a976697f34446ae819a94a409d603",
  "text": "present in the 1-second audio window. The SVM was trained on a subset of the \\nAudioset database [52], [61]; classes span many types of audio events occurring in public spaces, \\nsuch as, transportation and vehicles (e.g., aircraft, police siren, car horn), explosions (e.g., \\nfireworks, gunshot), weather sounds (e.g., rain, thunder, wind) and human activities (e.g., speech, \\nclapping, walking, shouting, laughter). Audio entity probabilities and labels from the Audio Entity \\nClassification module, along with both VGGish and cortical features, become inputs to \\nContextNet and the LinkBioMan Captioner, which incorporate these features with those from \\nother modules to derive higher-level contextual information. ContextNet4.2\\nThe ContextNet module is a fully connected network that is used to perform feature fusion on the \\noutputs of many primary and secondary modules. The ContextNet produces probabilities for alert \\nand semantic contexts. Alert probabilities are generated for experiments performed on the LBM \\nDataset only because it is the only dataset with Alert annotations. Figure 5 shows the ContextNet \\ndesign. The network architecture includes a dropout layer with the drop probability of 0.2 directly \\nafter the feature input layer. Using the dropout layer encourages ContextNet to explore \\nredundancies across the input feature space provided by different modules and not to overly rely \\non a small subset of the inputs. Following the dropout layer is a fully connected hidden layer with \\n1024 units and a Rectified Linear Unit (ReLU) activation function. The fully connected hidden \\nlayer is followed by another dropout layer with a 0.1 drop probability. This second dropout layer is \\nlastly followed by a fully connected layer for the output logits and a sigmoid activation function \\nproducing the final probabilities. We performed tests with additional hidden layers but found that \\nthey did not provide better performance on any of the datasets tested. L2 regularization was \\napplied to all weights. In both the LBM Dataset and the TRECVid Evaluation Dataset, most labels \\nhad a severe imbalance of positive and negative examples. To address this and create an unbiased \\nestimator, inverse-frequency weighting was used for each labels loss function. In all experiments, \\nContextNet was trained using TensorFlows Adam Optimizer for 2000 training steps, with a batch \\nsize of 100 random samples per step. xi Figure 5: ContextNet Design The ContextNet design shows that input modules can be easily swapped out for experimentation. Dropout \\nlayers are included only during training. The logits layer is sent through a sigmoid activation function to \\ngenerate the output probabilities. LinkBioMan Captioner4.3\\nThe LinkBioMan Captioner model modifies the Show and Tell Captioners [5] CNN-Long Short \\nTerm Memory (LSTM) layers to incorporate LinkBioMans multimodal features. The motivation \\nfor the modification is based on results from [3], [20], which demonstrated that incorporating \\nadditional information from multiple modalities (image, motion, audio) as well as semantic \\ninferences from other classifiers improved the overall captioner accuracy. The Show and Tell \\nCaptioner uses an encoder-decoder framework, where an (Inception v3) CNN is used to extract \\nsalient features from an image to encode, followed by a LSTM decoding the extracted features \\ninto a natural language representation. In comparison, the LinkBioMan Captioner (first version) \\nuses the extracted features from LinkBioMans primary and secondary modules plus their \\ninference scores, concatenates them with the image features from the pre-trained CNN of the \\nShow and Tell Captioner and run them through the LSTM decoder. Figure 6 shows the \\ncomparison between the Show and Tell Captioner and LinkBioMans Captioner (first version). \\nNote that the LSTM vector sizes and the combined CNN-LinkBioMan vector sizes are \\nproportional to the number of features that can be extracted from the LinkBioMan modules. As the \\ncombined CNN-LinkBioMan features are large and sparse, incorporating them directly into the \\nLSTM would drastically expand its cell size, risking overfitting. Therefore, only the LinkBioMan \\ninference scores that are more compact and summarize the detected entities extracted from each \\nLinkBioMan module are included in the combined CNN-LinkBioMan vector. The combined xii CNN-LinkBioMan vector is then fed into the LSTM of an expanded cell size. The LSTM decoder \\nis pre-trained on the original MSCOCO caption dataset [63] before being fine-tuned on our \\ncustomized LBM Dataset, using the following training parameters: sparse softmax cross entropy \\nloss, stochastic gradient descent with a learning rate of 2.0, decay rate of x0.5 per 8.0 epochs and \\nan LSTM dropout rate of 0.3. The result is a captioner that processes and translates the multimodal \\nfusion of video, audio, two streams (temporal, motion) and soft biometrics in less than 2 seconds. Inception v3\\nCNN Image Encoder LSTM Language \\nDecoder Embedding Layer Combined LinkBioMan \\nFeature Vector Audio \\nEncoder Attribute \\nEncoder Context \\nEncoder Combined CNN-\\nLinkBioMan Features Vector size:\\n2048 Vector size:\\n512 Vector size:\\n*1014 Vector size:\\n*1526 Cell size:\\n*1526 Vector size:\\n*Varies for each CNN Inception v3\\nCNN Image Encoder LSTM Language \\nDecoder Embedding Layer Vector size:\\n2048 Vector size:\\n512 Cell size:\\n512 Show and Tell Captioner LinkBioMan Captioner (First Version) LinkBioMan Multimodal Encoders Figure 6: Show and Tell Captioner vs. LinkBioMan Captioner (First Version) \\nThe diagram shows an overview of the original Show and Tell Captioner model compared with the \\nLinkBioMan Captioner (first version) model based off the Show and Tell Captioner [5]. The LinkBioMan \\nCaptioner (first version) uses LinkBioMans multimodal features and inference scores, that are concatenated \\nwith the pre-trained Show and Tell Captioners (Inception v3) CNN features before being fed into the LSTM. \\n*The LSTM cell size increases to support the combined CNN-LinkBioMan features (exact dimensions depend \\non LinkBioMan feature vector size: for an LinkBioMan vector size of 1014, the LSTM cell size expands from \\n512 to 1526). Preliminary experiments showed that the LinkBioMan Captioner (first version) model with \\nconcatenation did outperform the original Show and Tell Captioner in BLEU [64], ROUGE-L \\n[65] and METEOR [6] metrics (see Section 5.5 for details), but also suggested it may run the risk \\nof overfitting. This is partly due to our LBM Dataset being limited in size and the LSTM cell size \\nbeing expanded to incorporate the LinkBioMan features, resulting in a more complex LSTM \\ntrained with a more limited dataset than in the Show and Tell Captioners training scenario. To \\nresolve this issue, modifications to the LinkBioMan Captioner (first version) were made. First, the \\nfeatures extracted from each LinkBioMan module are concatenated and then fed through an \\nembedding layer. This embedding layer, consisting of a fully connected layer with a reduced cell \\nsize, is trained to compress and reduce the sparsity of the LinkBioMan features. Afterwards the \\nembedded features are then combined with the Show and Tell Captioners CNN features via \\nelement-wise summation before being fed into the LSTM, allowing the system to learn joint xiii relationships between the LinkBioMan features and the Show and Tell Captioners CNN features. \\nThis change allows us to preserve the original dimensions of the LSTM, further reducing the risk \\nof overfitting during the fine-tuning of the LSTM. Figure 7 shows the second version of \\nLinkBioMan Captioner. The LSTM cell size and the size of the input vector to the LSTM remain \\nthe same as those in the Show and Tell Captioner. Inception v3\\nCNN Image Encoder LSTM Language \\nDecoder Embedding Layer Inception v3\\nCNN Image Encoder LSTM Language \\nDecoder Embedding Layer Embedding Layer CNN \\nAudio Encoder CNN \\nAttribute \\nEncoder CNN \\nContext \\nEncoder LinkBioMan Multimodal Encoders Vector size:\\n2048 Vector size:\\n512 Cell size:\\n512 Vector size:\\n2048 Combined LinkBioMan Feature Vector Vector size:\\n512 Vector size:\\n1014 Vector size:\\n512 Cell size:\\n512 Vector size:\\n512 Element-wise Sum Show and Tell Captioner LinkBioMan Captioner (Second Version) Figure 7: Show and Tell Captioner vs. LinkBioMan Captioner (Second Version) \\nThe diagram shows an overview of the original Show and Tell Captioner model compared with the \\nLinkBioMan Captioner (second version) model based off the Show and Tell Captioner [5]. The LinkBioMan \\nCaptioner (second version) first compresses LinkBioMan features in an embedding layer before combining with \\nthe Show Tell Captioners CNN features via summation. The resulting feature vector size and LSTM cell size \\nremain the same as those in the Show and Tell Captioner [5]. Experiments and Analysis5\\nTo assess the efficacy of the LinkBioMan system, we conducted several experiments, which are \\ndetailed in the following subsections. Section 5.1 compares two approaches to classifying \\n(semantic) contexts using the LBM Dataset: ContextNet and Random Forests. Section 5.2 \\ndescribes the performance of LinkBioMans Alert with ContextNet. Section 5.3 compares the \\nperformance of ContextNet and Random Forests on the TRECVid Evaluation Dataset with the \\nreported results of the TRECVid performer teams. Section 5.4 conducts ablative tests with the \\nContextNet module to measure the contributions of each of its input modules. Sections 5.5 and 5.6 \\nmeasure the performance of the LinkBioMan Captioner using the LBM Dataset and the \\nYouTube2Text Dataset, respectively. Context Classifier: ContextNet Versus Random Forests5.1\\nUsing the LBM Dataset, we compared the performance of ContextNet against more traditional xiv random forests (ensemble of decision tree classifiers) for classifying contexts. For Random \\nForests, we trained separate forests for each context and alert using scikit-learns Random Forest \\nClassifier but found that its weight-based balancing system was unable to handle the high \\npositive/negative imbalance for some of our contexts. For this reason, it was necessary to train \\neach forest on a randomly sampled, balanced subset of the data. In this experiment, each",
  "updated_at": "6/28/2015 12:00:00 AM",
  "created_at": "2/19/1999 12:00:00 AM"
}