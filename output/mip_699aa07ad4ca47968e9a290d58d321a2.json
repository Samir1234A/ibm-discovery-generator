{
  "sdl_source_type": "MIP Projects",
  "chargeCode": "wtw",
  "longName": "lnsmxsqqvu-1076",
  "endDate": "2005-07-04T00:00:00",
  "phonebookDisplayName": "Remona Sanvicens",
  "startDate": "2008-05-06T00:00:00",
  "status": "Active",
  "portfolios": "insensate|chaired|syllabified|Galen|Shockley",
  "name": "jaggedest sharpest",
  "public_url": null,
  "project_url": "http://tubercular.com",
  "sdl_date": "2020-11-18T00:00:00",
  "countryPublished": "Dominica",
  "conference": "bike anyone's bj Olesya",
  "originalAuthorName": "Abdessamad l Winkelhage",
  "title": "grosbeak Kirchhoff's",
  "declaredTags": "collaborative and interactive brainstorming methods|RNA|ACE Direct",
  "releaseReason": "challenger/thunderously",
  "docName": "OY_42_1051",
  "fundingCenter": 61,
  "resourceURL": "https://eons.com",
  "fundingDepartment": "ja61",
  "caseNumber": "63-4244",
  "publicationDate": "3/17/2018 12:00:00 AM",
  "releaseYear": 2013,
  "releaseStatement": "Publicity/Promotion",
  "approver": "$Harlan $Grosshaus",
  "handCarry": 3,
  "authorDivision": "ml32",
  "copyrightOwner": "Porsha Mielchen",
  "lastModifiedDate": "6/24/2007 12:00:00 AM",
  "releaseDate": "11/11/2007 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "8802XBVX42",
  "materialType": "Book",
  "publicationType": "Book",
  "authorCenter": 77,
  "originalAuthorID": "San",
  "mitrePublicServer": 0,
  "subjectTerminology": "Management (General)",
  "dateEntered": "5/13/2002 12:00:00 AM",
  "documentInfoURL": "https://rearrangement motorman's baffles ventriloquism dissuaded.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE4  14-0961|CASE1: 16-0818|CASE1: 13-0202",
  "organization": "su80",
  "authorDepartment": "iw36",
  "publicationYear": 2012,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "Non-Sponsored",
  "level1": "Corporate Ops & Transformation",
  "fundingDivision": "relent termagant's patrols cr√®che polarization's",
  "publishedOutsideUSA": 0,
  "level3": "kf12",
  "level2": "vj95",
  "sdl_id": "699aa07ad4ca47968e9a290d58d321a2",
  "text": "recognition challenge, Int. J. \\nComput. Vis., vol. 115, no. 3, pp. 211252, 2015. [42] J. Huang et al., Speed/accuracy trade-offs for modern convolutional object detectors, \\nin IEEE CVPR, 2017, vol. 4. [43] S. Ren, K. He, R. Girshick, and J. Sun, Faster R-CNN: Towards real-time object \\ndetection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., \\nvol. 39, no. 6, pp. 11371149, Jun. 2017. [44] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V. Ferrari, We dont need no \\nbounding-boxes: Training object class detectors using only human verification, Feb. \\n2016. [45] C. Szegedy et al., Going deeper with convolutions, in Proceedings of the IEEE \\nconference on computer vision and pattern recognition, 2015, pp. 19. [46] B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva, Places: An image \\ndatabase for deep scene understanding, arXiv Prepr. arXiv1610.02055, 2016. [47] L. Fan, W. Huang, C. Gan, S. Ermon, B. Gong, and J. Huang, End-to-end learning of \\nmotion representation for video understanding, Apr. 2018. [48] K. Simonyan and A. Zisserman, Two-stream convolutional networks for action \\nrecognition in videos, in Advances in neural information processing systems, 2014, \\npp. 568576. [49] L. Wang et al., Temporal segment networks: Towards good practices for deep action \\nrecognition, in European Conference on Computer Vision, 2016, pp. 2036. [50] J. Huang, two-stream-action-recognition. [Online]. Available: \\nhttps://github.com/jeffreyhuang1/two-stream-action-recognition. [51] K. Soomro, A. R. Zamir, and M. Shah, UCF101: A Dataset of 101 Human Actions \\nClasses From Videos in The Wild, Dec. 2012. [52] S. Hershey et al., CNN architectures for large-scale audio classification, in \\nAcoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International \\nConference on, 2017, pp. 131135. [53] F. Pedregosa et al., Scikit-learn: Machine learning in Python, J. Mach. Learn. Res., \\nvol. 12, no. Oct, pp. 28252830, 2011. [54] N. Mesgarani, M. Slaney, and S. A. Shamma, Discrimination of speech from \\nnonspeech based on multiscale spectro-temporal modulations, IEEE Trans. Audio. \\nSpeech. Lang. Processing, vol. 14, no. 3, pp. 920930, 2006. [55] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, High-speed tracking with \\nkernelized correlation filters, IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 3, \\npp. 583596, 2015. xxxv [56] D. Li, X. Chen, and K. Huang, Multi-attribute learning for pedestrian attribute \\nrecognition in surveillance scenarios, in 2015 3rd IAPR Asian Conference on Pattern \\nRecognition (ACPR), 2015, pp. 111115. [57] Y. DENG, P. Luo, C. C. Loy, and X. Tang, Pedestrian attribute recognition at far \\ndistance, in Proceedings of the ACM International Conference on Multimedia MM \\n14, 2014, pp. 789792. [58] R. Bartyzal, Multi-label image classification with Inception net. [Online]. Available: \\nhttps://towardsdatascience.com/multi-label-image-classification-with-inception-net-\\ncbb2ee538e30. [59] C. Shallue, tensorflow/models/research/im2txt/, 2018. [Online]. Available: \\nhttps://github.com/tensorflow/models/tree/master/research/im2txt. [Accessed: 13-Mar-\\n2018]. [60] D. A. Depireux, J. Z. Simon, D. J. Klein, and S. A. Shamma, Spectro-temporal \\nresponse field characterization with dynamic ripples in ferret primary auditory cortex, \\nJ. Neurophysiol., vol. 85, no. 3, pp. 12201234, 2001. [61] J. F. Gemmeke et al., Audio Set: An ontology and human-labeled dataset for audio \\nevents. 2017. [62] C. Liu, C. Wang, F. Sun, and Y. Rui, Image2Text: a multimodal image captioner, in \\nProceedings of the 2016 ACM on Multimedia Conference, 2016, pp. 746748. [63] T.-Y. Lin et al., Microsoft COCO: Common objects in context, in European \\nconference on computer vision, 2014, pp. 740755. [64] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, BLEU: a method for automatic \\nevaluation of machine translation, in Proceedings of the 40th annual meeting on \\nassociation for computational linguistics, 2002, pp. 311318. [65] C.-Y. Lin, ROUGE: A Package for Automatic Evaluation of Summaries.\\n[66] Y. Bestgen, Exact expected average precision of the random baseline for system evaluation, Prague Bull. Math. Linguist., vol. 103, no. 1, 2015.\\n[67] E. Yilmaz and J. A. Aslam, Inferred AP: Estimating Average Precision with Incomplete Judgments.\\n[68] L. Zhu, Z. Xu, and Y. Yang, Bidirectional multirate reconstruction for temporal modeling in videos, Nov. 2016.\\n[69] L. Yao et al., Describing videos by exploiting temporal structure, pp. 45074515, 2015.\\n[70] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and K. Saenko, Sequence to sequence video to text. pp. 45344542, 2015. xxxvi Dataset DetailsAppendix B\\nDetails on TRECVid DatasetB.1 IACC 1A (TV2010): sin.30.qrels.tv10.gz\\nIACC 1B (TV2011): features.qrels.tv11.gz, featureL.qrels.tv11.gz\\nIACC 1C (TV2012): featuresF.qrels.tv12.gz\\nIACC 2C (TV2015): feature.qrels.7z All Data (Minus Unreadable Clips)B.2\\nActivity # of Positive Clips # of Negative Clips Boat_Ship 777 12875 Cheering 464 13283 Dancing 674 13791 Demonstration_Or_Protest 617 12484 Explosion Fire 873 15052 Running 597 13689 Throwing 249 15341 Clips with AudioB.3\\nActivity # of Positive Clips (with audio)\\n# of Negative Clips \\n(with audio) Boat_Ship 449 6023 Cheering 236 5244 Dancing 429 5404 Demonstration_Or_Protest 364 4966 Explosion Fire 402 5194 Running 347 5797 Throwing 164 6846 xxxvii Computing ResourcesAppendix C\\nLBM currently runs on an AWS EC2 P3.2xlarge instance, which is powered by 1 NVIDIA Tesla \\nV100 GPU with 16 GB memory, and 8 CPUs with 61 GB memory. xxxviii Acronyms and Abbreviations Appendix D\\nAP Average Precision\\nAPI Application Programming Interface\\nAWS Amazon Web Services\\nBLEU Bilingual Evaluation Understudy \\nCIDEr Consensus-based Image Description Evaluation\\nCN Context Net\\nCNN Convolutional Neural Network\\nCPU Central Processing Unit\\nEC2 Elastic Compute Cloud\\nFPR False Positive Rate\\nGB Gigabyte(s)\\nGPU Graphics Processing Unit\\ninfAP Inferred Average Precision\\nKCF Kernelized Correlation Filters\\nkHz Kilohertz\\nLSTM Long Short-Term Memory\\nmAP Mean Average Precision\\nMETEOR Metric for Evaluation of Translation with Explicit Ordering\\nMP4 MPEG-4\\nMPEG Moving Picture Experts Group\\nMSCOCO Microsoft Common Objects in Context\\nMSVD Microsoft Video Description Corpus\\nNIST National Institute of Standards and Technology\\nReLU Rectified Linear Unit\\nRF Random Forest\\nROUGE Recall-Oriented Understudy for Gisting Evaluation\\nSI Semantic Indexing\\nSVM Support Vector Machine\\nTPR True Positive Rate\\nTRECVid Text Retrieval Conference, Video Retrieval Evaluation\\nURL Uniform Resource Locator\\nVGG Visual Geometry Group xxxix _GoBack\\n _Toc54497488\\n _Toc54498619\\n _Toc63138695\\n _Toc63140075\\n _Toc63140209\\n _Toc64785106\\n _Toc82931581\\n _Toc64785108\\n _Toc64785109\\n _Toc527228703\\n _Ref524298790\\n _Toc527228728\\n _Hlk524087513\\n _Toc527228704\\n _Toc527228705\\n _Hlk525806175\\n _Ref524249803\\n _Ref526791386\\n _Ref526792895\\n _Toc527228706\\n _Ref524280580\\n _Toc527228729\\n _Ref524250115\\n _Toc527228707\\n _Ref524250173\\n _Toc527228708\\n _Ref524378213\\n _Toc527228739\\n _Toc527228709\\n _Ref524371084\\n _Toc527228730\\n _Ref524282220\\n _Ref526077575\\n _Toc527228740\\n _Toc527228710\\n _Ref524372509\\n _Toc527228731\\n _Toc527228711\\n _Ref524290922\\n _Toc527228732\\n _Ref526113393\\n _Toc527228712\\n _Ref524291160\\n _Toc527228733\\n _Ref524292365\\n _Toc527228734\\n _Toc527228713\\n _Ref524238192\\n _Toc527228714\\n _Ref524372658\\n _Toc527228735\\n _Ref524458157\\n _Toc527228715\\n _Ref524238214\\n _Ref524458265\\n _Toc527228716\\n _Hlk524648775\\n _Ref524354480\\n _Toc527228741\\n _Hlk524351818\\n _Ref524212648\\n _Toc527228736\\n _Ref524238624\\n _Toc527228717\\n _Ref527295480\\n _Ref524296009\\n _Toc527228737\\n _Ref524372777\\n _Toc527228738\\n _Ref524239212\\n _Toc527228718\\n _Ref524296947\\n _Toc527228742\\n _Ref524239223\\n _Ref526100130\\n _Toc527228719\\n _Hlk525934634\\n _Ref524378429\\n _Toc527228743\\n _Toc527228720\\n _Toc64785115\\n _Toc82932113\\n _Toc82932667\\n _Toc527228721\\n _Hlk524694777\\n _Ref525839500\\n _Toc527228722\\n _Toc527228723\\n _Ref524958100\\n _Toc527228724\\n _Toc527228725\\n _Toc527228726\\n _Toc527228727 \",",
  "updated_at": "8/2/2008 12:00:00 AM",
  "created_at": "6/11/2010 12:00:00 AM"
}