{
  "sdl_source_type": "MPL",
  "productName": "Kitchener Beatriz",
  "uploadedate": "2019-05-30T00:00:00",
  "productUrl": "http://Hialeah.com",
  "creatorNames": "Dragan j Villaplana;Amed v Campama;Meaghan g Delcea;Kenai e Merlini",
  "uploaded": "2019-05-10T00:00:00",
  "sdl_extracted_summary": "data field available to enter the actual data once the test is over. From a data tracking perspective, it is important to not only enter the planned value or objective, but the value that is actually achieved. Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 As discussed previously, phrasing strategic questions about the data helps to consider whether the data set is sufficient to answer those question, and therefore identify what additional data might be necessary that was not collected or identified previously. Examining the sample text and data fields, we can hypothesize the questions that might be important, and then check to see if the data supports them. For a single program, we might ask: a) How long are the missions for this system? \\nb) What is the mix of easily fixable problems vs Critical mission failures \\nc) What is the overall down-time for the mission from this MTBF and MTBCOMF? \\nd) What levels are being achieved now, compared to the value projected or planned? \\ne) Has there been a change in the projected or tested value since the last report? Overall the data is not fully supporting these questions, as the change since last report is not included, and the level being achieved is not tracked, only the planning value that is expected. Reviewing at the sample text, and imagining that the data for dozens or hundreds of programs is available, we can ask a different set of questions, such as: a) What are the common program data types or values that correlate to the programs that are \\nnot achieving their expected value? What makes some programs more likely to not meet their objectives at a significant level or manner? b) What is the overall trend over several years in achieving projected values? \\nc) How do programs or users compensate when projected values are not fully achieved? \\nd) Did programs that didnt meet their thresholds know that was coming? If so, what were they doing about it? Based on these questions, then we can add data fields, as long as the additions do not seem too onerous or expensive to collect. Fields that might be added here include: a) Change in value since the last reporting. \\nb) Actual Performance Achieved. \\nc) Standard deviation for the key parameter. \\nd) Actions taken to correct negative trends. This analysis is possible for every paragraph of these major procurement plans and documents, and could help move each text based response away from a set of long descriptive sentences and into a set of data fields that will provide the answers to key questions, often phrased as the W questions. These are why, what, who, when, which, where, and how. This could cause a significant increase in the number of data fields, but also allows comparative analysis of the project from a data and metric perspective. For the near term, the first step is to move away from piles of paper and documents files in a smooth and easily understood manner, and shift toward files of data. For this to happen in a smooth manner, and to allow project using data base methods to start using a new process soon, the methodology should be familiar and easy to understand and transition. This implies that blocks of text type responses are preferred at this time. In the long term, perhaps starting FY20, Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 other community stakeholders should be consulted in moving additional text fields into data type responses where appropriate, and identifying new report formats to capture a subset of text in a short and condensed manner that makes sense. Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 An Electronic Life Cycle Sustainment Plan (E-LCSP). \\nThe Life Cycle Sustainment Plan (LCSP) was also analyzed to consider the utility in shifting to an electronic data-based format. The LCSP is actually very amenable to this shift, as it primarily consists of tables of data in each section. This allowed a relatively rapid assembly of a prototype E-LCSP that can be packed into a data based form. The results of the conversion for the program-based are shown in Appendix E. Several data fields were also created from the LCSP instruction that were subsequently identified as either redundant or less valuable if a data based approach is being used, which are also shown in Appendix E. The LCSP also requires key data sets be provided for lower-level components and then again separately for each analysis method used. These are also shown in the Appendix. Like the TEMP, this leads to nested tables, which are accomplished by a unique code for each project. One possibility for this unique identifier is to use the budget Program Elements, then add an additional key code to that. Other examples might be a T Distribution Unlimited. Public Release Case Number 18-2212 Figure 6-1 Sample Life Cycle Sustainment Plan Data Input Form Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Cybersecurity Strategy & Program Summary \\nThe Cybersecurity Strategy and its attached Program Summary was also converted into a data base format to asses sits functionality in a data base schema. The summary report is document is a 3 page checklist of yes /no data, and relatively simple to convert to a data base format. The header for each data field, however, was very lengthy and had to be shortened to allow simple tracking in a Sharepoint list, as that is always shown in a horizontal manner, not a vertical manner. The resulting data ingest form however, was modified to include the full header text as an instruction, as shown in the Figure 7-1. For this data the 91x series of indices was chosen. Figure 7-1, Sample Cybersecurity Strategy Program Summary Data Input Format Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Other Acquisition Documents \\nThe AS, AP, TEMP, LCSP, and Cyber Security Strategy are the primary acquisition authorization documents to start a new program of record, after which an Acquisition Program Baseline and Acquisition Decision Memorandum are created. In order to minimize the amount of planning to a minimum level for projects that are supposed to be moving quickly, it is advisable to minimize the number of plans and documents needed for a new program using data- base methods. The Defense Acquisition University (DAU) provides an online tool that indicates which documents are needed for to authorize a program of record. This was converted to a static Excel File, and is shown below in Table 8-1. Some of these documents are not applicable to certain projects (e.g. the Spectrum Allocation document, which probably do not apply to a logistics software project). Those that are less common should be reserved to last place in efforts to create streamlined methods. The documents that apply to almost all of the projects, such as a Test Plan, were emphasized. A pilot program using data-driven methods could start as soon as the summer of 2018, in which case not all methods can be re-invented that fast, and even if they could the ongoing efforts to adjust, help users, solve problems, and test utility will all take time and imply a limited number of data driven conversions in the near term. Each of these documents was assessed for whether they should be converted to either a data-base method or a common template document method (e.g. RTI AP), and whether that should occur in the initial pilot program (phase 1 use) or later in a fully implemented system (aka phase 2 use). The results of this analysis for the STATUTORY requirements are shown in Table 8-2, and the results for the REGULATORY requirements are shown in Table 8-3. As seen, by leveraging the AS, AP, LCSP, TEMP, and Cyber plans in a data driven mode, all of the data needed to create a program of record are captured. The recommended approach includes phased implementation of program workflows for both data-based management methods and automated routing of legacy type documents, primarily those of external organizations. Table 8-1 below provides the phasing of the recommended implementation for creating automated data-based versions of the required statutory and regulatory program documents. Of the 9 phase-w workflows, 3 have already been prototyped, and three of the remaining items are quite small (the acquisition program baseline, the decision memorandum, and the exit criteria). This leaves the AS, Cybersecurity Strategy, and RFP appendix as still requiring data base list and workflow design efforts. The workflows created to route a legacy style document (uploaded as an attachment) is also quite minimal. Phase 1 SP/WF Phase 1 SPWF Routing Phase 2 SP/WF Phase 2 SPWF Routing Statutory Requirements 3 3 1 1 Regulatory Requirements 6 0 2 9 Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Figure 8-1 Summary Phasing Plan, Automated Procurement Documents and Workflows Table 8-1 STATUTORY Acquisition Requirements and Recommended Data-Based Streamlining Strategy STATUTORY Statutory requirements cannot be waived unless the statute",
  "sdl_date": "2020-01-16T00:00:00",
  "countryPublished": "Marshall Islands",
  "conference": "Floridan vassal's is Solmira",
  "originalAuthorName": "Edwar w Bahia",
  "title": "Boswell's viability straitjacketed waists",
  "declaredTags": "ARIA|multi-objective optimization|KPIs|Special Publication",
  "releaseReason": "cloudy/foray's",
  "docName": "KQ_98_1656",
  "fundingCenter": 54,
  "resourceURL": "https://Darin's.com",
  "fundingDepartment": "oy55",
  "caseNumber": "49-9374",
  "publicationDate": "6/28/2018 12:00:00 AM",
  "releaseYear": 2015,
  "releaseStatement": "Other",
  "approver": "$Zoraima $Yahmi",
  "handCarry": 6,
  "authorDivision": "pt86",
  "copyrightOwner": "Voica Toscano",
  "lastModifiedDate": "11/6/2002 12:00:00 AM",
  "releaseDate": "5/23/2001 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "4002RNEV27",
  "materialType": "Article",
  "publicationType": "Article",
  "authorCenter": 23,
  "originalAuthorID": "Leigha",
  "mitrePublicServer": 0,
  "subjectTerminology": "Health Services Administration",
  "dateEntered": "8/22/2007 12:00:00 AM",
  "documentInfoURL": "https://bummer's besotted airship's plectrum inequalities.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 17-3971",
  "organization": "uf76",
  "authorDepartment": "sy16",
  "publicationYear": 2016,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "FAA FFRDC Contracts",
  "level1": "MITRE Legacy",
  "fundingDivision": "Winkle's bricklayer besting aware natural",
  "publishedOutsideUSA": 0,
  "level3": "wb79",
  "level2": "mj95",
  "sdl_id": "be3d362351064328a527f8151e8e2822",
  "text": "    \"text\": \" Approved for Public Release; Distribution Unlimited: 18-2491.\\n Building a New Kind of Math to Identify Critical Cyber Connections Teaser: One of the challenges of managing vulnerabilities of cyber networks is the number of \\ninterdependent elements. MITRE researchers have developed a way to analyze these \\nrelationships by identifying the \\\"known unknowns\\\" and quantifying their impact. Several years ago, the then-Secretary of Defense talked about the known knowns and known \\nunknowns, which echoes what Confucius once wrote: To know what you know and what you \\ndo not know. That is true knowledge. It sounds almost mystical. In fact, much scientific research is based on this concept of seeking information where you only \\nhave a piece of the puzzle. Scientists develop hypotheses and then test them. Developing ways \\nto capture the impact of what's known to be unknown is often a critical ingredient in this \\nprocess. Especially if you don't want any unwelcome surprises. This is just as true in the evolving science of cybersecurity as in more traditional disciplines. For \\nexample, many types of systems today rely on assets connected in cyberspace. They're \\nincreasingly engineered by joining separate systems to provide capabilities and services. At \\nMITRE, researchers are using techniques to identify critical components in a cyber network or \\nmission. They must identify critical nodes to defend or attack in a cyber network, as well as \\nquantify a systems resilience. Quantifying the Nature of Dependencies To contribute to this approach to cyber, Dr. Les Servi and his team developed within our \\nindependent research program a set of methods to quantify the impact of known unknowns. \\nThey call their newly developed tool the Robust Network Analysis (RNA). They built upon a body \\nof knowledge called Functional Dependency Network Analysis (FDNA), first developed by MITRE \\nchief engineer Paul Garvey many years ago to quantify the dependency between nodes. Servi, project lead for a larger cyber analytics research initiative, explains what the team's work \\ninvolves. Lets say you have one task and sub-tasks and sub-sub tasks, he says. To do that \\ntask, you have to do this one. To do this task, you have to do these four tasks. You must map \\nthis all out and quantify the nature of the dependencies between the tasks, as well as quantify \\nwhat is not known about these dependencies. Once the system is mapped, researchers can identify the critical nodes. Servi identifies some of \\nthe questions you must answer. What are the nodes that you should attack if you want to hurt \\nthis network? If you wanted to defend the network, which nodes should you fortify? Which \\nnodes should you learn more about to better understand your vulnerability?\\\" For some systems, you can rank order the criticality of the nodes. For other systems, such https://www.mitre.org/research/overview\\nhttps://www.mitre.org/research/overview\\nhttps://www.mitre.org/research/overview\\nhttps://www.mitre.org/research/overview Approved for Public Release; Distribution Unlimited: 18-2491.\\n ordering is not meaningful, as the nodes most critical when faced with a highly capable \\nadversary might be very different from the nodes when faced with a much less capable \\nadversary. Building New Mathematics as a Hedge Against the Unknown FDNA is one approach to characterizing the dependencies between a task and its sub tasks,\\\" \\nServi says. This is a time-consuming process requiring experts in the system of interest. \\nFurthermore, even experts may not agree on the dependencies. To remedy this, Servis team \\ndeveloped a way of using simulation results to complement the experts opinions. However, even with this approach, if you dont know dependencies precisely because it's too \\ntime-consuming to get that precision, you have to figure out how to hedge against what's \\nunknown,\\\" he explains. \\\"We designed an algorithm to identify the set of critical nodes in such \\nsystems that helps practitioners hedge against what they know they dont know. He likens it to \\nnavigating with a very bad map.\\\" This process, known as robust optimization, took existing methods in the academic literature \\nand extended it to problems of interest to potential MITRE sponsors for mission network \\nanalysis. Servi says robust optimization is a a cautious hedge. You know you don't know things, so you \\nshould make decisions taking that into account. We joke that 'hope is a bad strategy for dealing \\nwith the unknown.' In other words, you're moving a \\\"fragile decision\\\" to a \\\"robust decision.\\\" Optimization methods have been aiding our national security for decades. As early as World \\nWar II, optimization methods assisted in locating German U-boats that were attacking \\nmerchants ships. Servis teams goal is to use and build on modern optimization methods to \\nassist in our current major challenge related to cyber networks. Robust Optimization in the Wider World Servi notes that MITREs robust optimization research isn't limited to cybersecurity and could \\npotentially help many government programs. For example, it could benefit government \\nacquisition a famously complex endeavor involving many moving parts and many unknowns. To spread the word about this work, the team has published papers in academic journals and \\nspoken at universities, as well as the Boston chapter of INFORMS (the Institute for Operations \\nResearch and the Management Sciences), which Servi currently runs. We're pushing the state of the art,\\\" he says. That's our job. However, we build prototype \\nsoftware to quantify the impact of this work to illustrate how it can be used by government \\nagencies. We want to build upon the intellectual property so we can make it available to a \\nbroader audience. https://www.mitre.org/capabilities/acquisition-effectiveness/acquisition-initiatives/agile-acquisition\\nhttps://www.mitre.org/capabilities/acquisition-effectiveness/acquisition-initiatives/agile-acquisition\\nhttps://www.semanticscholar.org/paper/Deriving-Global-Criticality-Conditions-from-Local-Servi-Garvey/30a0506995fac1751e47f6c45ca53dc2cb3bd161\\nhttp://annanagurney.blogspot.com/2018/03/fabulous-talk-by-informs-fellow-dr-les.html\\nhttp://annanagurney.blogspot.com/2018/03/fabulous-talk-by-informs-fellow-dr-les.html\\nhttp://annanagurney.blogspot.com/2018/03/fabulous-talk-by-informs-fellow-dr-les.html\\nhttp://annanagurney.blogspot.com/2018/03/fabulous-talk-by-informs-fellow-dr-les.html\\nhttp://annanagurney.blogspot.com/2018/03/fabulous-talk-by-informs-fellow-dr-les.html Approved for Public Release; Distribution Unlimited: 18-2491.\\n The government may find this too risky to do alone, but that's okay. We're in the business of \\nbridging between agencies and industry and taking risks though our research program, Servi \\nsays. \\\"This is just one example of how MITRE invests in creating new capabilities to help make the \\nworld a safer place, whether in the cyber realm or elsewhere. by Blair Gately _top\\n _GoBack \",",
  "updated_at": "11/11/2019 12:00:00 AM",
  "created_at": "3/23/2001 12:00:00 AM"
}