{
  "sdl_source_type": "MPL",
  "productName": "tuxedo soldiered",
  "uploadedate": "2010-07-16T00:00:00",
  "productUrl": "http://vein's.com",
  "creatorNames": "Rosamaria n Biederbeck;Joar d Aviram;Liane l Itey;Saila c De Juan",
  "uploaded": "2012-12-05T00:00:00",
  "sdl_extracted_summary": "a guy out at a higher altitude. Prob not a concern in real life \\nbecause programs would be in place to standardize.\\n(NCT) (Aircraft in front of CSL) what if I needed to take aircraft off final to get them in the \\nzone, no tools to show me how much I need, if I needed a short vector to set aircraft in \\nzone.\\n(NCT) Not enough space for final controller to make speed adjustments if needed. Results Topic 3: Monitor Configurations6.4\\nThe analyses in this section examine the Combined and Separate Monitor configurations for the \\nDay 1-2 Nominal scenarios. No lateral deviations were deliberately introduced; further monitor \\nconfiguration results for these off-nominal conditions are described in Section 5.6. As described in Table 4-5, monitor configuration for the Day 1-2 Nominal scenarios was divided \\nby simulation day. That is, the same monitor configuration was used for the first day, then the \\nother configuration was used for the second day. As the IM PA Tool configurations and \\nscenarios were varied in the same way between the two days, monitor configuration was the \\nprimary difference. Therefore, differences between the days with respect to the various metrics \\nshould be primarily attributable to the difference in monitor configuration. These metrics \\ninclude: Workload, Clarity of Roles and Responsibilities, Spacing Issue Detection, and \\nEffectiveness and Acceptability. Workload Acceptability6.4.1\\nAt the end of Day 1 and Day 2 (QT2), controllers were asked if their overall workload today was \\nacceptable. Their responses were then examined with respect to monitor configuration. \\nResponse Means and Standard Deviations are summarized in Table 5-19. Scale responses are \\nshown in Figure 5-28. Only Week 2-3 TRACON controller responses were included in the \\nanalysis. Table 5-19. Controller Responses to Acceptability of Overall Workload (Nominal Scenarios) Monitor Configuration xxxvii\\n Separate Combined\\nSample Size (n) 7 7 Mean (M) 96.1 95.7 Standard Deviation (SD) 3.8 6.7 Figure 5-28. My overall workload today was acceptable. Note: o indicates Week 1 TRACON controller responses for reference. These are not included in the M and SD computations. All Week 2-3 TRACON controllers agreed their overall workload was acceptable for both the \\nSeparate Monitor configuration (M=96.1; SD=3.8) and Combined Monitor configuration \\n(M=95.7; SD=6.7). No practical difference was observed between the two monitor \\nconfigurations. With respect to workload by monitor configuration, open-ended comments included: (Non-NCT TRACON / Separate Monitor configuration) Busier than [Combined], due to \\nrequired speed control. The IM PA was a very minimal part of the workload.\\n(Non-NCT TRACON / Separate Monitor configuration) Workload this low could lull a \\ncontroller into complacency.\\n(Non-NCT TRACON / Separate Monitor configuration) Somewhat complex with spacing \\ncomplexity between different runways and types. Not overworked, but not in drone \\nzone.\\n(Non-NCT TRACON / Combined Monitor configuration) I felt better about working both \\ncombined. The extra traffic helped me to stay focused. Clarity of Roles and Responsibilities6.4.2\\nAt the end of Day 1 and Day 2 (QT2), controllers were asked if when monitoring today, my roles \\nand responsibilities with respect to the IM Lead Aircraft, IM Trail Aircraft, and Other Aircraft \\nwere clear. Their responses were examined with respect to monitor configuration. Response \\nMeans and Standard Deviations are summarized in Table 5-20. Scale responses are shown in \\nFigure 5-29. Only Week 2-3 TRACON controller responses were included in the analysis. Table 5-20. Controller Responses to Clarity of Roles and Responsibilities xxxviii\\n Monitor Configuration Separate Combined\\nIM Lead Aircraft Sample Size (n) 7 7 Mean (M) 96.7 93.9 Standard Deviation (SD) 3.6 8.3 IM Trail Aircraft\\nSample Size (n) 7 7 Mean (M) 96.9 93.4 Standard Deviation (SD) 3.6 8.0 Other Aircraft\\nSample Size (n) 7 7 Mean (M) 96.7 92.4 Standard Deviation (SD) 3.6 8.5 Figure 5-29. When monitoring today, my roles and responsibilities with respect to the IM Lead Aircraft, \\nIM Trail Aircraft, and Other Aircraft were clear. Note: o indicates Week 1 TRACON controller responses for reference. These are not included in the M and SD computations. All Week 2-3 TRACON controllers agreed their roles and responsibilities were clear with respect \\nto all aircraft for both monitor configurations. Though no practical differences were observed in \\nthe responses, the overall response variability was slightly greater for the Combined Monitor \\nconfiguration. Spacing Issue Detection6.4.3\\nAt the end of Day 1 and Day 2 (QT2), controllers were asked if they were able to detect in a \\nsufficient amount of time when spacing / separation issues were developing within an IM PA xxxix\\n Aircraft Pair and Between Other Combinations. Their responses were examined with respect to \\nmonitor configuration. Response Means and Standard Deviations are summarized in Table 5-21 \\n. Scale responses are shown in Figure 5-30. Only Week 2-3 TRACON controller responses were \\nincluded in the analysis. Table 5-21. Controller Responses to Detection of Developing Spacing / Separation Issues Monitor Configuration Separate Combined\\nWithin an IM PA Aircraft Pair Sample Size (n) 7 7 Mean (M) 94.7 90.7 Standard Deviation (SD) 7.5 9.2 Between Other Combinations\\nSample Size (n) 7 7 Mean (M) 87.6 79.4 Standard Deviation (SD) 14.7 13.1 Figure 5-30. I was able to detect in a sufficient amount of time when spacing / separation issues were \\ndeveloping within an IM PA Aircraft Pair and Between Other Combinations. Note: o indicates Week 1 TRACON controller responses for reference. These are not included in the M and SD computations. All Week 2-3 TRACON controllers agreed they were able to detect in a sufficient amount of time \\nwhen spacing / separation issues were developing within an IM PA pair and Between Other \\nCombinations for both monitor configurations. Though no practical differences were observed \\nin the responses between the two monitor configurations, the means were slightly higher and \\noverall response variability was slightly lower for IM PA pairs versus other aircraft pair \\ncombinations. With respect to spacing issue detection, open-ended comments included: (Non-NCT TRACON / Separate Monitor configuration) The pairs were easier to determine xl\\n and required less focus.\\n(Non-NCT TRACON / Combined Monitor configuration) The IM PA pair did not have to be \\nwatched nearly as closely which allowed focus on the other a/c spacing.\\n(Non-NCT TRACON / Combined Monitor configuration) It is more difficult to determine \\ndistance between pairs than it is within pairs. Effectiveness and Comfort6.4.4\\nAt the end of Day 1 and Day 2 (QT2), controllers were asked if given the appropriate training \\nand IM PA-related tools, IM PA operations can be effectively monitored by the number of \\npositions I experienced today. Their responses were then examined with respect to monitor \\nconfiguration. Response Means and Standard Deviations are summarized in Table 5-22. Scale \\nresponses are shown in Figure 5-31. Only Week 2-3 TRACON controller responses were included \\nin the analysis. Table 5-22. Controller Responses to Effective Monitoring Monitor Configuration Separate Combined\\nSample Size (n) 7 7 Mean (M) 96.6 90.9 Standard Deviation (SD) 3.9 9.3 Figure 5-31. Given the appropriate training and IM PA-related tools, IM PA operations can be \\neffectively monitored by the number of positions I experienced today. Note: o indicates Week 1 TRACON controller responses for reference. These are not included in the M and SD computations. All Week 2-3 TRACON controllers agreed IM PA operations can be effectively monitored by both \\nthe Separate Monitor configuration (M=96.6; SD=3.9) and Combined Monitor configuration \\n(M=90.9; SD=9.3). Though no practical differences were observed in the responses, the overall \\nresponse variability was slightly greater for the Combined Monitor configuration. With respect to monitoring effectiveness by configuration, open-ended comments included: (NCT / Separate Monitor configuration) Depends on traffic, but two monitors \\n[controllers] seem better than one. xli\\n (Non-NCT TRACON / Separate Monitor configuration) Maybe overly monitored, but \\ndefinitely effective.\\n(NCT / Separate Monitor configuration) I feel it could be done by final by itself.\\n(Non-NCT TRACON / Combined Monitor configuration) Better today with one monitor. --------------------------------- At the end of Day 2 (QT3), after experiencing the nominal scenarios across all IM PA Tool and \\nmonitor configurations, controllers were asked if given the appropriate training and IM PA-\\nrelated tools, a single (Combined) Monitor controller can effectively ensure separation across \\nboth approaches during IM PA operations. Responses are shown for all TRACON (NCT and non-\\nNCT combined) and Tower Only. Response Means and Standard Deviations are summarized in \\nTable 5-23. Scale responses are shown in Figure 5-32. Table 5-23. Controller Responses to whether a Single (Combined) Monitor Controller can Effectively \\nEnsure Separation Across Both Approaches Participant Experience TRACON Tower\\nSample Size (n) 10 2 Mean (M) 84.2 89.0 Standard Deviation (SD) 15.6  Figure 5-32. Given the appropriate training and IM PA-related tools, a single (Combined) Monitor \\ncontroller can effectively ensure separation across both approaches during IM PA operations. Note: o indicates NCT controller responses. The majority (90%) of TRACON controllers agreed (M=84.2; SD=15.6) that a single Monitor \\ncontroller can effectively ensure separation across both approaches during IM PA operations. \\nOne NCT controller neither agreed nor disagreed and noted that more runs as a single monitor \\nwould be helpful to evaluate this better. With respect to the effectiveness of a Combined Monitor position, open-ended comments \\nincluded: (Non-NCT TRACON) I think it would be reasonable, especially when the final controller \\nsets up a well-spaced final. xlii\\n (Non-NCT TRACON) Depends on airport, but KSFO",
  "sdl_date": "2020-09-20T00:00:00",
  "countryPublished": "Kiribati",
  "conference": "mechanic fleece vv Milissa",
  "originalAuthorName": "Anje m Reles",
  "title": "paternal lithograph's",
  "declaredTags": "CyCAS|Obstacle Limitation Surfaces|data analytics|Virginia Tech interns|cyber/influence issues",
  "releaseReason": "bankbook/Nisei",
  "docName": "IK_97_8967",
  "fundingCenter": 91,
  "resourceURL": "https://mites.com",
  "fundingDepartment": "er36",
  "caseNumber": "82-5289",
  "publicationDate": "5/3/2019 12:00:00 AM",
  "releaseYear": 2019,
  "releaseStatement": "Other",
  "approver": "$Corono $Cerqueda",
  "handCarry": 0,
  "authorDivision": "yo18",
  "copyrightOwner": "Tammara Scetintsev",
  "lastModifiedDate": "12/24/2015 12:00:00 AM",
  "releaseDate": "2/18/2009 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "2243VBQV87",
  "materialType": "Paper",
  "publicationType": "Book",
  "authorCenter": 88,
  "originalAuthorID": "Lionel",
  "mitrePublicServer": 0,
  "subjectTerminology": "Airspace",
  "dateEntered": "9/14/2011 12:00:00 AM",
  "documentInfoURL": "https://quietly homer's canoeist's garrison Donetsk.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 18-1446|CASE2: 12-1089",
  "organization": "ke11",
  "authorDepartment": "hj39",
  "publicationYear": 2008,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "NSEC MOIE",
  "level1": "MITRE Public Sector",
  "fundingDivision": "standby's whiteness browbeaten diacritics breviary",
  "publishedOutsideUSA": 0,
  "level3": "mm82",
  "level2": "se69",
  "sdl_id": "3461700fe2c14157b5ca2ead17523f96",
  "text": "type are applicable to systems under development. Substitution: A complete replacement of a module or component (hardware, software, \\nfirmware) to be integrated into the system with one that has already been tampered with \\nin order to maliciously change its intended function or operation. Modification: Any change of existing design or other information that defines the system \\nunder development. In most cases, the change will be to cause a degradation or weakness \\nin later development or production. The intent of this analysis is to understand and characterize the adversarys goals within the \\nacquisition life cycle. This analysis does not represent the frequency of actual attack steps, but \\ninstead groups the 41 attack steps in relation to the acquisition lifecycle to observe trends in the \\noccurrence of the attack steps and relate them to the adversarys goals. \\nThe breakdown of substitution attack steps across the acquisition life cycle (Figure 6, below), \\nshows that they occur in both the Engineering & Manufacturing Development (15 attack steps) \\nand Production & Deployment (14 attack steps) phases nearly equally. Eleven of those attack \\nsteps occur in both the Engineering and Manufacturing Development and Production and \\nDeployment phases. Access to a component supplier is usually a prerequisite for performing a \\nsubstitution attack step. Understanding the adversarys goal of substituting hardware in the \\nEngineering & Manufacturing Development, Production & Deployment and Operations and \\nSupport phases of the acquisition life cycle, means that resiliency mitigations should be focused \\non hardware integrity and tracking. For an operational network, it can be assumed that some \\nhardware has been compromised. Software as a substitution attack frequently occurs during the \\nOperations and Support phase of the acquisition life cycle as software products are maintained.\\nInsertion can be directed at software, hardware, firmware or information. The number of attack \\nstepss characterized as insertion (9 attack steps) is smaller than substitution attack steps (24 \\nattack steps). \\nIt is important to note that many of the 41 attacks cited in the sources require some knowledge of \\nthe system under development, the suppliers, the development and production environments, etc. \\nIf access to information is properly controlled, adversarial reconnaissance can be curtailed. iii Operations and Support Production and Deployment Engineering and Manufacturing Development Technology Maturation and Risk Reduction Material Solution Analysis 0 2 4 6 8 10 12 14 16 Substitution Insertion Modification Figure 6. Supply Chain attack steps within the Acquisiton Lifecycle iv i Analysis of Applicable Existing Supply Chain Risk Appendix C\\nManagement Guidance \\nThe SCRM controls described in existing guidance and the resiliency mitigations proposed in \\nthis report complement each other as discussed below. Both should be used by SSEs. Below are \\nthe National Institute for Standards and Technology (NIST) documents reviewed for this task: NIST Special Publication (SP) 800-161 Supply Chain Risk Management Practices for \\nFederal Information Systems and Organizations [5]. This document is a supply chain \\nrisk management overlay for NIST SP 800-53 R4 [20].\\nNIST SP 800-30, Revision 1, Guide for Conducting Risk Assessments [12]\\nNIST SP 800-37, Revision 1, Guide for Applying the Risk Management Framework \\n(RMF) to Federal Information Systems [23] \\nNIST Federal Information Processing Standard (FIPS)199, Standards for Security \\nCategorization of Federal Information and Information Systems, [24] When conducting a risk assessment in accordance with the DoD Program Managers Guidebook \\nfor Integrating the Cybersecurity Risk Management Framework (RMF) into the System \\nAcquisition Lifecycle [24], using NIST SP 800-161 and NIST SP 800-30R10R1, an SSE works \\nwith their customer to assess risk at the mission and information system level. The \\nrecommendations of resiliency mitigations in Section 5 of this paper will guide the SSE to \\ninclude cyber resiliency as one method of risk mitigation. Table 15 below summarizes the \\nactivities an SSE performs when conducting a risk assessment and the complementary activities \\nperformed as part of a cyber resiliency analysis. For a discussion of how cyber resiliency relates \\nto the RMF, see the MITRE white paper The Risk Management Framework and Cyber \\nResiliency [17]. Table 15. Cyber Resiliency Activities Compared to Risk Management Framework Activities Risk Management Framework Activities \\nfor Assessing Risk Cyber Resiliency Analysis Activities Criticality Analysis Determine Mission essential cyber resources and cyber \\nresiliency objectives Analyze Threats and Known Vulnerabilities Analyze adversary capabilities, intent and targeting\\nDetermine likelihood of a threat exploiting \\na vulnerability Address inherent weaknesses in mission/business \\nprocesses, weaknesses in information security, \\narchitecture and cyber defense processes. Determine impact to system/mission Determine impact with a focus on Consequence to \\nMission Accept, Mitigate, Share, Transfer or Avoid \\nRisk Mitigate Adversary TTPs via cyber resiliency techniques Ensure mission and system can anticipate, withstand, \\nrecover from and adapt to adverse conditions, stresses, \\nattacks, or compromises on cyber resources The document Key Practices and Implementation Guide for the DoD Comprehensive National \\nCybersecurity Initiative 11 Supply Chain Risk Management Pilot Program provides 32 key ii practices for managing supply chain risks throughout a system design lifecycle [26]. It focuses \\non practices that enable the development and operation of systems to meet their cost, schedule \\nand performance requirements within a globalized market and with active adversaries. The \\naudience is system engineers, program managers, government prime contractors and \\nsubcontractors and those responsible for delivery and supporting systems with supply chain \\nassurance. \\nWe analyzed the key practices against cyber resiliency techniques and discovered where the key \\npractice supports cyber resiliency, is a part of cyber resiliency, has no overlap or is a complete \\noverlap with cyber resiliency. There are four key practices that overlap with cyber resiliency \\ntechniques: Table 16. Key Practices Guidance and Its Relation to Cyber Resiliency Key Practice Cyber Resiliency Technique\\nKP8 Protect Critical Elements and \\nProcesses Cyber resiliency technique selection driven by mission/business \\nobjectives, environment architecture and threat environment KP 9 Use defensive Design Cyber resiliency technique selection driven by mission/business \\nobjectives, environment architecture and threat environment KP 10 Use/Create standard \\ninterfaces to increase supplier \\ndiversity Part of the Diversity technique KP19 Perform Penetration \\nTesting Activities in this practice are part of Coordinated Defense \\nTechnique The key practices guide is a good source document for SSEs but focuses on recommendations for \\nprogram managers and acquisition specialists. iii Appendix D i Summary of Cyber Resiliency Techniques and Appendix E\\nApproaches Table 17 summarizes cyber resiliency techniques and the rationale for applying them (i.e., the \\nobjective an organization using it expects to achieve). Table 17. Cyber Resiliency Techniques Cyber Resiliency Technique Rationale\\nAdaptive Response: \\nImplement nimble cyber \\ncourses of action to manage \\nrisks Optimize the organizations ability to respond in a timely and \\nappropriate manner to adverse conditions, stresses, or attacks, thus \\nmaximizing the ability to maintain mission operations, limit \\nconsequences, and avoid destabilization. Analytic Monitoring: Gather, \\nfuse, and analyze data on an \\nongoing basis and in a \\ncoordinated way to identify \\npotential vulnerabilities, \\nadverse conditions, stresses, \\nor attacks, and damage Maximize the organizations ability to detect potential adverse \\nconditions, reveal the extent of adverse conditions, stresses, or \\nattacks, and identify potential or actual damage. Provide data \\nneeded for cyber situational awareness. Coordinated Defense: \\nManage multiple, distinct \\nmechanisms in a non-\\ndisruptive or complementary \\nway Ensure that failure of a single defensive barrier does not expose \\ncritical assets to threat exposure. Require threat events to overcome \\nmultiple safeguards; in the case of adversarial events, this makes it \\nmore difficult for the adversary to successfully attack critical \\nresources, increasing the cost to the adversary, and raising the \\nlikelihood of adversary detection. Ensure that uses of any given \\ndefensive mechanism do not create adverse unintended \\nconsequences by interfering with other defensive mechanisms. Deception: Mislead, confuse, \\nor hide critical assets from \\nthe adversary Mislead or confuse the adversary, or hide critical assets from the \\nadversary, making them uncertain how to proceed, delaying the \\neffect of their attack, increasing the risk to them of being discovered, \\ncausing them to misdirect or waste their attack and expose their \\ntradecraft prematurely. Diversity: Use heterogeneity \\nto minimize common mode \\nfailures, particularly attacks \\nexploiting common \\nvulnerabilities Limit the possibility of a collapse of critical functions due to failure of \\nreplicated common components. In the case of adversarial threats, \\ncause the adversary to work harder by developing malware or other \\nTactics, Techniques, and Procedures (TTPs) appropriate for multiple \\ntargets, increase the chance that the adversary will waste or expose \\nTTPs by applying them to targets for which they are inappropriate, \\nand maximize the chance that some of the defending organizations \\nsystems will survive the adversarys attack. Dynamic Positioning: \\nDistribute and dynamically \\nrelocate functionality or \\nassets Increase the ability of an organization to rapidly recover from non-\\nadversarial events (e.g., fires). Impede an adversarys ability to \\nlocate, eliminate or corrupt mission/business assets, and cause the \\nadversary to spend more time and effort to find the organizations \\ncritical assets, thereby increasing the chance of the adversary \\nrevealing their actions and tradecraft prematurely. ii Dynamic Representation: \\nConstruct and maintain \\ncurrent representations of \\nmission posture in light of \\ncyber events and cyber \\ncourses of action Support situational awareness, enhance understanding \\ndependencies among cyber and non-cyber resources, reveal \\npatterns/trends in adversary behavior; and validate the realism of \\ncourses of action. Non-Persistence: Generate \\nand retain resources as \\nneeded or for a limited time Reduce exposure to corruption,",
  "updated_at": "10/23/2013 12:00:00 AM",
  "created_at": "10/13/2014 12:00:00 AM"
}