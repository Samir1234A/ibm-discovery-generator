{
  "sdl_source_type": "MVC",
  "project_name": "gpq",
  "project_sponsor": "hwelebkgem-9348",
  "project_end": "2019-04-16T00:00:00",
  "portfolio": "owlish extraterrestrial",
  "super_portfolio": "JWF",
  "sub_portfolio": "qhi - Brahmanism's Chongqing",
  "clarify": "Cancelled",
  "project_url": "http://discreditable.com",
  "project_page_charge_code": "680QJRO",
  "project_leader": "Aurel X Klaynert",
  "sdl_date": "2020-10-29T00:00:00",
  "countryPublished": "Djibouti",
  "conference": "labs omnibus's qv Carmel",
  "originalAuthorName": "Kerstin l Bacelo",
  "title": "volley's",
  "declaredTags": "system entity structure|Medicare Advantage|Russian templates for influence|distributed systems",
  "releaseReason": "moveable/tackle",
  "docName": "HT_51_9744",
  "fundingCenter": 81,
  "resourceURL": "https://political.com",
  "fundingDepartment": "ej11",
  "caseNumber": "43-8348",
  "publicationDate": "4/9/2018 12:00:00 AM",
  "releaseYear": 2015,
  "releaseStatement": "Public Collaboration/Benchmarking/Standards Body",
  "approver": "$Haitz $Gallego Sacristana",
  "handCarry": 0,
  "authorDivision": "ej56",
  "copyrightOwner": "Merita Escayola",
  "lastModifiedDate": "6/29/2013 12:00:00 AM",
  "releaseDate": "4/16/2002 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "6081FBUL23",
  "materialType": "Paper",
  "publicationType": "Book",
  "authorCenter": 92,
  "originalAuthorID": "Genevie",
  "mitrePublicServer": 0,
  "subjectTerminology": "Management (General)",
  "dateEntered": "1/16/2008 12:00:00 AM",
  "documentInfoURL": "https://Angelina swimsuits recycled accretion's ultras.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 13-0202",
  "organization": "ta86",
  "authorDepartment": "vs55",
  "publicationYear": 2009,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "FAA MOIE",
  "level1": "MITRE Legacy",
  "fundingDivision": "solving Ada's relies statically Islam",
  "publishedOutsideUSA": 0,
  "level3": "xf85",
  "level2": "gh94",
  "sdl_id": "a1f3c9907c8a4410b989c821f2bfa09b",
  "text": "    \"text\": \" UP JORS software Latex paper template version 0.1 Software paper for submission to the Journal of Open Research Software To complete this template, please replace the blue text with your own. The paper has three main sections: (1) Overview; (2) Availability; (3) Reuse potential. Please submit the completed paper to: editor.jors@ubiquitypress.com (1) Overview Title Transplant2Mongo: A Python module to manage and store Organ Procurement and Transplantation Network (OPTN) data in MongoDB 1 Paper Authors 1. Harvey, Christine 2. Weigel, Robert PhD Paper Author Roles and Affiliations 1. Lead High Performance and Analytical Computing Engineer, The MITRE Cor- poration and PhD Student, George Mason University 2. Professor, George Mason University Abstract The transplant2mongo Python module allows users to transform Standard Trans- plant Analysis and Research (STAR) data files from the Organ Procurement and Transplantation Network (OPTN) into a MongoDB schema [1,2] . The STAR data are a collection of tab-separated files with inter-related records that is not designed for complex query. Any researcher planning to use data from an OPTN STAR files can use transplant2mongo to convert the information into a MongoDB docu- ment for analysis using open-source tools. The source code for transplant2mongo is available on GitHub at https://github.com/ceharvs/transplant2mongo and includes sample data files for initial testing and data query. Keywords Python; OPTN; UNOS; organ transplant; health care; analysis; MongoDB Introduction The Organ Procurement and Transplantation Network (OPTN) keeps a record of all organ donations, transplants, and waiting list registrations since 1987 in the United States [1]. This complex data set can be used to review, query, and ana- lyze the US organ donation system over time and across various factors. The total 1Approved for Public Release; Distribution Unlimited. Case Number 18-0298. c MITRE Corporation is provided for identification purposes only and is not intended to convey or imply MITREs concurrence with, or support for, the positions, opinions, or viewpoints expressed by the author. https://github.com/ceharvs/transplant2mongo UP JORS software Latex paper template version 0.1 database is around 9GB and contains records on hundreds of thousands of donors and over a million patients [2]. The transplant2mongo software translates data from OPTN Standard Transplant Analysis and Research (STAR) files into a Mon- goDB database. OTPN STAR files are designed to be analyzed and processed in either SPSS or SAS, according to the OPTN [2], which are both proprietary tools. This software was developed to allow researchers to use open-source software to manage the dataset and perform an analysis. transplant2mongo transforms the raw tab-separated value (TSV) STAR files into documents stored in a MongoDB database. Translating the data allows any user without specialized software to ex- plore and perform analysis on this complex and interconnected dataset. Due to the sensitive nature of STAR file data, the development of these tools was completed in a secure lab environment at The MITRE Corporation and the code repository does not include any STAR files. Using Python to complete the data transformation and MongoDB as a final repository for the data allows users to make use of only open-source tools. Python and the included modules read in the data from the STAR TSV files and convert it to MongoDB documents . Special consid- eration was made to process the data and remove erroneous symbols and characters that are not compatible with MongoDB. The resulting MongoDB has been used to perform analysis of the OPTN dataset and the results of the analysis will be considered in a future publication. Implementation and architecture This software was developed on a CentOS machine using Python 2.7 and MongoDB 2.6 in a secure lab environment, suitable for the sensitive data files. This tool was developed and tested using OPTN STAR file data from June 2014 and March 2015. The general file configuration of OPTN STAR files can be seen in Figure 1. Users of this tool may not have the complete UNOS STAR files, and may only have certain subsets of this information. In this case, the Makefile can be edited so that only a subset of the STAR files are processed. As shown in Figure 1, the TSV files are all stored in the highlighted, Delimited Text Files folder. All data processed with the tool should be contained in this folder. This folder contains a sub-folder for each of the data types: Deceased Donor, Intestine, Kidney Pancreas Kidney-Pancreas, Liver, Living Donor, and Thoracic. The Kidney Pancreas Kidney-Pancreas sub-folder contains in- formation on all patients and transplant recipients for kidney and pancreas, and combined kidney/pancreas transplants. The Thoracic folder includes data on both heart and lung transplant patients. The deceased donor and living donor fold- ers contain details on all organ donors, while the other folders provide data on all patients registered to the waiting list and those who received a transplant. Each of these major groups are represented as collections in the database with the patient or donor information as documents and the sub-folders as sub-documents. UP JORS software Latex paper template version 0.1 Figure 1: Structure of the OPTN STAR files. MongoDB was chosen as the database due to the NoSQL format, allowing multiple patient fields to be combined into a single document with multiple sub-documents. Many of the fields have changed over the years and many of the patient fields are missing information, making this data suitable for NoSQL storage. Alternatively, a SQL database system could be used. With a structured database implementation the number of columns would be very large and there would be many NULL fields in the database. A Makefile is used to build the database from the contents of the Delimited Text Files folder. The Makefile has targets for cleaning, processing, and importing the data into a specified MongoDB database. A linked directory using a symbolic link is used to avoid unnecessary parsing of spaces in the file names. Users need to specify their database location and name in the Makefile before execution. The default interface is localhost and database name is organ data. Users also need to define their components, which specifies the STAR files that the researcher has access to from UNOS. These values are selected from the following list: deceased, living, intestine, kidpan, liver, and thoracic. Data are copied from the original location and file structures are flattened to establish a simple structure for parsing. UP JORS software Latex paper template version 0.1 Once the data is in place, another Python script is run to generate JSON and add the documents into a MongoDB database. For the base files, the main donor or organ data files, the add patients.py script is run to generate major documents in the appropriate tables. For files that contain sub-document information such as follow-up visits or medications, the supplemental data.py script adds this infor- mation as a sub-document to the main patient or donor document. This script uses a unique identifier, such as DONOR ID, TRR ID CODE, or WL ID CODE, to match the supplemental data file entry with a unique record for the donor or organ type. Once a match is found, the supplemental data is added to the document in the database. The TSV files, having the extension .DAT, only contain data and do not include column names, therefore columns are determined by the .htm files corresponding to each .DAT files. Data are cleaned before insertion into the database. The cleaning process is han- dled in the Python scripts add patients.py and supplemental data.py, which use the clean string function. These scripts remove extraneous values such as extra commas, quotations, and new lines. The scripts also create Python datetime objects from potential date strings and convert strings to integers when possible. The scripts do not assume a particular data type for any column. The database contains many different columns and the typing is automatically determined by Python and MongoDB. All data is originally imported as a string. The Python scripts check if the data is a date or an integer, and if not, the data are kept as strings. Following execution of each of the import scripts, follow-up scripts are run to add age bins to all age fields of the data for fast and simple aggregation of the data. Quality control The scripts have been tested on two distinct UNOS STAR file data sets from 2014 and 2015. Each script prints out the number of lines in the file and then the number of patient documents successfully imported into the database. With the two testing sets, the line count results match the imported entry counts in all scenarios with the exception of a particular data file where a new line is hidden with quotation marks and the reported file length is one more than the imported record count. The data were manually confirmed in this situation and manual spot checks of all the data were performed. The Github repository includes sample data that can be used for testing the instal- lation. These data contain no real patient information and were generated by the developers of the software. The",
  "updated_at": "1/14/2011 12:00:00 AM",
  "created_at": "12/30/2007 12:00:00 AM"
}