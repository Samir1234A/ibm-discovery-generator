{
  "sdl_source_type": "platform",
  "field_launch_date": "2021-06-19T00:00:00",
  "changed": "2022-12-14T00:00:00",
  "field_platform_contacts": "Yixin I Vieito| Pelegrin R Stuparu| Ynes Q Royes| Yicheng Q Messing",
  "field_communities_of_practice": "jodhpurs turquoises| Xiaoping's skims| Gautama exploratory| adoringly carp's| ciabattas Tanzanian's| gypsies stargazer| programer Carnegie| Occidentals Jewell| enforcement overpowering| strictly Superior",
  "platform_leader_name": "Toi L Guillan",
  "field_banner_subhead": "Irkutsk's liverwurst's",
  "platform_url": "https://pinion.com",
  "sdl_date": "2020-10-23T00:00:00",
  "countryPublished": "Lithuania",
  "conference": "commercial demonic mt Huaying",
  "originalAuthorName": "Shawana i Froelic",
  "title": "cutlery's",
  "declaredTags": "cloud computing|Federal Aviation Administration|chemical database|RNAV arrivals|IoT",
  "releaseReason": "allude/ant",
  "docName": "WP_95_2414",
  "fundingCenter": 59,
  "resourceURL": "https://Fathers.com",
  "fundingDepartment": "yc68",
  "caseNumber": "78-3093",
  "publicationDate": "1/19/2019 12:00:00 AM",
  "releaseYear": 2001,
  "releaseStatement": "Conference/Workshop",
  "approver": "$Hristina $Judizmendi",
  "handCarry": 3,
  "authorDivision": "hc96",
  "copyrightOwner": "Liliane Sisternes",
  "lastModifiedDate": "6/29/2004 12:00:00 AM",
  "releaseDate": "1/21/2019 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "3999RYUE91",
  "materialType": "Article",
  "publicationType": "Paper",
  "authorCenter": 86,
  "originalAuthorID": "Hania",
  "mitrePublicServer": 0,
  "subjectTerminology": "Aviation and Aeronautics (General)",
  "dateEntered": "3/30/2011 12:00:00 AM",
  "documentInfoURL": "https://Jamel's Millicent Alphonse enthronement's landslide's.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 17-4622",
  "organization": "ju23",
  "authorDepartment": "pr81",
  "publicationYear": 2011,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "IRS and VA FFRDC Contracts",
  "level1": "MITRE Public Sector",
  "fundingDivision": "uric disinherited schoolmates Penderecki regicide",
  "publishedOutsideUSA": 0,
  "level3": "xc72",
  "level2": "af61",
  "sdl_id": "b5e31ce1f85a46f48d636487b8bff12f",
  "text": "a particular program resides in a data base file. Thus the large template AP contains a significant amount of descriptive text on the how part of the contracting and acquisition, which is not included in the data base. This separation works well for the AP, as that document contains not only project data but also process descriptions. The repetitive process text and project data was already separated into two sections when the RTI process was created. This pattern, or template, could also be applied to other procurement documents that contain a significant mix of data and process text. An example of a subsequent document that may follow this same pattern is the Systems Engineering Plan (SEP). In other documents that were subsequently analyzed, there was a mix of data and information that was retained and information that is recommended for discontinuation. 3.3 Additional Data Needed to Make Effective Decisions As stated, the data base record for the AP is taken from the RTI appendix. In the initial version of this data record an additional set of data has already been added, which is the actual dates for procurement events. This will allow backward-looking analysis to monitor and review approval times. The automated Sharepoint workflow records will also provide a useful record of approval dates, but the date data needs to be captured in the data base in order to do analysis. This will allow oversight executives to review data and answer questions like what is the average creation time for an AP appendix using this method, and what is the average approval time?. This raises the interesting possibility however, on what data is missing from an AP template? What questions would leadership like to ask about the new data-driven process as it starts to run, or on an annual basis, or after 5 years? Once those questions are identified, then the data base can be reviewed to see whether the proper data is included, and if not then it can be added (as long as the cost of gathering that data is considered less than the value in answering the question). A selection of possible strategic questions that might be useful are: a) How long does it take to create and approve a data-based AP? How does that compare to \\nlegacy documents? b) What is the average total contract value being created in the new process? \\nc) What is the average unit cost of the technology being procured? \\nd) How long is it typically taking to get from start of an AP to successful system test? \\ne) What is the percentage of achievement in primary engineering measurement parameters? \\nf) What is the percentage in achievement in secondary engineering measurement parameters? g) What is the average size of the basic RTI contract award for phase 1 studies? \\nh) What is the average over or under-run of the assessment/engineering phase of the RTI contracts when completed? [note additional cost at completion data needed] i) How many times are the systems integration collaborators in place on day-1 when the \\nbasic RTI contract starts? [additional data needed] Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 An executive oversight review team should be leveraged to develop the types of questions that they would like to ask. This might be done by handing out index cards to each of them during a steering team meeting and asking them to write down 3 questions that they would like to consider as the process continues. Then those questions can be similarly analyzed. This should be done with other relevant stakeholder groups on each of the documents being converted to a data base approach during the pilot program. The Electronic Acquisition Strategy \\nThe Acquisition Strategy was converted into a data based format in a similar manner. The AS contained a large amount of information that is specific to a program, and thus could not leverage the template approach like we did with the RTI AP. Sample screen captures of the E- AS data entry form is shown below in Figure 4-2. For the E-AS the textual data was converted into distinct data fields to capture useful data that could then be searched or tested. This helped reduce the amount of Free Text fields and should provide a more useful data base to DoD leadership. This data set should be reviewed at the end of a pilot program in order to test its utility and completeness. Additional data may be needed in some cases to provide a more rich and useful data base. The Yellow and Blue sections of the data set represent sub-tables of data that are repeat several times for a single program, for waivers to policy and RF signatures. These two would need to be created in a separate record with a unique key that associates the data to a program. A subset of the data, fields, the series numbered 7 to 11, is actually repeated with data in the Acquisition Plan, thus can be automatically filled in from that data. The 100 series of field numbers was chosen for the E-Acquisition Strategy data. The quantity of each data type used in the E-AS data base is shown in Figure 4-1. The actual data fields are provided in Appendix C of this report, as we will be covering this approach for several more documents and the detail fields will evolve as the pilot program tests them. Review of the E-AS data records indicate that essentially all of the Acquisition Strategy information has been captured, although some of the data fields represent an amount of data that was considered useful and required. Similar to the other data sets, this data should be used in a pilot program, tested for completeness by reviewing leadership questions, and tested for utility in case some of it is not being used and thus its collection could stop. Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 Figure 4-1 Occurrence Count of Data Types for a Draft Electronic Acquisition Strategy Row Labels Count of Data Type Currency 34 Currency/Sum 1 Date 29 Email 5 Link 5 Number 15 percent 6 Pulldown 11 Text 99 Y/N 8 Yes/No 27 (blank) Grand Total 240 Approved for Public Release; Distribution Unlimited. Public Release Case Number 18-2212 The Electronic-TEMP (E-TEMP) \\nThe Test and Evaluation Master Plan (TEMP) is another primary acquisition document, and is required by law (statute) for all sized ACAT programs prior to even a Milestone A decision to start a program. The TEMP also serves instead of the Acquisition Strategy (AS) document for ACAT-4 and below sized programs. Again, historically, and following the instructions in the DoD 5000.02 instructions for acquisition, a program office would develop their own TEMP for each program that describes their problem and planned testing, working with stakeholders from the PEO, COMOPTEVFOR OPNAV, and OSD/DOT&E (all of which need to approve the document). There is only a program-based model for the TEMP, so there is no need to try to flip the document model. However, we analyzed the TEMP to assess the amount of data type information that it contained, compared to the amount of process type text. At this point, as the data base already contains the project data of the E-AP, we also analyzed the data to see what was repetitive of data provided by other documents. 5.1 Adaptation to a Data Base Format Looking at what is required in a TEMP, we see a list of 7 general sections, some of which have extensive content in several subsections. Generally the content is lengthy, and oriented towards platforms, not systems. We also see a heavy emphasis on text based content in some sections, and not an approach that collects data. The content also lacks a requirement to show and discuss human subject testing plans, which is commonly required in university and NSF grant proposals. An abbreviated summary of the TEMP content required by ADDM 5000.02 is listed below. 1) Intro: Purpose, Mission description, System description 2) Test Mgt and Schedule: Roles, Database Info, Deficiency reporting, Updating, IT&E \\nSchedule 3) T&E Strategy: DT & OT Objectives, Evaluations, Framework, DT Eval approach \\nSummary, R&M, Performance, CTPs (Risks, Certs, issues, Objectives, M&S, limitations), Live Fire T&E (approach, objectives, M&S, limits), IOT&E Cert, OPEVAL Approach (schedule, COIs, Objectives, M&S, limits), Other Certs, Reliability, Future T&E. 4) Resource Summary: Intro, Test Articles, Sites, Equipment, Threat, Expendables, Op. \\nforces, M Distribution Unlimited. Public Release Case Number 18-2212 Test Plan Content Per DI-NDTI-80566A: 1) Title Page / Admin info \\n2) Introduction \\n3) Flow Diagrams Describe testing via block diagrams \\n4) Milestones (Schedule) Start and completion dates. \\n5) Participation Govt. and Contractor roles. \\n6) Location of each test event \\n7) Schedule When testing occurs. \\n8) Security measures \\n9) Master Test List Test, location, Spec used, Parameters, Special tests, Functional area, Objectives, Equipment, Support equipment, special test equipment, approach, Instrumentation, data to be recorded, Government facilities. In reviewing the outline of the TEMP and a contractors project test plan, we can make",
  "updated_at": "7/26/2007 12:00:00 AM",
  "created_at": "8/27/2019 12:00:00 AM"
}