{
  "sdl_source_type": "MPL",
  "productName": "rearward typifying",
  "uploadedate": "2013-04-25T00:00:00",
  "productUrl": "http://professionalism.com",
  "creatorNames": "Georgica d Nukala;Gulshan t Nabari;Agostina u Badostain;Siraj q Pfeiffer",
  "uploaded": "2012-12-09T00:00:00",
  "sdl_extracted_summary": "made available or \\nremoved from the display, depending on the scenario. This was intended to examine whether \\nthey were helpful or necessary for the controller IM PA monitoring task. The other IM PA tools \\nsuch as the IM Spacing List, IM Status, CSL/WSL Lines, Alerts, and Lateral Boundaries features \\nwere fixed. These were always available to participants, for every scenario. Day 3 for each session included a self-contained evaluation of the longitudinal alert timing \\nvalues. As described in Section 4.2.7, the default Predictive / Caution alert timings were the \\nsame as those implemented for ATPA: 45 seconds and 24 seconds, respectively. However, \\nreduced timings may ultimately be used to mitigate against nuisance alerts (which would occur \\nmore frequently with greater timings). Therefore, two additional alert timings were investigated \\nin the HITL to examine whether lower alert timings than those chosen for ATPA remain \\nacceptable to controllers for the IM PA monitoring task. These were: 35 seconds / 20 seconds \\nand 25 seconds / 15 seconds, for the Predictive and Caution Alerts, respectively. To create a \\nmore stressing condition, these scenarios only evaluated alert timing in the context of the \\nCombined Monitor configuration. However, all of the IM PA-related tools described previously \\nwere available to the Combined Monitor controller for these scenarios. Day 3 also took an initial look at lateral monitoring with deviations as a self-contained xxxv\\n evaluation in the afternoon. This included determining to what degree will controllers want to \\ndetect developing deviations before a Lateral Boundary is crossed, whether a 4:1 aspect ratio \\nFMA display is needed, especially with only an Exceedance Warning (no Lateral Predictive Alert) \\nwith a tighter Lateral Boundary size (ILS versus 500 ft), and how task acceptability was affected \\nby Combined versus Separate Monitor positions. Therefore, the Independent Variables for this \\nportion of the experiment included display type (STARS versus FMA) and the number of Monitor \\ncontrollers (Combined versus Separate). All of the IM PA-related tools described previously \\nwere available to the Monitor controllers for these scenarios. Traffic File Attributes5.5.5\\nThree general types of scenario traffic files were developed for the HITL, one for each type of \\nscenario. These included: 1) Nominal scenarios, 2) Alert Timing scenarios, and 3) Lateral \\nDeviation scenarios. Depending on what was being tested, these traffic files may have contained \\neither of two types of off-nominal events: Spacing Loss and Lateral Deviation. A Spacing Loss occurred when an IM Trail Aircraft began to encroach on one of the \\nsafety limits, triggering an alert. When this occurred, the controller procedures varied \\ndepending on if it was a Predictive Alert or a Caution Alert. \\nA Lateral Deviation occurred when during the IM PA operation, one aircraft sharply \\ndeviates from its approach path, either to the left or right, and keeps going; or, one \\naircraft starts a long, shallow deviation to the left or right. In this instance, the controller \\nfollowed the procedures for lateral deviations discussed. These only apply to aircraft in IM PA pairings, though loss-of-separation could occur between \\nnon-IM PA aircraft as well. None of these cases were intentionally introduced, however. The traffic file attributes for each type of scenario are summarized in the following sections. Nominal Scenarios5.5.5.1\\nFour nominal core traffic files approximately 20 minutes in length. Each had seven IM PA \\npairings, five with WSL and two without WSL. There were two non-paired SRS aircraft. There \\nwere two Spacing Loss events per scenario; one was an IM PA Predictive Alerts that was \\nintended to resolve without controller intervention being required. The second was an IM PA \\nPredictive Alert that led to a Caution Alert if the controller did not intervene. Given the scenario \\ntimeframe, these occurred at a higher frequency than would be experienced in actual \\noperations to provide controllers with concentrated experience in dealing with them and to get \\nfeedback on the alerting. These core traffic files were then cloned and varied in which aircraft experienced the Predictive \\nand Caution Alerts, the location of SRS aircraft within the flow, with respect to the IM PA \\npairings, the order of the IM PA pairings, and the aircraft call signs. Alert Timing Scenarios5.5.5.2\\nThe Alert Timing scenarios were based on the nominal scenario and was approximately 20 \\nminutes in duration, with six IM PA pairings. All pairs required a WSL and five of the six parings \\nwere manipulated to show an alert related to Spacing Loss. The same traffic file was used for \\neach run so participants could clearly isolate the effect of each alert timing set. xxxvi\\n A CSL alert can be triggered either earlier in the approach, or later. A WSL alert can only be \\ntriggered late, when the WSL is active. The distribution of the alerts in the scenario is shown in \\nTable 4-3. Table 4-3. Distribution of CSL and WSL Alerts in the Scenario Pair 1 Pair 2 Pair 3 Pair 4 Pair 5 Pair 6\\nEarly Late Early Late Early Late Early Late Early Late Early Late\\nCSL WSL CSL WSL CSL Lateral Deviation Scenarios5.5.5.3\\nEach Lateral Deviation scenario lasted for approximately 10 minutes and included four IM PA \\npairings, two of which exhibited Lateral Deviations that required the controller to take action. \\nThere were three potential lateral deviation variables considered in the scenario design. First, \\nthe deviating aircraft could be either be the IM Lead or IM Trail Aircraft. Second, the deviation \\nangle could be sharp or shallow. A sharp deviation occurs suddenly, with little warning. A \\nshallow deviation evolves gradually, over a long portion of the approach. And third, the \\ndeviation could either be inboard (toward the other aircraft) or outboard (away from the other \\naircraft). Based on the lateral deviation variables, two deviation types were developed: Deviation Type A: Lead / Sharp (30 deg) / Inboard.\\nDeviation Type B: Trail / Shallow (10 deg) / Outboard. A lateral deviation could also occur earlier or later in the approach. Therefore, four traffic files \\nwere developed for the Lateral Deviation scenarios. Variations included deviation location \\n(earlier versus later), deviation order (whether Type A occurred before Type B, or whether Type \\nB occurred before Type A), and which of the four IM PA pairings experienced them. Table 4-4 \\nshows the how the variations were implemented and distributed across each of the four traffic \\nfiles (named L through O). Table 4-4. Distribution of Lateral Deviation Types Pair 1 Pair 2 Pair 3 Pair 4\\nTraffic File 10 NM 5 NM 10 NM 5 NM 10 NM 5 NM 10 NM 5 NM L A B\\nM B A\\nN A B\\nO B A These lateral deviation events occurred at a far higher frequency than would be experienced \\nin actual operations. This was intended to provide controllers with concentrated \\nexperience in dealing with them to get feedback on the lateral monitoring display \\nfeatures. It should also be noted that the scenario presented in the scenario that \\ninvolved traffic file O was deliberately designed as an extreme case. The timing of this \\nscenario was such that that the shallow-deviating Trail crossed its Lateral Boundary and \\ntriggered its Exceedance Warning shortly after the Lead Aircraft (of a different pair) \\ntriggered its Exceedance Warning. In effect, this appeared to controllers as a near-\\nsimultaneous lateral deviation of two aircraft. In real world operations, a near- xxxvii\\n simultaneous lateral deviation of multiple aircraft on CSPR is expected to occur rarely, if \\nat all. An investigation of over 1.8 million approach paths did not detect any (Eckstein, \\nMassimini, McNeill, & Niles, 2012), nor was there a record of any in an examination of \\n7790 go-arounds that were logged over multiple years by NCT (Stassen, Domino, Hefley, \\n& Weitz, 2019). Though this has never been observed in real world operations, it was still \\nincluded in the simulation to stress the display features and probe for a potential failure \\npoint with IM PA Tools or Monitoring configuration. Scenario Matrices and Run Orders5.5.6\\nThis section describes the scenario matrices across the three weeks of runs. Some changes were \\nmade after the Week 1 scenarios were run, so they are described separately from the Week 2-3 \\nscenarios. The data reduction and analysis process accounted for those differences. The Day 1-2 scenarios were considered nominal operations. Though some longitudinal \\nSpacing Loss events were included, the alert timing values were fixed, and no deliberate Lateral \\nDeviations were introduced. As noted in Section 4.5.3, two controllers participated per run \\nwhile the other two filled out their post-run questionnaires. When the Combined Monitor \\nconfiguration was evaluated, one participant controller served as the Monitor and the other \\nserved as the 28R Final controller. When the Separate Monitor configuration was run, one \\nparticipant served as the 28L Monitor controller and the other served as the 28R Monitor \\ncontroller. The 28R Final controller position was not staffed in these cases, and IM PA pairs were \\ninitiated via the simulation before they were turned over to the Local controller. The scenario order for each day was counterbalanced across the three weeks. The monitor \\nconfigurations were alternated by run week. The daily simulation topic by week is summarized \\nin Table 4-5. Table 4-5. Daily Simulation Topic",
  "sdl_date": "2020-10-16T00:00:00",
  "countryPublished": "Tonga",
  "conference": "sender mentor qv Junliang",
  "originalAuthorName": "Giorgina t Funffinger",
  "title": "sneaker's yank congaed",
  "declaredTags": "Block Chain|risk-based decision making|managing chronic conditions|Air Force interns",
  "releaseReason": "salaam's/playwrights",
  "docName": "MX_30_2856",
  "fundingCenter": 34,
  "resourceURL": "https://serge.com",
  "fundingDepartment": "xy50",
  "caseNumber": "86-9402",
  "publicationDate": "3/3/2020 12:00:00 AM",
  "releaseYear": 2011,
  "releaseStatement": "Publicity/Promotion",
  "approver": "$Souhaib $Prithviraj",
  "handCarry": 9,
  "authorDivision": "bh17",
  "copyrightOwner": "Kenan Llaneras",
  "lastModifiedDate": "4/2/2012 12:00:00 AM",
  "releaseDate": "4/14/2011 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "6318WHUR58",
  "materialType": "Article",
  "publicationType": "Paper",
  "authorCenter": 94,
  "originalAuthorID": "Suleica",
  "mitrePublicServer": 0,
  "subjectTerminology": "Government Agency Operations (General)",
  "dateEntered": "7/12/2006 12:00:00 AM",
  "documentInfoURL": "https://Corleone pathological overlooked kitties swish's.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE2: 17-1719|CASE1: 16-4753|CASE1: 18-1446",
  "organization": "mo26",
  "authorDepartment": "vs10",
  "publicationYear": 2017,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "Independent Effort",
  "level1": "MITRE Legacy",
  "fundingDivision": "fashionably ulna's outlet Kaiser emptiest",
  "publishedOutsideUSA": 0,
  "level3": "kp43",
  "level2": "wl13",
  "sdl_id": "6ea7223ba53d4c32a4ced109ab63b86b",
  "text": "Ablative Tests, where the model is given all module \\ninputs at training time, but at testing time some of the module inputs are corrupted. The corruption \\nmechanism that we chose was the scrambling of the input features of each module separately. This \\nmethod was chosen because simply zeroing the feature values would mean different things for \\ndifferent modules. For Video Entity Detection, zeroing all features would simply mean that no \\nobjects are detected; however, for something more abstract like the Raw Inception Features, the \\nimpact of zeroing is not so clear. Additionally, corruption by scrambling preserves the distribution \\nof values for each modules feature vector. Once again, we performed corruptions of individual \\nmodules and groups of modules. Figure 11 shows the results. NONEAll RGB VideoAll Optical FlowAll AudioAll RGB Video + OpticalAll RGB Video + AudioAll Optical Flow + AudioAll Semantic FeaturesAll Non-Semantic FeaturesVideo Scene DetectionVideo Scene SegmentationVideo Entity ClassificationVideo Entity DetectionSoft BiometricsRaw Inception FeaturesTwo Streams SpatialTwo Streams MotionTwo Streams CombinedTS Spatial Raw ResNet FeaturesTS Motion Raw ResNet FeaturesAudio EntityAudio Feature VggishAudio Feature Cortical Modules Corrupted 0 0.1 0.2 0.3 0.4 0.5 0.6 m\\nAP Corruption Ablative Tests mAP Scores xxii Figure 11: Corruptive Ablative Tests of ContextNet with the LBM Dataset \\nThe graph shows results for our Corruption Ablative Tests with ContextNet on the LBM Dataset. In these tests, \\nall possible input features to ContextNet were given at training time, but at testing time, a subset of module \\nfeature vectors was corrupted through random scrambling. The scores reported are the mAP scores across all \\ncontexts, including Alert. The x-axis states which modules or group of modules were corrupted in the test. The \\ncolor coding of bars represents the following: Orange = No Modules Corrupted, Blue = Group of Modules \\nCorrupted, Green = Single Module Corrupted. See Table 2 for further details on individual modules. The significance of high and low mAP scores is swapped between the Constructive Ablative Tests \\nand the Corruptive Ablative Tests. In the Constructive Ablative Tests, if a module or a group of \\nmodules was very useful, then we saw high mAP scores for it. In the Corruptive Ablative Tests, if \\na module or a group of modules is important, we expect to see a large drop in the mAP scores \\nwhen those modules are corrupted. Thus, the importance of a module or a group of modules can \\nbe measured by how far the mAP score drops when that module or group of modules is corrupted. \\nHowever, an exact inverse correlation is not expected due to the redundancy among the modules. \\nSome modules that had high mAP scores in the Constructive Ablative Tests may not necessarily \\nlead to a significant mAP-score-drop in the Corruptive Ablative Tests, as ContextNet is focusing \\non other modules that cover the same area.\\nFrom the results, we can see that for many individual modules, corruption leads to little or no \\nchange in the mAP scores. This could mean that either (1) ContextNet is not attending to these \\nfeatures at all because it is focusing on other modules that cover the same area, or (2) the \\nredundancy of features is providing a degree of robustness, and thus, corruption of one redundant \\nmodule can be tolerated. For example, Two Streams Spatial and Two Streams Motion are \\nredundantly covered by Two Streams Combined and somewhat redundantly covered by the raw \\nTwo Streams ResNet features. Meanwhile, the Soft Biometrics module led to a significant drop in \\nperformance when corrupted, even though Soft Biometrics was one of the weakest modules in \\nisolation in the Constructive Ablative Tests. This may be because the Soft Biometrics modules \\nfeatures provide unique semantic information not covered by any other module, and thus, when \\nthat unique information is corrupted, it causes a significant drop in performance.\\nFurthermore, we can see that unsurprisingly corrupting All RGB Video, led to a much larger drop \\nin performance than corrupting Soft Biometrics. Our previous ablative experiment showed that in \\nisolation, RGB Video Only can perform almost as well as the full set. Therefore, it is not \\nsurprising that of the All RGB Video, All Optical Flow and All Audio, corrupting All RGB Video \\nled to the biggest drop in the mAP scores. However, this raises the question: Is ContextNet \\nactually using the optical flow and audio features in a meaningful way? If so, then corrupting them \\nshould lead to a deterioration in performance. In this test we can see that corrupting All Optical \\nFlow or All Audio does lead to a drop in performance. This is true both when corrupting them \\nindividually or along with All RGB Video. This shows that even when all modules are provided, \\nboth optical flow and audio are still used by ContextNet for useful supplemental information. \\nFinally, it is interesting to see that corrupting All Semantic Features produces a similar drop in \\nperformance as corrupting All Non-Semantic Features. The previous experiment suggested that xxiii the non-semantic features carried more information than the semantic features; however, in this \\ntest, corrupting each set has a similar impact. Captioner: On the LBM Dataset5.5\\nFor valid comparisons of LinkBioMan Captioner performance, we ran an identical round of \\ntraining and testing on the following captioner models: The Show and Tell Captioner, the \\nLinkBioMan Captioner (first version), henceforth referred to as concat LinkBioMan and the \\nLinkBioMan Captioner (second version), henceforth referred to as embed & add LinkBioMan. \\nAll captioners were first pre-trained on the MSCOCO dataset [63], before being fine-tuned on the \\nLBM Dataset. In the cases where we were evaluating LinkBioMan Captioner performance on the \\nYouTube2Text Dataset (Section 5.6), we follow a similar training scheme; pre-training on the \\nMSCOCO dataset before fine-tuning on the YouTube2Text Dataset.\\nNote that the size of the LBM Dataset is very small relative to the MSCOCO dataset; therefore, we \\ntook several precautions to combat overfitting. We performed fine-tuning for a variable number of \\nsteps, from only 50k all the way to a full 1 million. Additionally, the decaying learning rate \\ninitialized during MSCOCO training was preserved and lowered further during training on the \\nLinkBioMan dataset. Table 4 shows the captioner test results; the Model column describes the \\ncaptioner model used and the number in parentheses is the number of training steps fine-tuned on \\nthe LBM Dataset.\\nAs previously mentioned, our primary goal for the LinkBioMan Captioner is to develop a system \\ncapable of recognizing scene semantic context and producing scene descriptions based on multiple \\nmodalities. This requires training a captioner that can not only recognize the individual details \\nbehind each modality, but also fuse the information from each modality into a contextual \\nunderstanding that aligns with human judgment contained in the ground truth annotations. To \\nassess this alignment, we evaluated the trained captioners on the testing partition of the LBM \\nDataset, using the BLEU [64], ROUGE-L [65], and METEOR [6] metrics. BLEU uses n-gram \\nmatching that rewards captioner results if more n-grams in the result (words, phrases) match with \\nthose in the ground truth. This favors shorter sentences that are precise, capturing only patterns \\nexisting in the annotations. ROUGE-L uses longest-common-subsequence matching that rewards \\ncaptioner results if a larger fraction of the ground truth is encountered. This favors descriptive \\nsentences that recall more complete fragments of the ground truth. BLEU and METEOR run the \\nrisk of respectively favoring overly short or overly long caption statements, which is why pairing \\nthem together gives a more balanced assessment of the overall alignment with the ground truth \\ncaptions. Finally, the METEOR metric, advertised as being more correlated with human \\njudgment, uses a harmonic-mean combination of unigram-precision and unigram-recall \\naugmented by word stem and synonym matching between reference and candidate words. Thus, \\nphrases that are generated by the captioner and match the reference in meaning and context (but \\nnot necessarily verbatim) are better rewarded under METEOR. The combination of all three \\nmetrics allows us to perform a balanced assessment of the captioner in terms of precision, recall, \\nand context regarding human-level semantic scene understanding. xxiv Table 4: Captioner Performance on the LBM Dataset Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEO\\nR ROUGE-L Show and Tell (0k) 0.1397 0.0493 0.0200 0.0091 0.0557 0.1418\\nShow and Tell (50k) 0.2072 0.1281 0.0934 0.0727 0.0924 0.2221\\nShow and Tell (500k) 0.2198 0.1342 0.0950 0.0722 0.0984 0.2244\\nShow and Tell (1M) 0.2227 0.1366 0.0974 0.0745 0.0993 0.2263\\nconcat LinkBioMan \\n(50k) 0.2244 0.1404 0.1006 0.0767 0.1000 0.2304\\nconcat LinkBioMan \\n(500k) 0.2355 0.1484 0.1081 0.0848 0.1051 0.2411\\nconcat LinkBioMan \\n(1M) 0.2364 0.1488 0.1082 0.0848 0.1058 0.2424\\nembed & add \\nLinkBioMan (50k) 0.2365 0.1520 0.1128 0.0892 0.1095 0.2558\\nembed & add \\nLinkBioMan (500k) 0.2570 0.1700 0.1282 0.1019 0.1206 0.2708\\nembed & add \\nLinkBioMan (1M) 0.2574 0.1692 0.1270 0.1011 0.1197 0.2706 The results in Table 4 show LinkBioMan Captioner models outperform the Show and Tell \\nCaptioner in all metrics. This is likely because the LinkBioMan features contain information about \\naudio and temporal stream that would be unavailable to the Show and Tell Captioner. Of the two \\nLinkBioMan Captioner models, embed & add LinkBioMan gave a better performance than concat \\nLinkBioMan. One possible explanation for this is that the act of concatenating the LinkBioMan \\nfeatures expands the cell size of the",
  "updated_at": "1/11/2010 12:00:00 AM",
  "created_at": "3/9/2001 12:00:00 AM"
}