{
  "sdl_source_type": "MPL",
  "productName": "dispel upsurging",
  "uploadedate": "2010-11-22T00:00:00",
  "productUrl": "http://blackens.com",
  "creatorNames": "Rakel u Alijas;Nabiha n Gonnet",
  "uploaded": "2016-07-11T00:00:00",
  "sdl_extracted_summary": "LSTM, which can lead to overfitting. Additionally, \\nembedding the LinkBioMan features on top of the Show and Tell Captioners CNN feature \\nembedding space learned on the MSCOCO dataset may allow the model to better learn \\nrelationships between them.\\nIt should be noted that the scores achieved in Table 4 are lower than the values typically seen on \\nother datasets such as the MSCOCO dataset. This is likely because of the small size of the LBM \\nDataset, which has only 4404 video clips and 1 reference caption per clip. Because most metrics \\nare designed to compare a candidate caption against a group of references, they will tend to give \\nlower scores when only one ground truth reference is provided. We also observed that all \\ncaptioners that were fine-tuned on the LBM Dataset had a strong tendency to produce captions \\nidentical to ones in the training set. Depending on the model and the number of fine-tuning steps, \\n92-99% of captions generated came from the training dataset. This phenomenon of captioning by \\nexemplar was also observed in the original Show and Tell paper [5] where roughly 80% of their \\ntop captions generated came directly from the training dataset. The authors of that paper attributed \\nit to the relatively small size of their training datasets. Given that the LBM Dataset is even smaller, xxv this increased tendency to use training captions as exemplars is to be expected. Captioner: On the YouTube2Text Dataset5.6\\nWe trained and tested the Show and Tell Captioner, concat LinkBioMan and embed & add \\nLinkBioMan on the two configurations of YouTube2Text Dataset: the official \\nYouTube2Text Dataset and YouTube2Text clips with audio. The official YouTube2Text \\nDataset contains 1970 video clips without audio. We split the dataset into 1300 training clips, with \\nthe remaining 670 clips used for testing. Due to the relatively small size of the official \\nYouTube2Text Dataset, we again pre-trained our models on the MSCOCO dataset and then fine-\\ntuned the model on the YouTube2Text Dataset. Similar to the captioner training on the LBM \\nDataset, we maintained the same decaying learning rate between the pre-training and fine-tuning \\nsessions. Additionally, we experimented with 500k and then with 1 million fine-tuning training \\niterations on the YouTube2Text Dataset. We uniformly sampled clips at a rate of 1 frame per 2 \\nseconds for both training and testing. As each clip in the YouTube2Text Dataset contains multiple \\nreference captions, we chose to assign all captions to all clips for training. In total, our training \\ndataset consisted of 343,980 frame-caption pairs. Although the official YouTube2Text Dataset does not contain audio, it is distributed with \\nURLs to the original video clips that contain audio. Therefore, we followed [3] in \\ndownloading the YouTube2Text clips with audio to perform additional experiments. Because \\nsome clips were no longer available, we were able to download 1,620 of the 1,970, which \\namounts to approximately 82% of the official YouTube2Text Dataset size. Despite the \\nreduced dataset size, the proportion of the remaining subsets of the training and testing data \\nroughly matched the proportion of training and testing data in the official YouTube2Text \\nDataset, and thus, there was no need for redistribution between our training and testing \\ndatasets. As noted below, our models trained on the official YouTube2Text Dataset for 500k \\niterations performed better on almost all metrics than those trained on 1 million iterations. \\nTherefore, we chose to train captioner models on the YouTube2Text clips with audio for 500k \\niterations only, following the same training and evaluation procedure as done on the official \\nYouTube2Text Dataset. We also trained the Show and Tell Captioner model on the \\nYouTube2Text clips with audio; however, note that the Show and Tell Captioner does not \\nhandle audio features. For experiments, we normalized the captions by converting all generated words to lower-case \\nand removing any quotation marks from the produced captions. As both concat LinkBioMan \\nand embed & add LinkBioMan are based upon the Show and Tell Captioner [5], each model \\ngenerates one caption per sample, resulting in multiple captions per clip. To determine the \\nfinal caption to be used, we summed up the predicted confidence produced by the model for \\neach unique caption and chose the caption with the largest accumulated confidence. If \\nmultiple captions had the same accumulated confidence, then we would arbitrarily choose \\none caption at random (although this never happened in practice during our experiment). This \\ncaption voting scheme provides a balance in weight between the caption frequency and the xxvi confidence per caption. Because the model tends to copy reference captions from training, \\nmany clips had multiple verbatim duplicate captions. This behavior is not unexpected as \\npreviously explained in Section 5.5. To compare the LinkBioMan Captioner performance with other video captioners \\nperformance on the YouTube2Text Dataset, we used the BLEU [64], METEOR [6] and \\nCIDEr metrics [7]. CIDEr is a text evaluation metric that compares generated captions to an \\nestimated consensus, constructed through weighted n-grams from the ground truth captions. \\nIt replaces ROGUE due to a more thoroughly defined ground truth consensus (multiple \\nground truth captions per video) compared to the LBM Dataset (limited consensus due to 1 \\nground truth caption per video). [3] uses an encoder-decoder architecture fusing auditory, \\nvisual and motion information via an attention model that inputs feedback from the decoder \\nnetwork. [68] uses a recurrent model with multiple encoding rates and learns temporal \\nfeatures using a loss that incorporates the reconstruction of sequences both forward and \\nbackwards in time. [69] encodes local temporal video structure using a 3D-CNN as encoder \\nand uses an attention mechanism to select a subset of frames in order to exploit the global \\nstructure. [19] uses a Semantic Compositional Network, which weighs the decoder with tag-\\ndependent weight matrices and thus encodes semantic concepts. [70] uses two LSTMS, one \\nstacked onto the other, to map a sequence of video frames to a sequence of words that form \\npredicted caption. Table 5 displays our performance on the Youtube2Text Dataset with other video captioners \\nperformance. The last nine rows (in orange and green) contain results from our experiments. \\nIn the Model column, the numbers in parentheses correspond to the number of training \\nsteps fine-tuned on the official YouTube2Text Dataset. We found that both concat \\nLinkBioMan and embed & add LinkBioMan performed worse on the YouTube2Text clips \\nwith audio than on the official YouTube2Text Dataset. This is likely due to audio dubbing \\ncontained in some videos that are unrelated to the content, as investigated by [3]. Note that \\nthe Show and Tell Captioner did not use the audio features when trained on the \\nYouTube2Text clips with audio, and thus did not suffer or rather saw a slight increase in its \\nperformance because the YouTube2Text clips with audio is approximately 82% of the \\nofficial YouTube2Text Dataset. We note that embed & add LinkBioMan performs better than \\nthe concat LinkBioMan for all metrics used in our evaluation. This is similar to our results on \\nthe LBM Dataset, perhaps for the same reasons discussed in Section 5.5. Additionally, both \\nconcat LinkBioMan and embed & add LinkBioMan performed better than the Show and Tell \\nCaptioner for all metrics on the official YouTube2Text Dataset. Furthermore, both the concat \\nLinkBioMan and embed & add LinkBioMan performed better for almost all metrics when \\ntrained on 500k iterations compared to 1 million iterations, possibly because the networks \\nwere beginning to overfit to the training dataset when trained with a larger number of training \\niterations. It is interesting to note that embed & add LinkBioMan (500k) achieved higher \\nBLEU-4, METEOR, and CIDEr scores than [69] and a higher CIDEr score than [3] without \\nthe use of an explicit attention mechanism. Embed & add LinkBioMan (500k) also achieved xxvii a higher METEOR score than [70], suggesting that the embed & add layer might be more \\neffective than the additional LSTM layer. xxviii Table 5: Captioner Performance on the YouTube2Text Dataset and YouTube2Text Clips with Audio The last three rows (green) pertain to LinkBioMan Captioner models (concat, embed & add) on the YouTube2Text clips with audio \\nand the six rows above (orange) on the official YouTube2Text Dataset, with the best score underlined. All other rows display results of \\nvideo captioner models using the official YouTube2Text Dataset, except Attention Fusion (AV). The LinkBioMan Captioner results are \\nbetter without audio because the audio dubbing contained in some clips is unrelated to the content, as investigated by [3]. Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEO\\nR CIDEr Attention Fusion (V) [3] - 0.524 0.320 0.404 Attention Fusion (AV) [3] - 0.539 0.322 0.400 mGRU + pre-train GoogLeNet Features [68] 0.808 0.695 0.600 0.495 0.334 0.755 mGRU + pre-train ResNet-200 Features [68] 0.825 0.722 0.633 0.538 0.345 0.812 Enc-Dec + Local + Global [69] - 0.419 0.296 0.517 SCN-LSTM Ensemble of 5 [19] - 0.511 0.335 0.777 S2VT + RGB (VGG) + Flow (AlexNet) [70] - - 0.298  Show and Tell (500k) [5] 0.775 0.594 0.490 0.388 0.300 0.585 Show and Tell (1M) [5] 0.773 0.589 0.486 0.384 0.297 0.585 concat LinkBioMan (500k) 0.784 0.610 0.504 0.400 0.305 0.638 concat LinkBioMan (1M) 0.780 0.606 0.500 0.397 0.304 0.647 embed",
  "sdl_date": "2020-07-20T00:00:00",
  "countryPublished": "Croatia",
  "conference": "alighted autocrat's yi Maier",
  "originalAuthorName": "Xira x Barrecheguren",
  "title": "slickly conciliator's flagrantly",
  "declaredTags": "fractals|Arctic communications challenges|network",
  "releaseReason": "joyfulness's/chaise's",
  "docName": "XH_69_7332",
  "fundingCenter": 72,
  "resourceURL": "https://Galois.com",
  "fundingDepartment": "ed65",
  "caseNumber": "34-5816",
  "publicationDate": "8/14/2019 12:00:00 AM",
  "releaseYear": 2018,
  "releaseStatement": "Peer-reviewed Publication/Journal",
  "approver": "$Loreta $Feldscher",
  "handCarry": 3,
  "authorDivision": "bf85",
  "copyrightOwner": "Anghara Ellingham",
  "lastModifiedDate": "1/17/2005 12:00:00 AM",
  "releaseDate": "1/28/2003 12:00:00 AM",
  "onMitrePublicSrvr": 0,
  "projectNumber": "7047VTLH12",
  "materialType": "Paper",
  "publicationType": "Book",
  "authorCenter": 23,
  "originalAuthorID": "Gaizkane",
  "mitrePublicServer": 0,
  "subjectTerminology": "Air Traffic Management",
  "dateEntered": "9/13/2000 12:00:00 AM",
  "documentInfoURL": "https://imperfects anticipations rasher's refiner concerting.com",
  "softShell": 0,
  "publishedOnNonMITREServer": 0,
  "priorCaseNumbers": "CASE1: 17-4547|CASE1: 16-4196",
  "organization": "sx32",
  "authorDepartment": "hq20",
  "publicationYear": 2013,
  "sensitivity": "Public",
  "copyrightText": "(c) 2016 The MITRE Corporation All Rights Reserved",
  "fundingSource": "CAMH FFRDC Contracts",
  "level1": "MITRE National Security Sector",
  "fundingDivision": "fewest unsettling dungaree reload bettered",
  "publishedOutsideUSA": 0,
  "level3": "sx20",
  "level2": "kb96",
  "sdl_id": "e2f8a27bbb5f42e5a76e6b121f471702",
  "text": "the resiliency needs of that specific mission are addressed. \\nSection 5.3 provides a more detailed description of specific recommendations based on \\nenvironmental factors. Section 5.4 will show how resiliency mitigations might be tailored for a \\nspecific environment by revisiting the example attack described in Section 4.\\nThe application of cyber resilience mitigations to supply chain attacks must take both the threat \\nenvironment and the mission environment into account. The discussion in Section 3 described the \\nthreat in terms of adversary actions in the abstract, while the discussion in Section 4 described the \\nthreat in terms of adversary actions in a theoretical environment. Defender Goals6.2\\nThe overall defender goal is to enable missions to succeed despite being under attack. In some \\nsituations, the mission is a single event. More frequently the mission is ongoing and the defenses \\nmust evolve as the adversarys attack evolves. This defense is supported by four underlying \\ndefender activities.12 Just as with attacker goals, each defender goal is supported by a different \\ncombination of cyber resiliency mitigations. The defender goals are: Reduce Attacks: While defenders cannot eliminate all attacks the number and range of \\nattacks in the Operations and Support phase can be reduced by cyber resiliency \\nmitigations in earlier phases of the acquisition lifecycle. Diminish Success of Attacks: Cyber resiliency mitigations applied throughout the \\nacquisition lifecycle can make attacks that are not eliminated less successful. This can \\nmean either limiting the overall impact to the mission or limiting the impact on the \\nspecific element of the environment being attacked. Gain and Share Information about Adversary Activity: Sharing information about \\nadversary activity throughout the acquisition provides knowledge of adversarial activities \\nwithin the Operations and Support phase. Defenders can use the information acquired \\nfrom adversary activities in prior acquistion phases to detect and stop attacks as well as to \\nchange mission operations so that attacks do not stop the mission. Recover: Using information gained from earlier phases along with appropriate \\npreparations, helps defenders achieve recover operational abilities quickly enough to \\nensure the mission is accomplished. For this analysis, we provide a general characterization of defender activities undertaken to \\nenable mission success. It should be noted, however, that defenders focus may differ based on \\ntheir role within an acquisition lifecycle phase. For example, early in the acquisition lifecycle, \\nthe defender is the service element which provides ICT support to the Program Office. The \\ndefender is focused on preventing exfiltration of sensitive information about the system to be \\nacquired, and preventing specifications from being modified. \\nLater, in the acquisition lifecycle (during the Engineering and Manufacturing Development and \\nProduction & Deployment), the defender is the prime contractor, together with subcontractors xxvii and suppliers. The overall goals are to prevent exfiltration and to prevent unauthorized \\nmodification of specifications, design documents, and to-be-deployed-system components.\\nDuring the Operations & Sustainment phase, two defenders are active: the defender of the as-\\ndeployed system, and the defender of the maintenance environment. The defender of the as-\\ndeployed system is focused on mission assurance; the defender of the maintenance environment \\nis focused on protecting the portion of the supply chain for which they are responsible.\\nThe recover goal is usually part of limiting the effectiveness of attacks as refered to in [18]. \\nIn the case of supply chain attacks the difficulty of recovery and the need to specifically focus on \\nthat aspect makes it appropriate to emphasize this goal along with the other three. Effective Defense Throughout the Acquisition Lifecycle6.2.1\\nDefenders can more effectively address adversary activity if they defend the acquisition lifecycle \\nas a whole, rather than viewing each phase in isolation without sharing information or \\nconsidering the other phases. This is challenging because the environment and its ownership \\n(e.g., program office, contractor, or mission environment) changes as the acquisition lifecycle \\nprogresses. The Engineering and Manufacturing Development and Production and Deployment \\nphases provide the greatest opportunity for defenders both in terms of Cyber resiliency \\ntechniques and approaches (i.e., greatest number of techniques possible) as well as in terms of \\nimpact across the goals (i.e., the adversary has the most widely varied activity and is most active \\nin these phases). In addition, if defenders wait until Operations and Support to address the \\nadversarys threat to Operations and Support, the adversary has already acted and the \\npossibilities of remedy are limited because the environment is less flexible.\\nAs the system moves through the acquisition lifecycle, the adversary priorities change from \\nobtaining information about the system to developing and delivering initial exploits to get hooks \\ninto the development environment and the finally to attacking the end system and maintaining \\ncontrol. Similarly, as the system moves through the acquisition lifecycle, the defender priorities \\nshould shift from protecting information to protecting the component development environment \\nand gaining information about the adversary for defense activity in Engineering and \\nManufacturing Development and Production and Deployment and finally to detecting and \\nresponding to adversary activity in Production and Deployment and Operations & Sustainment. \\nMore specifically, the set of potential cyber resiliency mitigations increases over the acquisition \\nlifecycle to include: Protecting information from unauthorized access \\nAnalyzing what the adversary is doing, detecting their presence, \\nMaking it harder for the adversary to function in the development environment by making \\nit more diverse and deceptive, \\nResponding to the adversary activities in such a way that it minimizes the adversarys \\nability to successfully complete an attack and o\\ncause the adversary to expose their activities as much as possible.o Based on the adversary activities as described in [19], the most effective points at which to apply \\nthe cyber resiliency mitigations to reduce the number of attacks in the Operations and Support \\nand gain the most information about adversary targets and activities are in the Engineering and \\nManufacturing Development phase and the Production and Deployment. xxviii Cyber Resiliency Mitigations Considering Adversary and Defender 6.3\\nGoals Cyber resiliency countermeasures are any response taken to prevent, mitigate, or recover from \\none or more attack impact. Preventative countermeasures reduce the likelihood of an adverse event or subsequent \\neffect by avoiding or preventing the initial attack vector. Mitigating countermeasures constrain or otherwise decrease the rate of degradation \\ncaused by the adverse impact. Recovery countermeasures improve the rate of reconstitution, such as through restoring \\nlost capabilities or making additional resources available. Countermeasures can occur across people, processes, technology, and policy. The information \\ngained from these countermeasures can be used to evolve the mission operations and systems, \\nincreasing cyber resiliency.\\nBelow is a discussion of cyber resiliency countermeasures that can be used to mitigate the \\noperational impacts caused by successful supply chain attacks. The most widely applicable and \\neffective mitigations will be highlighted in this text. The rest can be found in the tables in \\nAppendix D. Materiel Solutions Analysis Phase6.3.1\\nIn the Materiel Solutions Analysis phase, the adversary goal is to gain information. Defenders \\nare usually not able to gain specific information about adversary interests because of the passive \\ntechniques used by, and the adversarys diverse interests at this point in their attack lifecycle. In \\ngeneral, defender environments tend to be enterprise IT and techniques such as Non-Persistence, \\nPrivilege Restriction and Segmentation are the best ways of defending against the adversary \\nactivities in this phase. \\nSpecifically, the Non-Persistence Information approach wipes information as soon as it is no \\nlonger needed. This approach can be applied to information caches and other temporary \\ninformation storage areas. Non-Persistent Services and Non-Persistent Connectivity approaches \\nremove the services and connectivity respectively when they are not being used for authorized \\npurposes thereby denying paths to the information. All three approaches reduce the opportunities \\nfor unauthorized access to information.\\nThe Privilege Management approach of the Privilege Restriction technique reduces the number \\nof resources accessible with individual resources based on criticality. Within the Material \\nSolutions Analysis phase, this causes the adversary to spend more effort to gain access credentials \\nto the resources most useful in gaining reconnaissance information. Similarly, the Privilege-\\nBased Usage Restriction approach reduces the opportunity for the adversary to gain access to \\nresources by restricting access to only those individuals who need a resource to perform their \\nduties. With this approach in place, the adversary must spend additional effort in identifying \\nwhich individuals have access to the resources they need. The Dynamic Privileges approach is \\none that changes the level of privileges assigned to users as well as the level of privileges needed \\nto access resources dynamically. For example, access to certain resources after regular work \\nhours could be restricted to a smaller group of people, thereby making it harder for the adversary \\nto conduct activities at times they would not be noticed. xxix There are two Segmentation approaches Predefined Segmentation and Dynamic Segmentation \\nthat are useful within the Material Solutions Analysis phase. Both these approaches reduce the \\nadversarys ability to exfiltrate data defeating their main goal in this phase. \\nIn addition, using the Temporal Unpredictability approach of the Unpredictability technique in \\nconjunction with Privilege Restriction can boost that techniques effectiveness by making it even \\nmore difficult for the adversary to determine which privileges are needed at a specific time. For \\nmore details please refer to Appendix C. Technology Maturity and Risk Reduction",
  "updated_at": "12/23/2013 12:00:00 AM",
  "created_at": "5/28/2005 12:00:00 AM"
}